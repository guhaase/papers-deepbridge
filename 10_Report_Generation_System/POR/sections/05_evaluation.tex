\section{Avaliacao}

Avaliamos o sistema de geracao de relatorios em tres dimensoes:
\begin{enumerate}
    \item \textbf{Estudo de Usabilidade}: 12 usuarios (6 data scientists, 6 stakeholders nao-tecnicos)
    \item \textbf{Case Studies}: 3 aplicacoes reais (fraud detection, medical diagnosis, credit scoring)
    \item \textbf{Performance Benchmarks}: Tempo de geracao, tamanho de arquivos, reproducibilidade
\end{enumerate}

\subsection{Estudo de Usabilidade}

\subsubsection{Metodologia}

\textbf{Participantes}:
\begin{itemize}
    \item \textbf{Grupo A - Data Scientists} (N=6): 3-8 anos de experiencia em ML
    \item \textbf{Grupo B - Stakeholders} (N=6): Product managers, auditores, executivos
\end{itemize}

\textbf{Tarefas}:
\begin{enumerate}
    \item \textbf{Criacao de relatorio} (Grupo A): Gerar relatorio de validacao para modelo fornecido usando (1) abordagem tradicional (Jupyter notebook) vs. (2) nosso sistema template-driven
    \item \textbf{Compreensao de relatorio} (Grupo B): Responder questoes sobre resultados de validacao baseado em relatorios gerados por (1) notebook customizado vs. (2) nosso sistema
    \item \textbf{Comparacao de modelos} (Ambos grupos): Comparar 3 modelos baseado em relatorios de validacao
\end{enumerate}

\textbf{Metricas}:
\begin{itemize}
    \item \textbf{Tempo de conclusao}: Minutos para completar tarefa
    \item \textbf{Acuracia}: Proporcao de respostas corretas
    \item \textbf{System Usability Scale (SUS)}: Questionario padrao (0-100)
    \item \textbf{NASA-TLX}: Carga cognitiva (0-100, menor = melhor)
    \item \textbf{Feedback qualitativo}: Entrevistas semi-estruturadas
\end{itemize}

\subsubsection{Resultados - Data Scientists (Grupo A)}

\textbf{Tarefa 1: Criacao de Relatorio de Uncertainty Quantification}

\begin{table}[h]
\centering
\caption{Comparacao de Tempo de Criacao de Relatorios}
\begin{tabular}{lrrl}
\toprule
\textbf{Abordagem} & \textbf{Tempo Medio} & \textbf{Desvio Padrao} & \textbf{Reducao} \\
\midrule
Jupyter Notebook (baseline) & 8.2 h & 1.4 h & - \\
Nosso Sistema & 1.2 h & 0.3 h & \textbf{85\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Analise detalhada do tempo}:
\begin{itemize}
    \item \textbf{Jupyter baseline}:
    \begin{itemize}
        \item Setup e imports: 15 min
        \item Data wrangling: 2.5 h
        \item Criacao de visualizacoes: 3.5 h
        \item Formatacao e styling: 1.5 h
        \item Documentacao: 0.7 h
    \end{itemize}
    \item \textbf{Nosso sistema}:
    \begin{itemize}
        \item Preparacao de dados: 0.5 h (reuso de resultados de validacao)
        \item Chamada de API: 5 min
        \item Customizacao (opcional): 0.4 h
        \item Review: 0.3 h
    \end{itemize}
\end{itemize}

\textbf{System Usability Scale (SUS)}:
\begin{itemize}
    \item Jupyter baseline: 62.5 (mediano)
    \item Nosso sistema: 87.3 (excelente)
\end{itemize}

\textbf{Feedback qualitativo}:
\begin{itemize}
    \item \textit{"Eliminei 90\% do trabalho tedioso de formatting. Posso focar em analise."} - P1
    \item \textit{"Templates padronizados facilitam comparacao entre modelos."} - P3
    \item \textit{"Visualizacoes interativas Plotly muito superiores a matplotlib estatico."} - P5
\end{itemize}

\subsubsection{Resultados - Stakeholders (Grupo B)}

\textbf{Tarefa 2: Compreensao de Relatorio de Robustness}

\begin{table}[h]
\centering
\caption{Acuracia em Questoes de Compreensao}
\begin{tabular}{lrr}
\toprule
\textbf{Tipo de Relatorio} & \textbf{Acuracia Media} & \textbf{Tempo Medio} \\
\midrule
Notebook customizado & 58\% & 12.5 min \\
Nosso sistema & 92\% & 6.8 min \\
\textbf{Melhoria} & \textbf{+34pp} & \textbf{-46\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Questoes respondidas} (10 total):
\begin{enumerate}
    \item Qual modelo e mais robusto a ruido gaussiano?
    \item Qual feature tem maior importancia instavel?
    \item Degradacao de accuracy para ruido de 10\%?
    \item Modelo passa em threshold minimo de robustez (accuracy $> 0.8$ para ruido $< 5\%$)?
    \item Qual tipo de ruido mais impacta performance?
    \item etc.
\end{enumerate}

\textbf{NASA-TLX (Carga Cognitiva)}:
\begin{itemize}
    \item Notebook customizado: 67.2 (carga alta)
    \item Nosso sistema: 32.8 (carga baixa)
    \item Reducao: \textbf{51\%}
\end{itemize}

\textbf{Feedback qualitativo}:
\begin{itemize}
    \item \textit{"Secoes claramente organizadas facilitam encontrar informacao relevante."} - P8
    \item \textit{"Graficos interativos permitem explorar dados sem pedir ajuda a data scientist."} - P10
    \item \textit{"Metricas agregadas no topo dao visao geral instantanea."} - P12
\end{itemize}

\subsubsection{Tarefa 3: Comparacao de Modelos}

Ambos grupos compararam 3 modelos de fraud detection baseado em relatorios de validacao multi-dimensao (uncertainty + robustness + fairness).

\textbf{Resultados}:

\begin{table}[h]
\centering
\caption{Comparacao de Modelos - Acuracia de Ranking}
\begin{tabular}{lcccc}
\toprule
\textbf{Grupo} & \textbf{Tipo Relatorio} & \textbf{Ranking Correto} & \textbf{Tempo} & \textbf{Confianca} \\
\midrule
\multirow{2}{*}{Data Scientists} & Notebooks & 4/6 (67\%) & 28 min & 6.2/10 \\
                                   & Nosso sistema & 6/6 (100\%) & 15 min & 8.9/10 \\
\midrule
\multirow{2}{*}{Stakeholders} & Notebooks & 2/6 (33\%) & 42 min & 4.1/10 \\
                               & Nosso sistema & 5/6 (83\%) & 22 min & 7.8/10 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Insights}:
\begin{itemize}
    \item Estrutura consistente entre relatorios facilita comparacao
    \item Metricas padronizadas permitem comparacao direta
    \item Visualizacoes lado-a-lado (facilmente copiadas entre relatorios) aceleram analise
\end{itemize}

\subsection{Case Studies}

\subsubsection{Case Study 1: Fraud Detection em Fintech}

\textbf{Contexto}:
Startup fintech desenvolvendo modelo de deteccao de fraude em transacoes de credito. Necessita relatorios de validacao para:
\begin{itemize}
    \item Stakeholders internos (product, engineering)
    \item Reguladores (Banco Central)
    \item Auditoria de fairness (evitar discriminacao)
\end{itemize}

\textbf{Uso do sistema}:
\begin{enumerate}
    \item \textbf{Robustness Report}: Avaliacao de resistencia a adversarial attacks
    \item \textbf{Fairness Report}: Analise de disparate impact por idade, genero
    \item \textbf{Uncertainty Report}: Quantificacao de incerteza em predicoes high-risk
\end{enumerate}

\textbf{Resultados}:
\begin{itemize}
    \item \textbf{Tempo de preparacao de relatorios}: 24h (baseline) $\rightarrow$ 3h (nosso sistema)
    \item \textbf{Aprovacao regulatoria}: Relatorio padronizado aceito sem revisoes
    \item \textbf{Deteccao de issues}: Fairness report identificou disparate impact em grupo 60+ anos (ratio=0.72), levando a re-treinamento com fairness constraints
\end{itemize}

\textbf{Quote do cliente}:
\begin{quote}
\textit{"Templates especializados economizaram semanas de trabalho. Relatorios HTML interativos impressionaram reguladores habituados a PDFs estaticos. Fairness report identificou bias que teriamos perdido em analise ad-hoc."}
--- Head of ML, Fintech Startup
\end{quote}

\subsubsection{Case Study 2: Medical Diagnosis System}

\textbf{Contexto}:
Hospital universitario desenvolvendo sistema de auxilio diagnostico para radiologia. Requisitos criticos:
\begin{itemize}
    \item Uncertainty quantification (modelos devem "saber o que nao sabem")
    \item Robustness a variacao de equipamentos de imagem
    \item Documentacao auditavel para FDA approval
\end{itemize}

\textbf{Uso do sistema}:
\begin{enumerate}
    \item \textbf{Uncertainty Report}: Conformal prediction para quantificar confidence intervals
    \item \textbf{Robustness Report}: Testes com imagens de diferentes scanners
    \item \textbf{Resilience Report}: Performance sob distribution shift (diferentes hospitais)
\end{enumerate}

\textbf{Resultados}:
\begin{itemize}
    \item \textbf{Coverage calibrado}: Uncertainty report demonstrou coverage de 95\% para alpha=0.05 em validacao externa
    \item \textbf{Robustness cross-device}: Degradacao $< 5\%$ entre scanners diferentes
    \item \textbf{FDA submission}: Relatorios HTML exportados para PDF incluidos em documentacao regulatoria
\end{itemize}

\textbf{Metricas de impacto}:
\begin{table}[h]
\centering
\caption{Metricas de Validacao - Medical Diagnosis}
\begin{tabular}{lrr}
\toprule
\textbf{Metrica} & \textbf{Baseline} & \textbf{Com Sistema} \\
\midrule
Relatorios gerados & 3 & 12 \\
Tempo total (horas) & 80 & 15 \\
Coverage calibrado & Nao medido & 95.2\% \\
Issues identificados & 2 & 7 \\
Aprovacao FDA & Pendente & Aprovado \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Case Study 3: Credit Scoring em Banco}

\textbf{Contexto}:
Banco comercial migrando de scorecard tradicional para modelo ML de credit scoring. Desafios:
\begin{itemize}
    \item Compliance com regulacoes de credito (ECOA, Fair Lending)
    \item Comparacao entre modelo antigo e novo
    \item Comunicacao de resultados para board executivo
\end{itemize}

\textbf{Uso do sistema}:
\begin{enumerate}
    \item \textbf{Fairness Report}: Analise de demographic parity e equalized odds
    \item \textbf{Hyperparameter Report}: Otimizacao de threshold de aprovacao
    \item \textbf{Comparative Report}: Modelo ML vs. scorecard tradicional
\end{enumerate}

\textbf{Resultados}:
\begin{itemize}
    \item \textbf{Fairness compliance}: Disparate impact ratio = 0.86 (acima de threshold 0.80)
    \item \textbf{Performance gain}: AUC 0.78 (ML) vs. 0.71 (scorecard), demonstrado via relatorios comparativos
    \item \textbf{Board approval}: Relatorios interativos apresentados em reuniao executiva, permitindo exploracao ao vivo de metricas
\end{itemize}

\textbf{Quote do cliente}:
\begin{quote}
\textit{"Relatorios HTML interativos mudaram dinamica de apresentacao para board. Executivos podiam fazer perguntas e eu filtrava dados em tempo real. Impossivel com PowerPoint estatico."}
--- Chief Data Officer, Banco Comercial
\end{quote}

\subsection{Performance Benchmarks}

\subsubsection{Tempo de Geracao}

\begin{table}[h]
\centering
\caption{Tempo de Geracao de Relatorios}
\begin{tabular}{lrrr}
\toprule
\textbf{Tipo Relatorio} & \textbf{Tamanho Dados} & \textbf{Tempo (s)} & \textbf{Throughput} \\
\midrule
Uncertainty & 10k samples & 2.3 & 435 reports/h \\
Robustness & 50k samples & 4.7 & 766 reports/h \\
Fairness & 100k samples & 6.2 & 580 reports/h \\
Resilience & 25k samples & 3.8 & 947 reports/h \\
Hyperparameter & 1k configs & 1.9 & 1,895 reports/h \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Performance escalavel}: Tempo cresce sub-linearmente com tamanho de dados devido a caching e lazy evaluation.

\subsubsection{Tamanho de Arquivos}

\begin{table}[h]
\centering
\caption{Tamanho de Relatorios Gerados}
\begin{tabular}{lrr}
\toprule
\textbf{Tipo Relatorio} & \textbf{HTML (KB)} & \textbf{PDF (KB)} \\
\midrule
Uncertainty (interactive) & 245 & - \\
Uncertainty (static) & 180 & 420 \\
Robustness (interactive) & 312 & - \\
Robustness (static) & 225 & 580 \\
Fairness (interactive) & 198 & - \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Trade-offs}:
\begin{itemize}
    \item Interactive HTML: Menor tamanho, requer JavaScript habilitado
    \item Static PDF: Maior tamanho, funciona em qualquer visualizador
\end{itemize}

\subsubsection{Reproducibilidade}

Testamos reproducibilidade gerando 100 relatorios para mesmos dados:

\begin{itemize}
    \item \textbf{Hash MD5}: 100\% identicos (mesmo hash para todos relatorios)
    \item \textbf{Byte-level comparison}: Identicos exceto timestamp (remover timestamp $\rightarrow$ 100\% identicos)
    \item \textbf{Visual comparison}: Screenshots pixel-perfect identicos
\end{itemize}

\textbf{Conclusao}: Sistema e perfeitamente reproduzivel (mesmo input $\rightarrow$ mesmo output).

\subsection{Limitacoes Identificadas}

Estudo revelou limitacoes:

\begin{enumerate}
    \item \textbf{Curva de aprendizado inicial}: Data scientists levam 2-3 horas para dominar sistema (vs. Jupyter que ja conhecem)
    \item \textbf{Customizacao profunda}: Mudancas radicais em layout requerem conhecimento de Jinja2 + HTML/CSS
    \item \textbf{Dependencia de Plotly.js}: Relatorios interativos requerem JavaScript, nao funcionam em ambientes restritos
    \item \textbf{Tamanho de datasets}: Graficos interativos degradam para $>$ 100k pontos (solucao: sampling)
\end{enumerate}

\subsection{Resumo de Resultados}

\begin{table}[h]
\centering
\caption{Resumo de Metricas de Avaliacao}
\begin{tabular}{lrr}
\toprule
\textbf{Metrica} & \textbf{Baseline} & \textbf{Melhoria} \\
\midrule
Tempo de criacao (data scientists) & 8.2 h & \textbf{-85\%} \\
Acuracia de compreensao (stakeholders) & 58\% & \textbf{+34pp} \\
Carga cognitiva (NASA-TLX) & 67.2 & \textbf{-51\%} \\
SUS score & 62.5 & \textbf{+40\%} \\
Tempo de comparacao de modelos & 28-42 min & \textbf{-46\%} \\
Reproducibilidade (MD5 hash match) & N/A & \textbf{100\%} \\
Throughput (reports/hora) & Manual & \textbf{435-1,895} \\
\bottomrule
\end{tabular}
\end{table}
