# Experimentos - DeepBridge Fairness Framework

Pasta contendo o plano de experimentos para validaÃ§Ã£o do paper "DeepBridge Fairness: Da Pesquisa Ã  RegulaÃ§Ã£o".

## ğŸ“ Estrutura de Arquivos

```
experimentos/
â”œâ”€â”€ README.md                    # Este arquivo
â”œâ”€â”€ PLANO_EXPERIMENTOS.md        # Plano completo e detalhado (17 seÃ§Ãµes)
â”œâ”€â”€ CHECKLIST_RAPIDO.md          # Checklist executivo para tracking
â”œâ”€â”€ scripts/                     # Scripts Python para executar experimentos
â”‚   â”œâ”€â”€ exp1_auto_detection.py
â”‚   â”œâ”€â”€ exp2_metrics_coverage.py
â”‚   â”œâ”€â”€ exp3_eeoc_validation.py
â”‚   â”œâ”€â”€ exp4_case_studies.py
â”‚   â”œâ”€â”€ exp5_usability.py
â”‚   â”œâ”€â”€ exp6_performance.py
â”‚   â”œâ”€â”€ exp7_threshold_opt.py
â”‚   â”œâ”€â”€ exp8_comparison.py
â”‚   â””â”€â”€ utils.py                 # FunÃ§Ãµes auxiliares
â”œâ”€â”€ data/                        # Datasets e ground truth
â”‚   â”œâ”€â”€ ground_truth.csv         # AnotaÃ§Ãµes manuais (500 datasets)
â”‚   â”œâ”€â”€ case_studies/            # Dados dos 4 case studies
â”‚   â””â”€â”€ synthetic/               # Datasets sintÃ©ticos para testes
â”œâ”€â”€ results/                     # Resultados dos experimentos
â”‚   â”œâ”€â”€ auto_detection/
â”‚   â”œâ”€â”€ eeoc_validation/
â”‚   â”œâ”€â”€ case_studies/
â”‚   â”œâ”€â”€ usability/
â”‚   â”œâ”€â”€ performance/
â”‚   â””â”€â”€ comparison/
â””â”€â”€ reports/                     # RelatÃ³rios e visualizaÃ§Ãµes
    â”œâ”€â”€ experiment_summary.pdf
    â”œâ”€â”€ figures/                 # GrÃ¡ficos e tabelas
    â””â”€â”€ reproduction_guide.md    # Como reproduzir experimentos
```

## ğŸš€ Quick Start

### 1. PreparaÃ§Ã£o do Ambiente

```bash
# Criar ambiente virtual
python -m venv venv_experiments
source venv_experiments/bin/activate  # Linux/Mac
# ou
venv_experiments\Scripts\activate  # Windows

# Instalar dependÃªncias
pip install deepbridge
pip install aif360 fairlearn aequitas  # Ferramentas para comparaÃ§Ã£o
pip install pandas numpy scipy scikit-learn
pip install matplotlib seaborn plotly
pip install pytest pytest-cov

# Verificar instalaÃ§Ã£o
python -c "from deepbridge import DBDataset; print('DeepBridge OK')"
```

### 2. Executar Experimento Exemplo

```bash
# Teste rÃ¡pido com COMPAS dataset
cd scripts/
python exp4_case_studies.py --dataset compas --quick

# SaÃ­da esperada:
# âœ… Atributos detectados: ['race', 'sex', 'age'] (3/3)
# âœ… Tempo de anÃ¡lise: 7.2 min
# âœ… ViolaÃ§Ã£o detectada: FPR difference 22pp
# âœ… Threshold Ã³timo: 0.62 (FPR â†’ 8pp)
```

### 3. Ver Checklist de Progresso

```bash
cat CHECKLIST_RAPIDO.md
```

## ğŸ“Š Experimentos Principais

### Experimentos CrÃ­ticos (â­ Prioridade MÃXIMA)

1. **Auto-DetecÃ§Ã£o** (`exp1_auto_detection.py`)
   - 500 datasets
   - Target: F1 â‰¥ 0.90
   - Tempo estimado: 20h

2. **VerificaÃ§Ã£o EEOC/ECOA** (`exp3_eeoc_validation.py`)
   - Regra 80% + Question 21
   - Target: 100% precisÃ£o
   - Tempo estimado: 8h

3. **Case Studies** (`exp4_case_studies.py`)
   - COMPAS, German Credit, Adult, Healthcare
   - Target: 75-79% economia de tempo
   - Tempo estimado: 40h

4. **Usabilidade** (`exp5_usability.py`)
   - N=20 participantes
   - Target: SUS â‰¥ 85
   - Tempo estimado: 60h (inclui recrutamento)

5. **Performance** (`exp6_performance.py`)
   - 3 tamanhos de datasets
   - Target: Speedup â‰¥ 2.9x
   - Tempo estimado: 12h

6. **ComparaÃ§Ã£o** (`exp8_comparison.py`)
   - AIF360, Fairlearn, Aequitas
   - Target: Feature matrix validada
   - Tempo estimado: 16h

**Total estimado**: ~156 horas (4 semanas full-time)

## ğŸ“– Documentos

### [PLANO_EXPERIMENTOS.md](PLANO_EXPERIMENTOS.md)
Documento master com:
- 17 seÃ§Ãµes de experimentos detalhados
- Metodologias completas
- MÃ©tricas de validaÃ§Ã£o
- CritÃ©rios de sucesso
- Timeline de 18 semanas

### [CHECKLIST_RAPIDO.md](CHECKLIST_RAPIDO.md)
Checklist executivo com:
- 6 experimentos crÃ­ticos
- Tabela de validaÃ§Ã£o de claims
- Red flags e riscos
- Timeline resumido

## ğŸ¯ Claims do Paper a Validar

| Claim | Valor | Experimento |
|-------|-------|-------------|
| Auto-detecÃ§Ã£o F1-Score | 0.90 | 1.1 |
| Auto-detecÃ§Ã£o Precision | 0.92 | 1.1 |
| Auto-detecÃ§Ã£o Recall | 0.89 | 1.1 |
| MÃ©tricas totais | 15 (4+11) | 2.1 |
| SUS Score | 85.2 | 5.1 |
| NASA-TLX | 32.1 | 5.2 |
| Taxa de sucesso | 95% | 5.3 |
| Time-to-insight | 10.2 min | 5.4 |
| Speedup mÃ©dio | 2.9x | 6.1 |
| ReduÃ§Ã£o de memÃ³ria | 40-42% | 6.2 |
| COMPAS tempo | 7.2 min | 4.1 |
| German Credit tempo | 5.8 min | 4.2 |
| Adult tempo | 12.4 min | 4.3 |
| Healthcare tempo | 9.1 min | 4.4 |

## âš ï¸ CritÃ©rios MÃ­nimos para PublicaÃ§Ã£o

Para o paper ser aceito no FAccT 2026, os seguintes critÃ©rios DEVEM ser atendidos:

### âœ… ObrigatÃ³rios (Deal-breakers):
1. **EEOC/ECOA**: 100% precisÃ£o (sem margem de erro)
2. **SUS**: â‰¥ 75 (claim: 85.2)
3. **Speedup**: â‰¥ 2.0x (claim: 2.9x)
4. **Case Studies**: 4/4 completos
5. **Usabilidade N**: â‰¥ 15 participantes (claim: 20)

### â­ Recomendados:
1. **F1 auto-detecÃ§Ã£o**: â‰¥ 0.85 (claim: 0.90)
2. **Taxa de sucesso**: â‰¥ 85% (claim: 95%)
3. **Datasets**: â‰¥ 300 (claim: 500)

## ğŸ”¬ ExecuÃ§Ã£o dos Experimentos

### Ordem Recomendada:

```bash
# Semana 1-2: Setup
scripts/setup_environment.sh
scripts/collect_datasets.py

# Semana 3-4: Auto-detecÃ§Ã£o
python scripts/exp1_auto_detection.py --full

# Semana 5-6: MÃ©tricas + EEOC
python scripts/exp2_metrics_coverage.py
python scripts/exp3_eeoc_validation.py

# Semana 7-9: Case Studies
python scripts/exp4_case_studies.py --all

# Semana 10-12: Usabilidade
python scripts/exp5_usability.py --recruit --execute

# Semana 13-14: Performance
python scripts/exp6_performance.py --all-sizes

# Semana 15: ComparaÃ§Ã£o
python scripts/exp8_comparison.py --tools all

# Semana 16: Robustness
python scripts/exp9_edge_cases.py

# Semana 17-18: AnÃ¡lise e RelatÃ³rios
python scripts/generate_reports.py --output reports/
```

## ğŸ“ˆ Tracking de Progresso

Use o arquivo `CHECKLIST_RAPIDO.md` para tracking diÃ¡rio:

```bash
# Ver status atual
grep "â¬œ\|ğŸ”„\|âœ…" CHECKLIST_RAPIDO.md

# Atualizar status de um experimento
# â¬œ NÃ£o iniciado â†’ ğŸ”„ Em progresso â†’ âœ… Completo
```

## ğŸ¤ Contribuindo com Experimentos

Se vocÃª for executar os experimentos:

1. **Clone o ambiente**:
   ```bash
   git clone <repo>
   cd papers/02_Fairness_Framework/experimentos
   ```

2. **Siga o PLANO_EXPERIMENTOS.md** para metodologia exata

3. **Salve resultados em `results/`** seguindo estrutura:
   ```
   results/
   â”œâ”€â”€ <experimento_id>/
   â”‚   â”œâ”€â”€ raw_data.csv          # Dados brutos
   â”‚   â”œâ”€â”€ processed_data.csv    # Dados processados
   â”‚   â”œâ”€â”€ analysis.txt          # AnÃ¡lise textual
   â”‚   â””â”€â”€ figures/              # GrÃ¡ficos
   ```

4. **Documente problemas** em `issues.md`

## ğŸ“ Contato

**ResponsÃ¡vel**: [Adicionar nome e email]

**DÃºvidas sobre experimentos**: Consulte PLANO_EXPERIMENTOS.md seÃ§Ã£o correspondente

**Bugs ou issues**: Abra issue no repositÃ³rio

## ğŸ“š ReferÃªncias

### Papers Base:
- Bellamy et al. (2018) - AI Fairness 360
- Bird et al. (2020) - Fairlearn
- Saleiro et al. (2018) - Aequitas

### Metodologias:
- Brooke (1996) - System Usability Scale
- Hart & Staveland (1988) - NASA Task Load Index

### Datasets:
- COMPAS - ProPublica
- German Credit - UCI Repository
- Adult Income - UCI Repository

---

**Ãšltima atualizaÃ§Ã£o**: 2025-12-06

**Status do Projeto**: â¬œ NÃ£o iniciado

**Prazo**: SubmissÃ£o FAccT 2026 (verificar deadline exato)

**Boa sorte com os experimentos! ğŸš€**
