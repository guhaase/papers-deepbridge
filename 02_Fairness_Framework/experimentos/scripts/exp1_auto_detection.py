#!/usr/bin/env python3
"""
Experimento 1: Auto-Detec√ß√£o de Atributos Sens√≠veis

Valida as seguintes claims do paper:
- F1-Score: 0.90 (Precision: 0.92, Recall: 0.89)
- Testado em 500 datasets reais
- 100% acur√°cia nos 4 case studies

Uso:
    python exp1_auto_detection.py --n-datasets 500
    python exp1_auto_detection.py --quick  # Teste r√°pido com 50 datasets
"""

import argparse
import time
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Set, Tuple
from sklearn.metrics import precision_score, recall_score, f1_score

# Importar DeepBridge
try:
    from deepbridge import DBDataset
except ImportError:
    print("‚ö†Ô∏è  DeepBridge n√£o instalado. Instale com: pip install deepbridge")
    exit(1)


class AutoDetectionExperiment:
    """Experimento de auto-detec√ß√£o de atributos sens√≠veis."""

    def __init__(self, n_datasets: int = 500):
        self.n_datasets = n_datasets
        self.results = []
        self.ground_truth_file = Path("../data/ground_truth.csv")
        self.output_dir = Path("../results/auto_detection")
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def load_ground_truth(self) -> pd.DataFrame:
        """Carrega ground truth anotado manualmente."""
        if not self.ground_truth_file.exists():
            print(f"‚ö†Ô∏è  Ground truth n√£o encontrado em {self.ground_truth_file}")
            print("üìù Voc√™ precisa primeiro anotar os datasets manualmente.")
            print("\nFormato esperado do CSV:")
            print("dataset_name,source,sensitive_attributes")
            print("compas,kaggle,'race,sex,age'")
            print("german_credit,uci,'age,sex,foreign_worker'")
            return None

        return pd.read_csv(self.ground_truth_file)

    def run_on_dataset(
        self,
        dataset_name: str,
        df: pd.DataFrame,
        target_column: str,
        ground_truth: Set[str]
    ) -> Dict:
        """Executa auto-detec√ß√£o em um dataset."""

        start_time = time.time()

        try:
            # Criar DBDataset (ir√° auto-detectar atributos)
            dataset = DBDataset(
                data=df,
                target_column=target_column
            )

            # Obter atributos detectados
            detected = set(dataset.detected_sensitive_attributes)

            # Calcular m√©tricas
            tp = len(detected & ground_truth)
            fp = len(detected - ground_truth)
            fn = len(ground_truth - detected)
            tn = 0  # N√£o aplic√°vel neste contexto

            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

            elapsed = time.time() - start_time

            result = {
                'dataset_name': dataset_name,
                'n_features': len(df.columns),
                'n_samples': len(df),
                'ground_truth': sorted(ground_truth),
                'detected': sorted(detected),
                'tp': tp,
                'fp': fp,
                'fn': fn,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'time_seconds': elapsed,
                'status': 'success'
            }

            # Identificar falsos positivos e negativos
            false_positives = detected - ground_truth
            false_negatives = ground_truth - detected

            if false_positives:
                result['false_positives'] = sorted(false_positives)
            if false_negatives:
                result['false_negatives'] = sorted(false_negatives)

            return result

        except Exception as e:
            return {
                'dataset_name': dataset_name,
                'status': 'error',
                'error_message': str(e)
            }

    def run_all(self, datasets: List[Tuple[str, pd.DataFrame, str, Set[str]]]):
        """Executa experimento em todos datasets."""

        print(f"üöÄ Iniciando experimento de auto-detec√ß√£o")
        print(f"üìä Total de datasets: {len(datasets)}")
        print(f"üìÅ Resultados ser√£o salvos em: {self.output_dir}")
        print("-" * 60)

        for i, (name, df, target, ground_truth) in enumerate(datasets, 1):
            print(f"\n[{i}/{len(datasets)}] Processando: {name}")
            print(f"   Atributos esperados: {sorted(ground_truth)}")

            result = self.run_on_dataset(name, df, target, ground_truth)

            if result['status'] == 'success':
                print(f"   ‚úÖ Detectado: {result['detected']}")
                print(f"   üìà Precision: {result['precision']:.3f} | Recall: {result['recall']:.3f} | F1: {result['f1_score']:.3f}")

                if result.get('false_positives'):
                    print(f"   ‚ö†Ô∏è  False Positives: {result['false_positives']}")
                if result.get('false_negatives'):
                    print(f"   ‚ö†Ô∏è  False Negatives: {result['false_negatives']}")
            else:
                print(f"   ‚ùå Erro: {result['error_message']}")

            self.results.append(result)

        return self.results

    def analyze_results(self):
        """Analisa resultados agregados."""

        df_results = pd.DataFrame([r for r in self.results if r['status'] == 'success'])

        if len(df_results) == 0:
            print("\n‚ùå Nenhum resultado bem-sucedido para analisar.")
            return

        print("\n" + "=" * 60)
        print("üìä RESULTADOS AGREGADOS")
        print("=" * 60)

        # M√©tricas gerais
        precision_mean = df_results['precision'].mean()
        precision_std = df_results['precision'].std()
        recall_mean = df_results['recall'].mean()
        recall_std = df_results['recall'].std()
        f1_mean = df_results['f1_score'].mean()
        f1_std = df_results['f1_score'].std()

        print(f"\nüìà M√©tricas Gerais (N={len(df_results)}):")
        print(f"   Precision: {precision_mean:.3f} ¬± {precision_std:.3f}")
        print(f"   Recall:    {recall_mean:.3f} ¬± {recall_std:.3f}")
        print(f"   F1-Score:  {f1_mean:.3f} ¬± {f1_std:.3f}")

        # Valida√ß√£o das claims do paper
        print(f"\n‚úÖ Valida√ß√£o de Claims:")

        claim_precision = 0.92
        claim_recall = 0.89
        claim_f1 = 0.90

        precision_ok = precision_mean >= claim_precision - 0.05
        recall_ok = recall_mean >= claim_recall - 0.05
        f1_ok = f1_mean >= claim_f1 - 0.05

        print(f"   Precision ‚â• {claim_precision}: {'‚úÖ PASS' if precision_ok else '‚ùå FAIL'} (obtido: {precision_mean:.3f})")
        print(f"   Recall ‚â• {claim_recall}:    {'‚úÖ PASS' if recall_ok else '‚ùå FAIL'} (obtido: {recall_mean:.3f})")
        print(f"   F1-Score ‚â• {claim_f1}:  {'‚úÖ PASS' if f1_ok else '‚ùå FAIL'} (obtido: {f1_mean:.3f})")

        # An√°lise de erros
        print(f"\nüîç An√°lise de Erros:")

        total_fp = df_results['fp'].sum()
        total_fn = df_results['fn'].sum()
        total_tp = df_results['tp'].sum()

        print(f"   True Positives:  {total_tp}")
        print(f"   False Positives: {total_fp}")
        print(f"   False Negatives: {total_fn}")

        # Falsos positivos mais comuns
        all_fp = []
        for result in self.results:
            if result['status'] == 'success' and result.get('false_positives'):
                all_fp.extend(result['false_positives'])

        if all_fp:
            from collections import Counter
            fp_counts = Counter(all_fp)
            print(f"\n   Top 5 False Positives:")
            for attr, count in fp_counts.most_common(5):
                print(f"      - {attr}: {count}x")

        # Falsos negativos mais comuns
        all_fn = []
        for result in self.results:
            if result['status'] == 'success' and result.get('false_negatives'):
                all_fn.extend(result['false_negatives'])

        if all_fn:
            from collections import Counter
            fn_counts = Counter(all_fn)
            print(f"\n   Top 5 False Negatives:")
            for attr, count in fn_counts.most_common(5):
                print(f"      - {attr}: {count}x")

        # Salvar resultados
        output_file = self.output_dir / "auto_detection_results.csv"
        df_results.to_csv(output_file, index=False)
        print(f"\nüíæ Resultados salvos em: {output_file}")

        # Gerar relat√≥rio resumido
        summary = {
            'n_datasets': len(df_results),
            'precision_mean': precision_mean,
            'precision_std': precision_std,
            'recall_mean': recall_mean,
            'recall_std': recall_std,
            'f1_mean': f1_mean,
            'f1_std': f1_std,
            'total_tp': int(total_tp),
            'total_fp': int(total_fp),
            'total_fn': int(total_fn),
            'claim_precision_validated': precision_ok,
            'claim_recall_validated': recall_ok,
            'claim_f1_validated': f1_ok
        }

        summary_file = self.output_dir / "summary.json"
        import json
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)

        print(f"üìã Resumo salvo em: {summary_file}")

        return summary


def load_example_datasets() -> List[Tuple[str, pd.DataFrame, str, Set[str]]]:
    """Carrega datasets de exemplo para teste r√°pido."""

    print("‚ö†Ô∏è  Modo r√°pido: usando datasets sint√©ticos de exemplo")
    print("üìù Para o experimento completo, voc√™ precisa coletar 500 datasets reais\n")

    datasets = []

    # Dataset 1: COMPAS-like
    np.random.seed(42)
    n = 1000

    df1 = pd.DataFrame({
        'age': np.random.randint(18, 70, n),
        'race': np.random.choice(['White', 'Black', 'Hispanic', 'Other'], n),
        'sex': np.random.choice(['Male', 'Female'], n),
        'priors_count': np.random.randint(0, 20, n),
        'income': np.random.randint(20000, 100000, n),
        'recidivism': np.random.choice([0, 1], n)
    })

    datasets.append(('compas_synthetic', df1, 'recidivism', {'age', 'race', 'sex'}))

    # Dataset 2: Credit-like
    df2 = pd.DataFrame({
        'age_group': np.random.choice(['<25', '25-60', '>60'], n),
        'gender': np.random.choice(['M', 'F'], n),
        'foreign_worker': np.random.choice(['yes', 'no'], n),
        'credit_history': np.random.randint(1, 5, n),
        'employment': np.random.randint(0, 10, n),
        'credit_risk': np.random.choice([0, 1], n)
    })

    datasets.append(('credit_synthetic', df2, 'credit_risk', {'age_group', 'gender', 'foreign_worker'}))

    # Dataset 3: Adult-like
    df3 = pd.DataFrame({
        'age': np.random.randint(18, 80, n),
        'sex': np.random.choice(['Male', 'Female'], n),
        'education': np.random.randint(6, 16, n),
        'occupation': np.random.choice(['Tech', 'Sales', 'Service', 'Other'], n),
        'hours_per_week': np.random.randint(20, 80, n),
        'income': np.random.choice([0, 1], n)
    })

    datasets.append(('adult_synthetic', df3, 'income', {'age', 'sex'}))

    # Dataset 4: Healthcare-like
    df4 = pd.DataFrame({
        'patient_age': np.random.randint(0, 100, n),
        'ethnicity': np.random.choice(['White', 'Black', 'Hispanic', 'Asian'], n),
        'insurance': np.random.choice(['Private', 'Medicare', 'Medicaid', 'None'], n),
        'diagnosis_code': np.random.randint(1000, 9999, n),
        'readmission': np.random.choice([0, 1], n)
    })

    datasets.append(('healthcare_synthetic', df4, 'readmission', {'patient_age', 'ethnicity'}))

    # Dataset 5: Com false positives esperados (race_time)
    df5 = pd.DataFrame({
        'runner_id': np.arange(n),
        'race_time': np.random.uniform(15.0, 45.0, n),  # Tempo de corrida (N√ÉO √© atributo sens√≠vel!)
        'age_bracket': np.random.choice(['18-30', '31-50', '51+'], n),
        'gender': np.random.choice(['M', 'F', 'O'], n),
        'finish_position': np.random.randint(1, n+1, n)
    })

    datasets.append(('running_race', df5, 'finish_position', {'age_bracket', 'gender'}))

    return datasets


def main():
    parser = argparse.ArgumentParser(description='Experimento 1: Auto-Detec√ß√£o de Atributos Sens√≠veis')
    parser.add_argument('--n-datasets', type=int, default=500, help='N√∫mero de datasets (default: 500)')
    parser.add_argument('--quick', action='store_true', help='Teste r√°pido com 5 datasets sint√©ticos')

    args = parser.parse_args()

    print("=" * 60)
    print("üî¨ EXPERIMENTO 1: AUTO-DETEC√á√ÉO DE ATRIBUTOS SENS√çVEIS")
    print("=" * 60)

    if args.quick:
        # Modo r√°pido: usar datasets sint√©ticos
        datasets = load_example_datasets()
        experiment = AutoDetectionExperiment(n_datasets=len(datasets))
    else:
        # Modo completo: carregar datasets reais
        experiment = AutoDetectionExperiment(n_datasets=args.n_datasets)

        print("\n‚ö†Ô∏è  Modo completo requer:")
        print("   1. Coletar 500 datasets (Kaggle, UCI, OpenML)")
        print("   2. Anotar ground truth manualmente (2 especialistas)")
        print("   3. Salvar em ../data/ground_truth.csv")
        print("\nüí° Use --quick para teste r√°pido com datasets sint√©ticos\n")

        # Tentar carregar ground truth
        gt_df = experiment.load_ground_truth()
        if gt_df is None:
            print("‚ùå Execute primeiro com --quick ou prepare os dados.")
            return

        # TODO: Implementar carregamento dos datasets reais a partir do ground truth
        print("‚ùå Carregamento de datasets reais ainda n√£o implementado.")
        print("   Implemente a fun√ß√£o load_real_datasets() baseado no ground_truth.csv")
        return

    # Executar experimento
    results = experiment.run_all(datasets)

    # Analisar resultados
    summary = experiment.analyze_results()

    print("\n" + "=" * 60)
    print("‚úÖ EXPERIMENTO CONCLU√çDO")
    print("=" * 60)


if __name__ == '__main__':
    main()
