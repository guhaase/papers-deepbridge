# üöÄ Guia de Execu√ß√£o Passo a Passo

Guia pr√°tico para executar todos os experimentos necess√°rios para validar o paper DeepBridge Fairness.

---

## üìã Pr√©-requisitos

### 1. Ambiente Python

```bash
# Verificar vers√£o Python (necess√°rio ‚â• 3.8)
python --version

# Criar ambiente virtual
cd /home/guhaase/projetos/DeepBridge/papers/02_Fairness_Framework/experimentos
python -m venv venv
source venv/bin/activate  # Linux/Mac

# Instalar depend√™ncias
pip install --upgrade pip
pip install deepbridge
pip install aif360 fairlearn aequitas
pip install pandas numpy scipy scikit-learn
pip install matplotlib seaborn plotly
pip install jupyter notebook  # Para an√°lises interativas
pip install pytest pytest-cov  # Para testes
```

### 2. Verificar Instala√ß√£o

```bash
python -c "from deepbridge import DBDataset; print('‚úÖ DeepBridge OK')"
python -c "from aif360.datasets import BinaryLabelDataset; print('‚úÖ AIF360 OK')"
python -c "import fairlearn; print('‚úÖ Fairlearn OK')"
```

### 3. Estrutura de Diret√≥rios

```bash
# J√° foi criada automaticamente, mas verifique:
tree -L 2 experimentos/

# Deve mostrar:
# experimentos/
# ‚îú‚îÄ‚îÄ scripts/
# ‚îú‚îÄ‚îÄ data/
# ‚îú‚îÄ‚îÄ results/
# ‚îî‚îÄ‚îÄ reports/
```

---

## üéØ Fase 1: Teste R√°pido (1 dia)

**Objetivo**: Validar que tudo est√° funcionando antes de executar experimentos completos.

### Passo 1.1: Executar Auto-Detec√ß√£o em Modo R√°pido

```bash
cd scripts/
python exp1_auto_detection.py --quick
```

**Sa√≠da Esperada**:
```
üî¨ EXPERIMENTO 1: AUTO-DETEC√á√ÉO DE ATRIBUTOS SENS√çVEIS
========================================================
üöÄ Iniciando experimento de auto-detec√ß√£o
üìä Total de datasets: 5

[1/5] Processando: compas_synthetic
   Atributos esperados: ['age', 'race', 'sex']
   ‚úÖ Detectado: ['age', 'race', 'sex']
   üìà Precision: 1.000 | Recall: 1.000 | F1: 1.000
...

üìä RESULTADOS AGREGADOS
========================================================
üìà M√©tricas Gerais (N=5):
   Precision: 0.XXX ¬± 0.XXX
   Recall:    0.XXX ¬± 0.XXX
   F1-Score:  0.XXX ¬± 0.XXX

‚úÖ Valida√ß√£o de Claims:
   Precision ‚â• 0.92: ‚úÖ PASS
   Recall ‚â• 0.89:    ‚úÖ PASS
   F1-Score ‚â• 0.90:  ‚úÖ PASS
```

**‚úÖ Se passou**: Continue para pr√≥ximo passo
**‚ùå Se falhou**: Verifique instala√ß√£o do DeepBridge ou abra issue

---

## üìä Fase 2: Coleta de Dados (2-3 semanas)

**Objetivo**: Coletar 500 datasets com ground truth anotado.

### Passo 2.1: Coletar Datasets

**Fontes**:
1. **Kaggle** (200 datasets):
   ```bash
   # Instalar Kaggle CLI
   pip install kaggle

   # Configurar API key (https://www.kaggle.com/docs/api)
   mkdir -p ~/.kaggle
   cp kaggle.json ~/.kaggle/
   chmod 600 ~/.kaggle/kaggle.json

   # Buscar datasets relevantes
   kaggle datasets list -s "credit scoring"
   kaggle datasets list -s "hiring"
   kaggle datasets list -s "healthcare"
   kaggle datasets list -s "criminal justice"
   ```

2. **UCI Repository** (150 datasets):
   - Acesse: https://archive.ics.uci.edu/ml/datasets.php
   - Filtros: Classification, Tabular, >1000 samples
   - Baixe datasets relevantes para fairness

3. **OpenML** (100 datasets):
   ```python
   from sklearn.datasets import fetch_openml

   # Exemplo
   data = fetch_openml(name='adult', version=1, as_frame=True)
   ```

4. **Sint√©ticos** (50 datasets):
   - Use scripts de gera√ß√£o sint√©tica para controle

### Passo 2.2: Anotar Ground Truth

**Criar arquivo**: `data/ground_truth.csv`

**Formato**:
```csv
dataset_name,source,target_column,sensitive_attributes,n_samples,n_features
compas,kaggle,two_year_recid,"race,sex,age",7214,12
german_credit,uci,credit_risk,"age,sex,foreign_worker",1000,20
adult,uci,income,">50K,sex,race",48842,14
...
```

**Processo de Anota√ß√£o**:

1. **Revisor 1** anota todos 500 datasets
2. **Revisor 2** anota todos 500 datasets (independentemente)
3. Calcular Cohen's Kappa (deve ser > 0.85)
4. Resolver discord√¢ncias por consenso

**Script auxiliar**:
```bash
# Criar template para anota√ß√£o
python scripts/create_annotation_template.py --n-datasets 500

# Calcular inter-rater agreement
python scripts/calculate_kappa.py \
    --reviewer1 data/annotations_reviewer1.csv \
    --reviewer2 data/annotations_reviewer2.csv
```

---

## üî¨ Fase 3: Experimentos Principais (4-6 semanas)

### Semana 1-2: Auto-Detec√ß√£o

```bash
# Executar experimento completo (500 datasets)
python scripts/exp1_auto_detection.py --n-datasets 500

# Analisar resultados
jupyter notebook analysis/exp1_analysis.ipynb

# Verificar se passou nos crit√©rios
grep "PASS" results/auto_detection/summary.json
```

**Crit√©rios de Sucesso**:
- [ ] F1-Score ‚â• 0.85 (target: 0.90)
- [ ] Precision ‚â• 0.90
- [ ] Recall ‚â• 0.85
- [ ] Kappa inter-rater ‚â• 0.85

### Semana 3: Verifica√ß√£o EEOC/ECOA

```bash
# Executar testes de conformidade
python scripts/exp3_eeoc_validation.py

# Verificar 100% precis√£o
cat results/eeoc_validation/summary.txt
```

**Crit√©rios de Sucesso**:
- [ ] 100% precis√£o em regra 80% (0 erros)
- [ ] 100% precis√£o em Question 21
- [ ] 0 falsos positivos

### Semana 4-6: Case Studies

```bash
# COMPAS
python scripts/exp4_case_studies.py --dataset compas

# German Credit
python scripts/exp4_case_studies.py --dataset german_credit

# Adult Income
python scripts/exp4_case_studies.py --dataset adult

# Healthcare
python scripts/exp4_case_studies.py --dataset healthcare

# Ou executar todos de uma vez
python scripts/exp4_case_studies.py --all
```

**Crit√©rios de Sucesso** (cada dataset):
- [ ] Tempo de an√°lise documentado
- [ ] Atributos detectados corretamente
- [ ] Viola√ß√µes identificadas
- [ ] Threshold √≥timo calculado
- [ ] Relat√≥rio gerado

---

## üë• Fase 4: Estudo de Usabilidade (3-4 semanas)

### Semana 1: Recrutamento

**Perfil dos Participantes**:
- Data Scientists ou ML Engineers
- 2-8 anos de experi√™ncia em ML
- Pelo menos 65% com experi√™ncia em fairness tools
- N = 20 participantes

**Canais de Recrutamento**:
- LinkedIn (grupos de ML)
- Twitter (#MachineLearning #ResponsibleAI)
- Confer√™ncias (NeurIPS, ICML, FAccT)
- Empresas parceiras

**Incentivos**:
- $50 Amazon gift card
- Co-autoria em acknowledgments
- Early access to tool

### Semana 2-3: Execu√ß√£o

**Protocol** (60 minutos por participante):

1. **Briefing** (5 min):
   - Explicar objetivo do estudo
   - Obter consentimento informado
   - Configurar screen recording

2. **Setup** (10 min):
   - Instalar DeepBridge
   - Carregar dataset Adult Income
   - Verificar ambiente funcional

3. **Tarefas** (35 min):
   - **Task 1** (15 min): Detectar bias em modelo
   - **Task 2** (10 min): Verificar EEOC compliance
   - **Task 3** (10 min): Encontrar threshold √≥timo

4. **Question√°rios** (10 min):
   - System Usability Scale (SUS)
   - NASA Task Load Index (TLX)
   - Perguntas demogr√°ficas

5. **Entrevista** (10 min):
   - "O que voc√™ mais gostou?"
   - "O que foi mais dif√≠cil?"
   - "O que voc√™ mudaria?"

**Executar**:
```bash
# Para cada participante
python scripts/exp5_usability.py --participant-id P01

# Isso ir√°:
# 1. Gerar instru√ß√µes personalizadas
# 2. Cronometrar tarefas
# 3. Coletar m√©tricas
# 4. Salvar resultados em results/usability/P01/
```

### Semana 4: An√°lise

```bash
# Calcular SUS scores
python scripts/analyze_sus.py --input results/usability/

# Calcular TLX
python scripts/analyze_tlx.py --input results/usability/

# An√°lise qualitativa (entrevistas)
python scripts/thematic_analysis.py --transcripts results/usability/*/interview.txt
```

**Crit√©rios de Sucesso**:
- [ ] N ‚â• 15 participantes completaram
- [ ] SUS m√©dio ‚â• 75 (target: 85.2)
- [ ] Taxa de sucesso ‚â• 85% (target: 95%)
- [ ] NASA-TLX ‚â§ 40 (target: 32.1)

---

## ‚ö° Fase 5: Performance (1-2 semanas)

### Passo 5.1: Configurar Hardware

**Op√ß√£o 1: AWS** (recomendado para reprodutibilidade)
```bash
# Lan√ßar inst√¢ncia m5.2xlarge
aws ec2 run-instances \
    --image-id ami-xxxxx \
    --instance-type m5.2xlarge \
    --key-name my-key

# SSH na inst√¢ncia
ssh -i my-key.pem ubuntu@<instance-ip>

# Setup
git clone <repo>
cd experimentos
source setup_aws.sh
```

**Op√ß√£o 2: Local** (se tiver hardware equivalente)
- 8 CPUs
- 32GB RAM
- SSD storage

### Passo 5.2: Executar Benchmarks

```bash
# Todos os tamanhos (Small, Medium, Large)
python scripts/exp6_performance.py --all-sizes

# Isso ir√° executar 5 repeti√ß√µes de cada:
# - Small: 1K amostras, 20 features
# - Medium: 50K amostras, 50 features
# - Large: 500K amostras, 100 features
```

**Dura√ß√£o Estimada**: 8-12 horas

**Crit√©rios de Sucesso**:
- [ ] Speedup Small ‚â• 3.5x
- [ ] Speedup Medium ‚â• 2.5x
- [ ] Speedup Large ‚â• 2.0x
- [ ] Redu√ß√£o mem√≥ria ‚â• 35%

---

## üîÑ Fase 6: Compara√ß√£o com Ferramentas (1 semana)

### Passo 6.1: Instalar Ferramentas

```bash
pip install aif360==0.2.9
pip install fairlearn==0.10.0
pip install aequitas==2.0.0
```

### Passo 6.2: Executar Compara√ß√£o

```bash
# Feature comparison
python scripts/exp8_comparison.py --tools all --test-features

# Metric accuracy comparison
python scripts/exp8_comparison.py --tools all --test-accuracy
```

**Crit√©rios de Sucesso**:
- [ ] DeepBridge tem todas features claimed
- [ ] Outras ferramentas N√ÉO t√™m features exclusivas claimed
- [ ] Diferen√ßa de m√©tricas < 1%

---

## üìä Fase 7: An√°lise e Relat√≥rios (1-2 semanas)

### Passo 7.1: Gerar Relat√≥rios

```bash
# Relat√≥rio consolidado
python scripts/generate_reports.py \
    --experiments all \
    --output reports/experiment_summary.pdf

# Figuras para o paper
python scripts/generate_figures.py \
    --output reports/figures/ \
    --format pdf,png

# Tabelas LaTeX
python scripts/generate_tables.py \
    --output reports/tables/ \
    --format latex
```

### Passo 7.2: Validar Checklist

```bash
# Validar todas as claims
python scripts/validate_claims.py --checklist CHECKLIST_RAPIDO.md

# Sa√≠da esperada:
# ‚úÖ Claim 1: Auto-detec√ß√£o F1=0.90 - VALIDATED
# ‚úÖ Claim 2: 100% acur√°cia case studies - VALIDATED
# ...
```

### Passo 7.3: Preparar Reproduction Package

```bash
# Criar package para submission
python scripts/create_reproduction_package.py \
    --include scripts,data,results \
    --output reproduction_package.zip

# Conte√∫do:
# - README.md com instru√ß√µes
# - Scripts completos
# - Dados (se permitido por licen√ßa)
# - Resultados agregados
# - Requirements.txt
```

---

## ‚ö†Ô∏è Troubleshooting

### Problema: Auto-detec√ß√£o F1 < 0.85

**Diagn√≥stico**:
```bash
python scripts/debug_auto_detection.py --analyze-errors
```

**Solu√ß√µes**:
1. Ajustar threshold de similaridade
2. Expandir dicion√°rio de sin√¥nimos
3. Melhorar context filtering
4. Revisar ground truth (poss√≠veis erros de anota√ß√£o)

### Problema: SUS < 75

**Diagn√≥stico**:
```bash
python scripts/analyze_usability_issues.py --detailed
```

**Solu√ß√µes**:
1. Melhorar documenta√ß√£o
2. Adicionar tutoriais
3. Simplificar API
4. Mais exemplos pr√°ticos

### Problema: Speedup < 2.0x

**Diagn√≥stico**:
```bash
python scripts/profile_performance.py --component threshold_opt
```

**Solu√ß√µes**:
1. Otimizar threshold optimization (grid search esparso)
2. Paralelizar c√°lculos
3. Melhorar caching
4. Usar numba/cython para hot paths

### Problema: Recrutamento < 15 participantes

**A√ß√µes**:
1. Aumentar incentivos ($75)
2. Estender prazo de recrutamento
3. Recrutar em mais canais
4. Fazer estudo piloto (N=10) + validation (N=5)

---

## üìû Suporte

### D√∫vidas T√©cnicas:
- Consulte `PLANO_EXPERIMENTOS.md` se√ß√£o espec√≠fica
- Abra issue no reposit√≥rio
- Email: [seu-email]

### Problemas com Scripts:
```bash
# Ativar modo debug
export DEBUG=1
python scripts/exp1_auto_detection.py --quick --verbose
```

### Issues Conhecidos:
- Ver `issues.md` para lista atualizada

---

## ‚úÖ Checklist Final

Antes de submeter o paper, verifique:

### Experimentos:
- [ ] Auto-detec√ß√£o: 500 datasets, F1 ‚â• 0.85
- [ ] EEOC/ECOA: 100% precis√£o
- [ ] Case Studies: 4/4 completos
- [ ] Usabilidade: N ‚â• 15, SUS ‚â• 75
- [ ] Performance: Speedup ‚â• 2.0x
- [ ] Compara√ß√£o: 3 ferramentas testadas

### Artefatos:
- [ ] Todos resultados em `results/`
- [ ] Figuras em `reports/figures/`
- [ ] Tabelas em `reports/tables/`
- [ ] Reproduction package criado
- [ ] README atualizado

### Documenta√ß√£o:
- [ ] Metodologia documentada
- [ ] Resultados documentados
- [ ] Limita√ß√µes documentadas
- [ ] IRB approval (se necess√°rio)
- [ ] Licen√ßas de dados verificadas

### Paper:
- [ ] Se√ß√£o 5 (Evaluation) atualizada com resultados
- [ ] Figuras inseridas
- [ ] Tabelas inseridas
- [ ] Claims validadas
- [ ] Ap√™ndice t√©cnico inclu√≠do

---

**Boa sorte! üöÄ**

**Estimativa Total de Tempo**: 12-18 semanas (3-4.5 meses)

**Pr√≥ximos Passos Imediatos**:
1. ‚úÖ Executar teste r√°pido (`python exp1_auto_detection.py --quick`)
2. üìä Iniciar coleta de datasets
3. üë• Come√ßar recrutamento para usabilidade
4. üìÖ Revisar timeline e ajustar se necess√°rio
