# ğŸ“‘ Ãndice Completo - Experimentos DeepBridge Fairness

Ãndice de todos os arquivos criados e sua finalidade.

---

## ğŸ“„ DocumentaÃ§Ã£o Principal

### 1. [RESUMO_EXECUTIVO.md](RESUMO_EXECUTIVO.md) â­ **LEIA PRIMEIRO**
- VisÃ£o geral de 15 claims a validar
- Timeline de 18 semanas
- Recursos necessÃ¡rios (~$1,300)
- Riscos e mitigaÃ§Ãµes
- Dashboard de progresso

### 2. [PLANO_EXPERIMENTOS.md](PLANO_EXPERIMENTOS.md) ğŸ“‹ **DOCUMENTO MASTER**
- **17 seÃ§Ãµes detalhadas** cobrindo:
  - 8 grupos de experimentos principais
  - Metodologias step-by-step
  - MÃ©tricas de validaÃ§Ã£o
  - CritÃ©rios de sucesso
  - Timeline detalhado
  - ContingÃªncias e mitigaÃ§Ãµes

### 3. [GUIA_EXECUCAO.md](GUIA_EXECUCAO.md) ğŸš€ **PASSO A PASSO**
- Setup do ambiente (Python, deps)
- ExecuÃ§Ã£o fase por fase
- Comandos exatos
- Troubleshooting comum
- Checklist final

### 4. [CHECKLIST_RAPIDO.md](CHECKLIST_RAPIDO.md) âœ… **TRACKING DIÃRIO**
- 6 experimentos crÃ­ticos
- Tabela de validaÃ§Ã£o de claims
- Red flags e aÃ§Ãµes
- Timeline resumido (8-18 semanas)

### 5. [README.md](README.md) ğŸ“– **OVERVIEW**
- Estrutura de arquivos
- Quick start
- Claims principais
- CritÃ©rios mÃ­nimos para publicaÃ§Ã£o

---

## ğŸ Scripts Python

### Experimentos Principais

#### 1. [scripts/exp1_auto_detection.py](scripts/exp1_auto_detection.py)
**Experimento 1: Auto-DetecÃ§Ã£o de Atributos SensÃ­veis**
- Testa em 500 datasets
- Valida F1-Score â‰¥ 0.90
- AnÃ¡lise de erros (FP/FN)
- Uso: `python exp1_auto_detection.py --quick`

#### 2. [scripts/exp3_eeoc_validation.py](scripts/exp3_eeoc_validation.py)
**Experimento 3: VerificaÃ§Ã£o EEOC/ECOA**
- Testa Regra 80% (10 casos controlados)
- Testa Question 21 (7 casos)
- Valida Adverse Action Notices
- **CRÃTICO**: 100% precisÃ£o obrigatÃ³ria
- Uso: `python exp3_eeoc_validation.py`

#### 3. [scripts/exp4_case_studies.py](scripts/exp4_case_studies.py)
**Experimento 4: Case Studies**
- COMPAS (recidivism)
- German Credit (credit scoring)
- Adult Income (employment)
- Healthcare (readmission)
- Valida tempo de anÃ¡lise
- Uso: `python exp4_case_studies.py --dataset compas`

### Scripts Auxiliares

#### 4. [scripts/utils.py](scripts/utils.py)
**Utilidades Comuns**
- `timer()`: Context manager para medir tempo
- `save_json()`, `save_csv()`: Helpers de I/O
- `create_synthetic_dataset()`: GeraÃ§Ã£o de dados sintÃ©ticos
- `validate_claim()`: ValidaÃ§Ã£o de claims
- `ExperimentLogger`: Logger estruturado
- `check_dependencies()`: Verifica instalaÃ§Ãµes

#### 5. [scripts/calculate_inter_rater_agreement.py](scripts/calculate_inter_rater_agreement.py)
**AnÃ¡lise de ConcordÃ¢ncia entre Anotadores**
- Calcula Cohen's Kappa
- Valida Kappa â‰¥ 0.85
- Identifica discordÃ¢ncias
- Uso: `python calculate_inter_rater_agreement.py --reviewer1 r1.csv --reviewer2 r2.csv`

---

## âš™ï¸ ConfiguraÃ§Ã£o e Setup

### 1. [requirements.txt](requirements.txt)
DependÃªncias Python:
- Core: `deepbridge`, `pandas`, `numpy`, `scipy`
- ML: `scikit-learn`, `xgboost`, `lightgbm`
- Fairness: `aif360`, `fairlearn`, `aequitas`
- Viz: `matplotlib`, `seaborn`, `plotly`
- Testes: `pytest`

Instalar: `pip install -r requirements.txt`

### 2. [setup.sh](setup.sh)
Script automatizado de setup:
- Cria venv
- Instala dependÃªncias
- Cria diretÃ³rios
- Testa instalaÃ§Ã£o
- Uso: `chmod +x setup.sh && ./setup.sh`

---

## ğŸ“Š Dados e Templates

### 1. [data/ground_truth_template.csv](data/ground_truth_template.csv)
Template para anotaÃ§Ã£o manual:
- Colunas: dataset_name, source, n_samples, sensitive_attributes
- Exemplos: COMPAS, German Credit, Adult, Healthcare
- Use como base para anotar 500 datasets

### Estrutura de DiretÃ³rios

```
data/
â”œâ”€â”€ ground_truth.csv              # AnotaÃ§Ãµes finalizadas (500 datasets)
â”œâ”€â”€ annotations_reviewer1.csv     # AnotaÃ§Ãµes do revisor 1
â”œâ”€â”€ annotations_reviewer2.csv     # AnotaÃ§Ãµes do revisor 2
â”œâ”€â”€ case_studies/                 # Datasets dos case studies
â”‚   â”œâ”€â”€ compas.csv
â”‚   â”œâ”€â”€ german_credit.csv
â”‚   â”œâ”€â”€ adult.csv
â”‚   â””â”€â”€ healthcare.csv
â””â”€â”€ synthetic/                    # Datasets sintÃ©ticos para testes
```

---

## ğŸ“ˆ Resultados

Estrutura onde resultados sÃ£o salvos:

```
results/
â”œâ”€â”€ auto_detection/
â”‚   â”œâ”€â”€ auto_detection_results.csv
â”‚   â”œâ”€â”€ summary.json
â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â””â”€â”€ false_positives_analysis.txt
â”œâ”€â”€ eeoc_validation/
â”‚   â”œâ”€â”€ eeoc_80_rule_validation.csv
â”‚   â”œâ”€â”€ eeoc_question_21_validation.csv
â”‚   â”œâ”€â”€ adverse_action_notices_sample.json
â”‚   â””â”€â”€ summary.json
â”œâ”€â”€ case_studies/
â”‚   â”œâ”€â”€ compas_result.json
â”‚   â”œâ”€â”€ german_credit_result.json
â”‚   â”œâ”€â”€ adult_income_result.json
â”‚   â”œâ”€â”€ healthcare_result.json
â”‚   â””â”€â”€ case_studies_summary.csv
â”œâ”€â”€ usability/
â”‚   â”œâ”€â”€ sus_scores.csv
â”‚   â”œâ”€â”€ tlx_scores.csv
â”‚   â””â”€â”€ P01/, P02/, ... P20/     # Por participante
â”œâ”€â”€ performance/
â”‚   â””â”€â”€ performance_benchmarks.csv
â””â”€â”€ comparison/
    â””â”€â”€ tool_comparison_matrix.csv
```

---

## ğŸ“Š RelatÃ³rios

```
reports/
â”œâ”€â”€ experiment_summary.pdf        # Resumo consolidado
â”œâ”€â”€ reproduction_guide.md         # Como reproduzir
â””â”€â”€ figures/                      # Figuras para o paper
    â”œâ”€â”€ auto_detection_f1.png
    â”œâ”€â”€ sus_scores.png
    â”œâ”€â”€ performance_speedup.png
    â””â”€â”€ ...
```

---

## ğŸ¯ Fluxo de Trabalho Recomendado

### Fase 1: Setup Inicial (Dia 1)
```bash
# 1. Ler documentaÃ§Ã£o
cat RESUMO_EXECUTIVO.md
cat GUIA_EXECUCAO.md

# 2. Setup ambiente
./setup.sh

# 3. Teste rÃ¡pido
cd scripts/
python exp1_auto_detection.py --quick
python exp3_eeoc_validation.py
```

### Fase 2: Coleta de Dados (Semanas 1-2)
```bash
# 1. Coletar 500 datasets (Kaggle, UCI, OpenML)
# 2. Anotar ground truth (2 revisores)
# 3. Calcular concordÃ¢ncia
python scripts/calculate_inter_rater_agreement.py \
    --reviewer1 data/annotations_reviewer1.csv \
    --reviewer2 data/annotations_reviewer2.csv
```

### Fase 3: Experimentos Core (Semanas 3-9)
```bash
# Experimento 1: Auto-detecÃ§Ã£o
python scripts/exp1_auto_detection.py --n-datasets 500

# Experimento 3: EEOC/ECOA
python scripts/exp3_eeoc_validation.py

# Experimento 4: Case Studies
python scripts/exp4_case_studies.py --dataset all
```

### Fase 4: Usabilidade (Semanas 10-13)
```bash
# Recrutamento â†’ ExecuÃ§Ã£o â†’ AnÃ¡lise
# (Scripts para exp5_usability.py ainda nÃ£o criados)
```

### Fase 5: ValidaÃ§Ã£o (Semanas 14-16)
```bash
# Performance, ComparaÃ§Ã£o, Robustness
# (Scripts ainda nÃ£o criados)
```

### Fase 6: FinalizaÃ§Ã£o (Semanas 17-18)
```bash
# Gerar relatÃ³rios
python scripts/generate_reports.py --experiments all

# Criar reproduction package
python scripts/create_reproduction_package.py
```

---

## âœ… Checklist de Arquivos Criados

### DocumentaÃ§Ã£o (6 arquivos)
- [x] RESUMO_EXECUTIVO.md
- [x] PLANO_EXPERIMENTOS.md
- [x] GUIA_EXECUCAO.md
- [x] CHECKLIST_RAPIDO.md
- [x] README.md
- [x] INDEX.md (este arquivo)

### Scripts Principais (3 arquivos)
- [x] exp1_auto_detection.py
- [x] exp3_eeoc_validation.py
- [x] exp4_case_studies.py

### Scripts Auxiliares (2 arquivos)
- [x] utils.py
- [x] calculate_inter_rater_agreement.py

### ConfiguraÃ§Ã£o (3 arquivos)
- [x] requirements.txt
- [x] setup.sh
- [x] data/ground_truth_template.csv

### Estrutura de DiretÃ³rios
- [x] scripts/
- [x] data/ (case_studies/, synthetic/)
- [x] results/ (6 subdiretÃ³rios)
- [x] reports/ (figures/)

**Total: 14 arquivos + estrutura de diretÃ³rios** âœ…

---

## ğŸš§ PrÃ³ximas ImplementaÃ§Ãµes (TODO)

### Scripts Faltantes

1. **exp2_metrics_coverage.py**
   - Validar 15 mÃ©tricas (4 prÃ© + 11 pÃ³s)
   - Comparar com cÃ¡lculo manual
   - Edge cases

2. **exp5_usability.py**
   - Protocol para participantes
   - Coleta de SUS/TLX
   - AnÃ¡lise de tarefas

3. **exp6_performance.py**
   - Benchmarks (Small/Medium/Large)
   - Speedup vs manual
   - Memory profiling

4. **exp7_threshold_optimization.py**
   - Pareto frontier validation
   - Threshold recommendations

5. **exp8_comparison.py**
   - Feature matrix (AIF360, Fairlearn, Aequitas)
   - Metric accuracy comparison

6. **exp9_edge_cases.py**
   - Dataset pequeno (n=50)
   - Desbalanceado extremo (99:1)
   - Missing values
   - Multiclass

7. **generate_reports.py**
   - Consolidar todos resultados
   - Gerar figuras para paper
   - LaTeX tables

8. **create_reproduction_package.py**
   - Zip com scripts + dados + README
   - Para submission

---

## ğŸ“ Suporte e Troubleshooting

### Problemas Comuns

**1. Import Error: No module named 'deepbridge'**
```bash
pip install deepbridge
# ou
pip install -r requirements.txt
```

**2. Experimento falhou: Dataset nÃ£o encontrado**
- Verifique se o arquivo existe em `data/case_studies/`
- Use `--quick` para gerar dados sintÃ©ticos

**3. Kappa < 0.85**
- Revisar guidelines de anotaÃ§Ã£o
- Re-anotar datasets com discordÃ¢ncia
- Consultar `GUIA_EXECUCAO.md` seÃ§Ã£o Troubleshooting

**4. Tempo excedeu target**
- Normal em primeira execuÃ§Ã£o (overhead)
- Rode mÃºltiplas vezes e calcule mÃ©dia
- Otimize cÃ³digo se necessÃ¡rio

### Onde Encontrar Ajuda

- **Metodologia**: `PLANO_EXPERIMENTOS.md` seÃ§Ã£o especÃ­fica
- **ExecuÃ§Ã£o**: `GUIA_EXECUCAO.md`
- **Tracking**: `CHECKLIST_RAPIDO.md`
- **CÃ³digo**: ComentÃ¡rios inline nos scripts

---

## ğŸ“Š Status do Projeto

**Ãšltima AtualizaÃ§Ã£o**: 2025-12-06

**Fase Atual**: Planejamento completo âœ…

**PrÃ³ximos Passos**:
1. Setup ambiente (`./setup.sh`)
2. Teste rÃ¡pido (`python exp1_auto_detection.py --quick`)
3. Coletar 500 datasets
4. Iniciar Experimento 1

**Progresso Geral**: 0% (planejamento 100%)

---

## ğŸ“ Para CitaÃ§Ã£o

Se vocÃª usar este framework de experimentos, por favor cite:

```bibtex
@misc{deepbridge_fairness_experiments2025,
  title={Experimental Framework for DeepBridge Fairness Validation},
  author={[Seu Nome]},
  year={2025},
  note={Framework para validaÃ§Ã£o de claims do paper DeepBridge Fairness}
}
```

---

**Boa sorte com os experimentos! ğŸš€**

**QuestÃµes**: Consulte `README.md` ou abra issue no repositÃ³rio.
