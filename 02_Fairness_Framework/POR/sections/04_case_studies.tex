\section{Case Studies}
\label{sec:case_studies}

Demonstramos a eficácia do DeepBridge Fairness através de quatro estudos de caso representando domínios regulados: justiça criminal (COMPAS), crédito (German Credit), contratação (Adult Income), e saúde (Healthcare). Para cada caso, reportamos: (1) violações detectadas, (2) conformidade EEOC/ECOA, (3) threshold ótimo, e (4) tempo de análise.

\subsection{Case Study 1: COMPAS -- Recidivism Prediction}

\subsubsection{Contexto}

COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) é um sistema de predição de risco de reincidência amplamente usado no sistema judicial dos EUA. ProPublica investigou o sistema e encontrou bias racial~\cite{angwin2016machine}.

\textbf{Dataset}: 7,214 réus de Broward County, Florida (2013-2014)
\begin{itemize}
    \item \textbf{Target}: recidivou em 2 anos (binary)
    \item \textbf{Features}: 12 (idade, gênero, raça, histórico criminal)
    \item \textbf{Atributos Sensíveis}: race (African-American, Caucasian, Hispanic, Other), gender (Male, Female)
    \item \textbf{Modelo}: Random Forest Classifier (baseline para replicar bias original)
\end{itemize}

\subsubsection{Análise DeepBridge}

\textbf{Auto-Detecção}:
\begin{lstlisting}
dataset = DBDataset(df_compas, target='two_year_recid', model=rf_model)
print(dataset.detected_sensitive_attributes)
# ['race', 'sex', 'age']  # 100% acurácia
\end{lstlisting}

\textbf{Métricas Pré-Treinamento}:
\begin{itemize}
    \item \textbf{Class Balance (race)}: 0.67 [WARNING] -- African-Americans têm 1.5x taxa base de recidivismo (confounding histórico)
    \item \textbf{KL Divergence}: 0.23 -- Distribuições de features diferem significativamente entre raças
\end{itemize}

\textbf{Métricas Pós-Treinamento} (threshold default 0.5):

\begin{table}[h]
\centering
\caption{Métricas de fairness COMPAS por raça (threshold 0.5)}
\label{tab:compas_metrics}
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Métrica} & \textbf{African-American} & \textbf{Caucasian} & \textbf{Diferença} \\
\midrule
Statistical Parity & 0.59 & 0.38 & 0.21 [VIOLATION] \\
Disparate Impact & 1.55 & 1.00 & -- \\
Equal Opportunity & 0.72 & 0.65 & 0.07 \\
FNR Difference & 0.28 & 0.35 & -0.07 \\
FPR Difference & 0.45 & 0.23 & 0.22 [VIOLATION] \\
Precision & 0.63 & 0.71 & -0.08 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Violações Detectadas}:
\begin{enumerate}
    \item \textbf{Statistical Parity}: 21pp de diferença (threshold: <10pp)
    \item \textbf{Disparate Impact}: DI=1.55 (não viola regra 80\%, mas favorece African-Americans em seleção)
    \item \textbf{FPR Difference}: 22pp -- African-Americans têm 2x taxa de False Positives (o bias crítico identificado por ProPublica)
\end{enumerate}

\textbf{Verificação EEOC}:
\begin{verbatim}
EEOC 80% Rule: NOT APPLICABLE (sistema não é "selection")
Note: COMPAS não é sistema de contratação, mas sistema de
assessment de risco. Regra 80% não se aplica formalmente.

Fairness Concern: Equalized Odds violado (FPR disparity)
recomendação: Equalizar FPR via threshold adjustment
\end{verbatim}

\textbf{Otimização de Threshold}:

DeepBridge identificou threshold ótimo = \textbf{0.62} que:
\begin{itemize}
    \item Reduz FPR difference de 22pp → 8pp
    \item Mantém accuracy acima 68\%
    \item Equalized Odds: EOdds = 0.09 (< threshold 0.10)
\end{itemize}

\textbf{Tempo de Análise}: \textbf{7.2 minutos} (vs. 35 minutos com AI Fairness 360 + análise manual)

\subsection{Case Study 2: German Credit -- Credit Scoring}

\subsubsection{Contexto}

German Credit dataset é benchmark clássico para credit scoring~\cite{dua2017uci}. Aplicável a ECOA (Equal Credit Opportunity Act).

\textbf{Dataset}: 1,000 clientes de banco alemão
\begin{itemize}
    \item \textbf{Target}: bom crédito (binary)
    \item \textbf{Features}: 20 (idade, estado civil, histórico de crédito, emprego)
    \item \textbf{Atributos Sensíveis}: age (< 25, 25-60, >60), sex (male, female), foreign\_worker (yes, no)
    \item \textbf{Modelo}: XGBoost Classifier
\end{itemize}

\subsubsection{Análise DeepBridge}

\textbf{Auto-Detecção}:
\begin{lstlisting}
dataset = DBDataset(df_credit, target='credit_risk', model=xgb_model)
print(dataset.detected_sensitive_attributes)
# ['age', 'sex', 'foreign_worker']  # 100% acurácia
\end{lstlisting}

\textbf{Métricas Pós-Treinamento} (por idade):

\begin{table}[h]
\centering
\caption{Métricas de fairness German Credit por idade (threshold 0.5)}
\label{tab:credit_metrics}
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Métrica} & \textbf{<25} & \textbf{25-60} & \textbf{>60} \\
\midrule
Approval Rate & 0.52 & 0.71 & 0.68 \\
Disparate Impact & 0.73 [VIOLATION] & 1.00 & 0.96 \\
Equal Opportunity & 0.58 & 0.72 & 0.70 \\
Precision & 0.65 & 0.78 & 0.75 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Verificação ECOA}:
\begin{verbatim}
ECOA Compliance Check:
- Age <25: DI = 0.73 [VIOLATION OF 80% RULE]
- Selection rate: 52% vs. 71% (reference)
- Shortfall: 7pp to reach 80% threshold

Action Required:
- Adjust threshold OR retrain model with fairness constraints
- Generate adverse action notices for denied applicants

Sample Adverse Action Notice:
"Your credit application was denied. Primary reasons:
  1. Insufficient credit history (score: 320/800)
  2. High debt-to-income ratio (45% vs. recommended <36%)"
\end{verbatim}

\textbf{Threshold Optimization}:

Pareto frontier identificou 3 thresholds candidatos:
\begin{enumerate}
    \item \textbf{t=0.38}: DI=0.82 [COMPLIANT], Accuracy=69\%
    \item \textbf{t=0.45}: DI=0.80 [BARELY COMPLIANT], Accuracy=72\%
    \item \textbf{t=0.50}: DI=0.73 [VIOLATION], Accuracy=74\%
\end{enumerate}

\textbf{Recomendação}: t=0.45 balanceia conformidade ECOA com performance aceitável.

\textbf{Tempo de Análise}: \textbf{5.8 minutos}

\subsection{Case Study 3: Adult Income -- Employment Screening}

\subsubsection{Contexto}

Adult Income dataset (UCI) prediz se indivíduo ganha >50K/ano~\cite{dua2017uci}. Comumente usado como proxy para decisões de contratação (EEOC applicable).

\textbf{Dataset}: 48,842 indivíduos do US Census (1994)
\begin{itemize}
    \item \textbf{Target}: income >50K (binary)
    \item \textbf{Features}: 14 (idade, educação, ocupação, raça, sexo, país de origem)
    \item \textbf{Atributos Sensíveis}: sex (Male, Female), race (White, Black, Asian-Pac-Islander, Amer-Indian-Eskimo, Other)
    \item \textbf{Modelo}: LightGBM Classifier
\end{itemize}

\subsubsection{Análise DeepBridge}

\textbf{Métricas Pós-Treinamento} (por sexo):

\begin{table}[h]
\centering
\caption{Métricas de fairness Adult Income por sexo (threshold 0.5)}
\label{tab:adult_metrics}
\small
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Métrica} & \textbf{Female} & \textbf{Male} \\
\midrule
Predicted High Income \% & 14.2\% & 32.8\% \\
Disparate Impact & 0.43 [VIOLATION] & 1.00 \\
Equal Opportunity & 0.48 & 0.71 \\
Equalized Odds & 0.23 [VIOLATION] & -- \\
Accuracy & 83.5\% & 85.2\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Verificação EEOC}:
\begin{verbatim}
EEOC 80% Rule Verification:
- Female: DI = 0.43 [SEVERE VIOLATION]
- Selection rate: 14.2% vs. 32.8% (Male)
- Shortfall: 37pp to reach 80% threshold

EEOC Question 21:
- Female: 32.4% representation [VALID]
- Male: 67.6% representation [VALID]

Risk Assessment: HIGH
- Severe disparate impact
- Would likely face EEOC challenge if deployed
\end{verbatim}

\textbf{Análise de Causa-Raiz}:

DeepBridge analisa feature importance por grupo:
\begin{itemize}
    \item \textbf{Female}: Top features = [education, hours\_per\_week, occupation]
    \item \textbf{Male}: Top features = [occupation, age, capital\_gain]
    \item \textbf{Bias Source}: ``occupation'' é proxy de gender (enfermeiras=F, engenheiros=M)
\end{itemize}

\textbf{Recomendação de Mitigação}:
\begin{enumerate}
    \item \textbf{Threshold adjustment}: Insuficiente (DI max = 0.65 mesmo com t=0.1)
    \item \textbf{Reweighting}: Treinar com sample weights balanceando grupos
    \item \textbf{Adversarial debiasing}: Adicionar adversary penalizando predições de gender
\end{enumerate}

\textbf{Tempo de Análise}: \textbf{12.4 minutos} (dataset maior)

\subsection{Case Study 4: Healthcare Risk Prediction}

\subsubsection{Contexto}

Predição de risco de readmissão hospitalar em 30 dias. Regulado por HIPAA e em breve por AI Act (EU).

\textbf{Dataset}: 10,000 pacientes de hospital (dados sintéticos baseados em MIMIC-III)
\begin{itemize}
    \item \textbf{Target}: readmissão em 30 dias (binary)
    \item \textbf{Features}: 25 (idade, raça, diagnósticos, comorbidades)
    \item \textbf{Atributos Sensíveis}: race (White, Black, Hispanic, Asian), age\_group (<50, 50-70, >70)
    \item \textbf{Modelo}: Neural Network (3 layers, 128-64-32 neurons)
\end{itemize}

\subsubsection{Análise DeepBridge}

\textbf{Métricas Pós-Treinamento} (por raça):

\begin{table}[h]
\centering
\caption{Métricas de fairness Healthcare por raça (threshold 0.5)}
\label{tab:health_metrics}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Métrica} & \textbf{White} & \textbf{Black} & \textbf{Hispanic} & \textbf{Asian} \\
\midrule
Predicted Readmission & 22\% & 31\% & 28\% & 19\% \\
Disparate Impact & 1.00 & 1.41 & 1.27 & 0.86 \\
Equal Opportunity & 0.68 & 0.75 & 0.71 & 0.65 \\
FNR (miss risk) & 0.32 & 0.25 & 0.29 & 0.35 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Questão Ética Crítica}:

Modelo prediz \textit{maior} risco para Black/Hispanic patients. Causas possíveis:
\begin{enumerate}
    \item \textbf{Bias histórico}: Disparidades reais em acesso a cuidados de saúde (modelo reflete realidade injusta)
    \item \textbf{Proxy features}: Zip code, insurance type são proxies de raça
    \item \textbf{Label bias}: Readmissões podem ser influenciadas por bias de médicos em admissões
\end{enumerate}

\textbf{Recomendação DeepBridge}:
\begin{verbatim}
WARNING: Clinical Context Required
- Higher predicted risk for minority groups detected
- Possible causes: (1) legitimate health disparities OR
  (2) biased features/labels
- Action: Clinical review of feature importance
- Consider: Remove zip_code, insurance_type
- Monitor: Real-world outcomes by race after deployment
\end{verbatim}

\textbf{Threshold Optimization}: NÃO RECOMENDADO neste caso
\begin{itemize}
    \item Ajustar threshold pode \textit{reduzir} detecção de risco em grupos vulneráveis
    \item Potencial dano: Pacientes de alto risco não recebem intervenções preventivas
    \item Abordagem preferida: Mitigação via feature engineering, não threshold
\end{itemize}

\textbf{Tempo de Análise}: \textbf{9.1 minutos}

\subsection{Síntese dos Case Studies}

\begin{table}[h]
\centering
\caption{Resumo comparativo dos case studies}
\label{tab:case_summary}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Métrica} & \textbf{COMPAS} & \textbf{Credit} & \textbf{Adult} & \textbf{Health} \\
\midrule
Atributos detectados & 3/3 & 3/3 & 2/2 & 2/2 \\
Violações EEOC/ECOA & 1 & 1 & 2 & N/A \\
Threshold ajustável? & Sim & Sim & Limitado & Não \\
Tempo análise (min) & 7.2 & 5.8 & 12.4 & 9.1 \\
Tempo manual (min) & 35 & 25 & 50 & 40 \\
Economia de tempo & 79\% & 77\% & 75\% & 77\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Insights Principais}:
\begin{enumerate}
    \item \textbf{Auto-detecção 100\% acurada}: Todos atributos sensíveis detectados em todos datasets
    \item \textbf{Violações frequentes}: 3/4 casos violam regra 80\% ou equalized odds
    \item \textbf{Context matters}: Healthcare requer análise clínica, não apenas ajuste de threshold
    \item \textbf{Economia consistente}: 75-79\% de redução de tempo vs. análise manual
\end{enumerate}
