\section{Implementacao}

\subsection{Arquitetura}

Implementamos framework em Python 3.9+ integrado ao DeepBridge:

\begin{verbatim}
deepbridge/
├── core/
│   └── experiment/
│       ├── experiment.py           # Orquestrador central
│       ├── test_runner.py          # Execucao de testes
│       ├── parameter_standards.py  # Configs padrao
│       └── managers/               # Managers especializados
│           ├── robustness_manager.py
│           ├── uncertainty_manager.py
│           └── model_manager.py
├── validation/
│   ├── fairness/
│   │   └── metrics.py              # 15 metricas (1,601 linhas)
│   ├── robustness/
│   │   ├── weakspot_detector.py   # 461 linhas
│   │   └── overfit_analyzer.py    # 466 linhas
│   └── wrappers/
│       ├── robustness_suite.py    # 861 linhas
│       ├── uncertainty_suite.py   # 1,240 linhas
│       ├── fairness_suite.py
│       └── resilience_suite.py
├── distillation/
│   └── techniques/
│       ├── surrogate.py            # Surrogate models
│       ├── knowledge_distillation.py
│       └── ensemble.py
└── utils/
    ├── model_registry.py           # 12 tipos de modelo
    └── dataset_factory.py
\end{verbatim}

\textbf{Total}: ~5,000 linhas de codigo core, 50+ metricas implementadas.

\subsection{Implementacao do Robustness Suite}

\subsubsection{Perturbation Testing}

\begin{lstlisting}[language=Python]
class RobustnessSuite:
    """Suite completa de testes de robustez"""

    def __init__(self, dataset, metric='AUC',
                 n_iterations=10, verbose=True):
        self.dataset = dataset
        self.metric = metric
        self.n_iterations = n_iterations
        self.verbose = verbose

    def config(self, config_name='full'):
        """Configura niveis de teste"""
        configs = {
            'quick': {
                'perturbation_levels': [0.1, 0.5],
                'methods': ['gaussian']
            },
            'medium': {
                'perturbation_levels': [0.1, 0.3, 0.5, 0.7],
                'methods': ['gaussian', 'quantile']
            },
            'full': {
                'perturbation_levels': [0.1, 0.2, 0.3, 0.5, 0.7, 1.0],
                'methods': ['gaussian', 'quantile']
            }
        }
        self.config_params = configs[config_name]
        return self

    def run(self):
        """Executa todos testes de robustez"""
        results = {
            'robustness_score': 0.0,
            'perturbation_impacts': {},
            'weakspots': [],
            'overfitting_analysis': {}
        }

        # 1. Perturbation testing
        baseline_score = self._compute_baseline()

        for method in self.config_params['methods']:
            for alpha in self.config_params['perturbation_levels']:
                perturbed_score = self._test_perturbation(
                    method, alpha
                )
                gap = baseline_score - perturbed_score
                results['perturbation_impacts'][
                    f'{method}_{alpha}'
                ] = gap

        # Robustness score: 1 - mean gap
        mean_gap = np.mean(
            list(results['perturbation_impacts'].values())
        )
        results['robustness_score'] = 1 - mean_gap

        # 2. Weakspot detection
        detector = WeakspotDetector(
            slice_method='quantile',
            n_slices=10,
            severity_threshold=0.15
        )
        weakspots = detector.detect_weak_regions(
            X=self.dataset.X_test,
            y_true=self.dataset.y_test,
            y_pred=self.dataset.model.predict(
                self.dataset.X_test
            ),
            slice_features=self.dataset.feature_subset,
            metric='mae' if self.metric == 'MAE' else 'f1'
        )
        results['weakspots'] = weakspots['weakspots']

        # 3. Overfitting analysis
        analyzer = OverfitAnalyzer(n_slices=10)
        for feature in self.dataset.feature_subset[:3]:
            overfit_result = analyzer.compute_gap_by_slice(
                X_train=self.dataset.X_train,
                X_test=self.dataset.X_test,
                y_train=self.dataset.y_train,
                y_test=self.dataset.y_test,
                model=self.dataset.model,
                slice_feature=feature,
                metric_func=roc_auc_score
            )
            results['overfitting_analysis'][feature] = overfit_result

        return results

    def _test_perturbation(self, method, alpha):
        """Aplica perturbacao e calcula metrica"""
        if method == 'gaussian':
            X_pert = self._add_gaussian_noise(
                self.dataset.X_test, alpha
            )
        elif method == 'quantile':
            X_pert = self._add_quantile_noise(
                self.dataset.X_test, alpha
            )

        y_pred = self.dataset.model.predict(X_pert)

        if self.metric == 'AUC':
            return roc_auc_score(self.dataset.y_test, y_pred)
        elif self.metric == 'F1':
            return f1_score(self.dataset.y_test, y_pred)
        # ... outras metricas
\end{lstlisting}

\subsubsection{Weakspot Detector}

\begin{lstlisting}[language=Python]
class WeakspotDetector:
    """Detecta regioes onde modelo degrada"""

    def __init__(self, slice_method='quantile',
                 n_slices=10, severity_threshold=0.15):
        self.slice_method = slice_method
        self.n_slices = n_slices
        self.threshold = severity_threshold

    def detect_weak_regions(self, X, y_true, y_pred,
                           slice_features, metric='mae'):
        """
        Identifica weakspots via slice analysis

        Returns:
            {
                'weakspots': List[Dict],
                'summary': Dict,
                'slice_analysis': Dict
            }
        """
        weakspots = []
        global_perf = self._compute_metric(
            y_true, y_pred, metric
        )

        for feature in slice_features:
            # Criar slices
            slices = self._create_slices(
                X[feature], self.slice_method
            )

            # Analisar cada slice
            for slice_idx, slice_mask in enumerate(slices):
                if slice_mask.sum() < 10:  # Skip small slices
                    continue

                slice_perf = self._compute_metric(
                    y_true[slice_mask],
                    y_pred[slice_mask],
                    metric
                )

                # Calcular severity
                severity = global_perf - slice_perf

                if severity > self.threshold:
                    # Extrair limites do slice
                    slice_values = X[feature][slice_mask]
                    weakspots.append({
                        'feature': feature,
                        'slice_idx': slice_idx,
                        'slice_range': (
                            slice_values.min(),
                            slice_values.max()
                        ),
                        'severity': severity,
                        'performance': slice_perf,
                        'n_samples': slice_mask.sum(),
                        'description': self._generate_description(
                            feature, slice_values
                        )
                    })

        # Ordenar por severity
        weakspots.sort(key=lambda x: x['severity'], reverse=True)

        return {
            'weakspots': weakspots,
            'summary': {
                'total_weakspots': len(weakspots),
                'features_affected': len(set(
                    w['feature'] for w in weakspots
                )),
                'max_severity': max(
                    [w['severity'] for w in weakspots],
                    default=0
                )
            }
        }

    def _generate_description(self, feature, values):
        """Gera descricao interpretavel do weakspot"""
        return f"{feature} in range [{values.min():.2f}, {values.max():.2f}]"
\end{lstlisting}

\subsection{Implementacao do Uncertainty Suite}

\subsubsection{CRQR Otimizada}

\begin{lstlisting}[language=Python]
class UncertaintySuite:
    """Uncertainty quantification via CRQR"""

    def __init__(self, dataset, confidence=0.9,
                 use_cache=True, verbose=True):
        self.dataset = dataset
        self.alpha = 1 - confidence
        self.use_cache = use_cache
        self.cache = {}

    def run(self):
        """Executa CRQR com otimizacoes"""

        # Step 1: Train quantile regressors (com cache)
        cache_key = self._compute_cache_key()

        if self.use_cache and cache_key in self.cache:
            q_low_model, q_high_model = self.cache[cache_key]
        else:
            q_low_model = self._train_quantile_model(
                quantile=self.alpha/2
            )
            q_high_model = self._train_quantile_model(
                quantile=1 - self.alpha/2
            )

            if self.use_cache:
                self.cache[cache_key] = (q_low_model, q_high_model)

        # Step 2: Calibrate
        q_low_cal = q_low_model.predict(self.dataset.X_cal)
        q_high_cal = q_high_model.predict(self.dataset.X_cal)

        residuals = np.maximum(
            q_low_cal - self.dataset.y_cal,
            self.dataset.y_cal - q_high_cal
        )

        n = len(residuals)
        quantile_level = np.ceil((n+1)*(1-self.alpha))/n
        Q = np.quantile(residuals, quantile_level)

        # Step 3: Predict intervals
        q_low_test = q_low_model.predict(self.dataset.X_test)
        q_high_test = q_high_model.predict(self.dataset.X_test)

        intervals_low = q_low_test - Q
        intervals_high = q_high_test + Q

        # Step 4: Evaluate
        coverage = self._compute_coverage(
            intervals_low, intervals_high, self.dataset.y_test
        )

        widths = intervals_high - intervals_low

        # Step 5: Feature importance (permutation)
        feature_importance = self._permutation_importance(
            q_low_model, q_high_model
        )

        return {
            'coverage': coverage,
            'expected_coverage': 1 - self.alpha,
            'mean_width': widths.mean(),
            'median_width': np.median(widths),
            'normalized_width': widths.mean() / (
                self.dataset.y_test.max() -
                self.dataset.y_test.min()
            ),
            'uncertainty_quality_score': self._compute_quality_score(
                coverage, widths
            ),
            'feature_importance': feature_importance,
            'intervals': (intervals_low, intervals_high)
        }

    def _train_quantile_model(self, quantile):
        """Treina HistGradientBoosting com early stopping"""
        from sklearn.ensemble import HistGradientBoostingRegressor

        model = HistGradientBoostingRegressor(
            loss='quantile',
            quantile=quantile,
            max_iter=100,
            early_stopping=True,
            validation_fraction=0.1,
            n_iter_no_change=10,
            random_state=42
        )

        model.fit(self.dataset.X_train, self.dataset.y_train)
        return model

    def _permutation_importance(self, q_low_model, q_high_model):
        """
        Feature importance via permutation
        70-80% mais rapido que retreinamento
        """
        from sklearn.inspection import permutation_importance

        # Usar apenas q_low_model para importance
        # (empiricamente suficiente)
        result = permutation_importance(
            q_low_model,
            self.dataset.X_test,
            self.dataset.y_test,
            n_repeats=10,
            random_state=42,
            n_jobs=-1
        )

        return {
            'importances_mean': result.importances_mean,
            'importances_std': result.importances_std,
            'feature_names': self.dataset.feature_names
        }

    def _compute_quality_score(self, coverage, widths):
        """
        Uncertainty quality: balance coverage e width
        Score alto = boa coverage + intervalos estreitos
        """
        expected_coverage = 1 - self.alpha

        # Coverage score: penaliza under/over coverage
        coverage_diff = abs(coverage - expected_coverage)
        coverage_score = max(0, 1 - coverage_diff / 0.2)

        # Width score: normalizado pelo range do target
        target_range = (self.dataset.y_test.max() -
                       self.dataset.y_test.min())
        normalized_width = widths.mean() / target_range
        width_score = max(0, 1 - normalized_width)

        # Score final: 70% coverage, 30% width
        return 0.7 * coverage_score + 0.3 * width_score
\end{lstlisting}

\subsection{Implementacao do Fairness Suite}

\subsubsection{Metricas de Fairness}

\begin{lstlisting}[language=Python]
class FairnessSuite:
    """15 metricas de fairness com compliance"""

    def __init__(self, dataset, protected_attributes,
                 verbose=True):
        self.dataset = dataset
        self.protected_attrs = protected_attributes
        self.verbose = verbose

    def run(self):
        """Executa todas metricas de fairness"""
        results = {
            'pre_train_metrics': {},
            'post_train_metrics': {},
            'overall_fairness_score': 0.0,
            'compliance': {},
            'violations': []
        }

        # Pre-training metrics
        results['pre_train_metrics'] = {
            'class_balance': self._class_balance(),
            'concept_balance': self._concept_balance(),
            'kl_divergence': self._kl_divergence(),
            'js_divergence': self._js_divergence()
        }

        # Post-training metrics
        y_pred = self.dataset.model.predict(self.dataset.X_test)

        for attr in self.protected_attrs:
            attr_results = self._compute_post_train_metrics(
                attr, y_pred
            )
            results['post_train_metrics'][attr] = attr_results

        # Compliance scoring
        results['compliance'] = self._compute_compliance(
            results['post_train_metrics']
        )

        # Overall score
        results['overall_fairness_score'] = (
            results['compliance']['score']
        )

        # Identify violations
        results['violations'] = self._identify_violations(
            results['post_train_metrics']
        )

        return results

    def _compute_post_train_metrics(self, attr, y_pred):
        """Calcula 11 metricas pos-treinamento"""
        y_true = self.dataset.y_test
        protected = self.dataset.metadata[attr]

        protected_mask = (protected == 1)
        reference_mask = (protected == 0)

        # Selection rates
        rate_protected = y_pred[protected_mask].mean()
        rate_reference = y_pred[reference_mask].mean()

        return {
            'statistical_parity': abs(
                rate_protected - rate_reference
            ),
            'disparate_impact': (
                rate_protected / rate_reference
                if rate_reference > 0 else 0
            ),
            'equal_opportunity': self._equal_opportunity(
                y_true, y_pred, protected
            ),
            'equalized_odds': self._equalized_odds(
                y_true, y_pred, protected
            ),
            'precision_difference': self._precision_diff(
                y_true, y_pred, protected
            ),
            'accuracy_difference': self._accuracy_diff(
                y_true, y_pred, protected
            ),
            # ... outras 5 metricas
        }

    def _compute_compliance(self, post_train_metrics):
        """
        Compliance scoring com pesos regulatorios
        EEOC 80% rule = peso 3 (CRITICAL)
        """
        weights = {
            'disparate_impact': 3,  # EEOC critical
            'statistical_parity': 2,
            'equal_opportunity': 2,
            'equalized_odds': 2,
            # ... outros pesos
        }

        total_weight = 0
        passed_weight = 0

        for attr, metrics in post_train_metrics.items():
            for metric_name, value in metrics.items():
                weight = weights.get(metric_name, 1)
                total_weight += weight

                # Check if passed
                if metric_name == 'disparate_impact':
                    # EEOC 80% rule
                    passed = value >= 0.80
                elif metric_name == 'statistical_parity':
                    # Difference <= 0.1
                    passed = value <= 0.1
                # ... outros thresholds

                if passed:
                    passed_weight += weight

        score = (passed_weight / total_weight) * 100

        return {
            'score': score,
            'interpretation': self._interpret_score(score),
            'total_tests': len(post_train_metrics),
            'passed_tests': passed_weight / total_weight
        }

    def _interpret_score(self, score):
        """Interpretacao regulatoria do score"""
        if score >= 90:
            return "Compliance robusto - deployment aprovado"
        elif score >= 75:
            return "Compliance adequado - melhorias recomendadas"
        elif score >= 60:
            return "Compliance marginal - acoes necessarias"
        else:
            return "Nao-compliant - deployment arriscado"
\end{lstlisting}

\subsection{Otimizacoes de Performance}

\subsubsection{Caching de Modelos}

\begin{lstlisting}[language=Python]
def _compute_cache_key(self):
    """Gera chave de cache baseada em hash de dados"""
    import hashlib

    X_hash = hashlib.md5(
        self.dataset.X_train.values.tobytes()
    ).hexdigest()

    y_hash = hashlib.md5(
        self.dataset.y_train.values.tobytes()
    ).hexdigest()

    return f"{X_hash}_{y_hash}_{self.alpha}"
\end{lstlisting}

\textbf{Impacto}: Evita retreinamento de modelos CRQR para mesmos dados. Reducao de 30-50s em testes repetidos.

\subsubsection{Lazy Loading de Modelos Alternativos}

\begin{lstlisting}[language=Python]
# Em Experiment.__init__
# NÃO inicializa alternative_models automaticamente
self.alternative_models = None

# Apenas carrega quando explicitamente requisitado
def _load_alternative_models(self):
    if self.alternative_models is None:
        self.alternative_models = self._create_alternative_models()
\end{lstlisting}

\textbf{Impacto}: Economia de 30-50s na inicializacao do Experiment.

\subsubsection{Paralelizacao de Testes}

\begin{lstlisting}[language=Python]
from concurrent.futures import ThreadPoolExecutor

def run_tests(self, config_name='full'):
    """Executa testes em paralelo quando possivel"""

    # Testes independentes podem rodar em paralelo
    independent_tests = [
        'robustness',
        'uncertainty',
        'fairness'
    ]

    results = {}

    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = {
            executor.submit(
                self._run_test, test_name
            ): test_name
            for test_name in independent_tests
            if test_name in self.tests
        }

        for future in futures:
            test_name = futures[future]
            results[test_name] = future.result()

    # Resilience depende de outros testes
    if 'resilience' in self.tests:
        results['resilience'] = self._run_test('resilience')

    return results
\end{lstlisting}

\textbf{Impacto}: Reducao de 40-50\% no tempo total de execucao para config 'full'.

\subsection{Integracao com DeepBridge}

\subsubsection{ModelRegistry}

Suporte a 12 tipos de modelos, incluindo interpretaveis:

\begin{lstlisting}[language=Python]
class ModelType(Enum):
    # Modelos Interpretaveis
    DECISION_TREE = "decision_tree"
    LINEAR = "linear"
    LASSO = "lasso"
    RIDGE = "ridge"
    ELASTIC_NET = "elastic_net"

    # Semi-Interpretaveis
    GBM = "gbm"
    RANDOM_FOREST = "random_forest"

    # Complexos
    XGB = "xgboost"
    LIGHTGBM = "lightgbm"
    CATBOOST = "catboost"
    NEURAL_NET = "neural_network"
\end{lstlisting}

\subsubsection{Auto-deteccao de Atributos Protegidos}

\begin{lstlisting}[language=Python]
def _auto_detect_protected_attributes(self, X):
    """
    Auto-detecta atributos sensiveis via fuzzy matching
    Threshold: 70% similaridade
    """
    from fuzzywuzzy import fuzz

    protected_keywords = [
        'gender', 'sex', 'race', 'ethnicity',
        'age', 'religion', 'disability'
    ]

    detected = []
    for col in X.columns:
        for keyword in protected_keywords:
            similarity = fuzz.ratio(
                col.lower(), keyword.lower()
            )
            if similarity >= 70:
                detected.append(col)
                break

    return detected
\end{lstlisting}

\subsection{Metricas de Qualidade de Codigo}

\begin{table}[h]
\centering
\caption{Estatisticas de Implementacao}
\begin{tabular}{lr}
\toprule
\textbf{Metrica} & \textbf{Valor} \\
\midrule
Total de arquivos Python & 249 \\
Linhas de codigo core & ~5,000 \\
Metricas/metodos implementados & 50+ \\
Testes unitarios & 120+ \\
Cobertura de testes & 85\% \\
Documentacao (docstrings) & 95\% \\
\bottomrule
\end{tabular}
\end{table}
