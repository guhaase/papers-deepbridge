# CNN Implementation Plan for HPM-KD

**Status**: Ready to implement
**Requirement**: PyTorch installation
**Timeline**: 2 weeks
**Expected Results**: 98-99% accuracy on MNIST

---

## üìã OVERVIEW

### Current Status

‚úÖ **sklearn Implementation**: 91.67% accuracy (Full MNIST)
‚è≥ **CNN Implementation**: Not started (PyTorch not installed)

### Goal

Implement CNN-based HPM-KD to:
- Close gap to paper results (99.15% target)
- Use proper teacher/student architecture
- Validate framework with neural networks
- Generate paper-ready results

---

## üîß INSTALLATION REQUIREMENTS

### Step 1: Install PyTorch

```bash
# CPU version (for testing)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# OR GPU version (for training)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### Step 2: Verify Installation

```bash
python3 -c "import torch; print(f'PyTorch {torch.__version__} installed')"
python3 -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
```

### Step 3: Install Additional Dependencies

```bash
pip install torchvision  # For dataset loaders
pip install tqdm         # For progress bars
```

---

## üèóÔ∏è ARCHITECTURE DESIGN

### Teacher Model: ResNet-18 (Adapted for MNIST)

**Original ResNet-18**: 11M parameters (ImageNet, 224√ó224 RGB)
**Adapted for MNIST**: 4.2M parameters (MNIST, 28√ó28 grayscale)

```
Input: 28√ó28√ó1 (grayscale)
‚îú‚îÄ‚îÄ Conv1: 3√ó3, 64 channels
‚îú‚îÄ‚îÄ Layer1: 2√ó ResBlock(64 ‚Üí 64)
‚îú‚îÄ‚îÄ Layer2: 2√ó ResBlock(64 ‚Üí 128), stride=2
‚îú‚îÄ‚îÄ Layer3: 2√ó ResBlock(128 ‚Üí 256), stride=2
‚îú‚îÄ‚îÄ Layer4: 2√ó ResBlock(256 ‚Üí 512), stride=2
‚îú‚îÄ‚îÄ GlobalAvgPool: 512 ‚Üí 512
‚îî‚îÄ‚îÄ FC: 512 ‚Üí 10 classes
```

**Expected Performance**: 99.3-99.5% on MNIST

### Student Model: MobileNet-V2 (Adapted)

**Target**: 0.4M parameters (10.5√ó compression)

```
Input: 28√ó28√ó1 (grayscale)
‚îú‚îÄ‚îÄ Conv1: 3√ó3, 32 channels
‚îú‚îÄ‚îÄ DSConv1: 32 ‚Üí 64
‚îú‚îÄ‚îÄ DSConv2: 64 ‚Üí 128, stride=2
‚îú‚îÄ‚îÄ DSConv3: 128 ‚Üí 128
‚îú‚îÄ‚îÄ DSConv4: 128 ‚Üí 256, stride=2
‚îú‚îÄ‚îÄ DSConv5: 256 ‚Üí 256
‚îú‚îÄ‚îÄ DSConv6: 256 ‚Üí 512, stride=2
‚îú‚îÄ‚îÄ GlobalAvgPool: 512 ‚Üí 512
‚îî‚îÄ‚îÄ FC: 512 ‚Üí 10 classes
```

**Expected Performance**: 98.8-99.0% with HPM-KD

### Baseline Model: SimpleCNN

**Target**: 50k parameters (very small)

```
Input: 28√ó28√ó1
‚îú‚îÄ‚îÄ Conv1: 3√ó3, 32 channels ‚Üí MaxPool
‚îú‚îÄ‚îÄ Conv2: 3√ó3, 64 channels ‚Üí MaxPool
‚îú‚îÄ‚îÄ Conv3: 3√ó3, 64 channels ‚Üí MaxPool
‚îú‚îÄ‚îÄ FC1: 576 ‚Üí 128
‚îî‚îÄ‚îÄ FC2: 128 ‚Üí 10 classes
```

**Expected Performance**: 98.5-98.8% (Direct Training)

---

## üìä EXPECTED RESULTS

### Performance Targets

| Model | Params | Accuracy | Gap to Teacher |
|-------|--------|----------|----------------|
| **Teacher (ResNet-18)** | 4.2M | 99.3-99.5% | - |
| **Direct Training (SimpleCNN)** | 50k | 98.5-98.8% | -0.7% |
| **Traditional KD** | 0.4M | 98.9-99.1% | -0.4% |
| **HPM-KD (MobileNet)** | 0.4M | **99.0-99.2%** | **-0.2%** |

### Comparison with sklearn Results

| Method | sklearn (10k) | sklearn (70k) | CNN (70k) | Improvement |
|--------|--------------|--------------|-----------|-------------|
| Teacher | 94.10% | 96.57% | **99.40%** | **+2.83pp** |
| Direct | 65.20% | 65.54% | **98.70%** | **+33.16pp** |
| Trad KD | 67.35% | 68.54% | **99.00%** | **+30.46pp** |
| HPM-KD | 89.50% | 91.67% | **99.15%** | **+7.48pp** |

**Key**: CNN implementation should close the **-7.48% gap** completely!

---

## üî¨ IMPLEMENTATION STEPS

### Phase 1: Model Implementation (Days 1-2)

**Files to Create**:

1. **`cnn_models.py`** ‚úÖ Already created!
   - TeacherResNet class
   - StudentMobileNet class
   - SimpleCNN baseline
   - Helper functions

2. **`cnn_training.py`** (Next)
   - Training loop
   - Validation loop
   - Learning rate scheduling
   - Model checkpointing

3. **`cnn_distillation.py`** (Next)
   - Knowledge distillation loss
   - Temperature-scaled softmax
   - Feature matching (optional)
   - Integration with HPM-KD

### Phase 2: Teacher Training (Days 3-4)

```python
# Train teacher model
python3 train_teacher.py \
    --model resnet18 \
    --dataset mnist \
    --epochs 20 \
    --batch-size 128 \
    --lr 0.1 \
    --save-path models/teacher_resnet18.pth
```

**Expected**:
- Training time: 10-15 minutes (CPU) or 2-3 minutes (GPU)
- Final accuracy: 99.3-99.5%
- Save checkpoint for distillation

### Phase 3: Baseline Training (Days 5-6)

```python
# Direct training (no distillation)
python3 train_student.py \
    --model simplecnn \
    --dataset mnist \
    --epochs 20 \
    --batch-size 128 \
    --lr 0.1 \
    --save-path models/student_direct.pth
```

**Expected**:
- Training time: 5-10 minutes
- Final accuracy: 98.5-98.8%

### Phase 4: Traditional KD (Days 7-8)

```python
# Traditional Knowledge Distillation
python3 train_kd.py \
    --teacher models/teacher_resnet18.pth \
    --student mobilenet \
    --dataset mnist \
    --epochs 20 \
    --batch-size 128 \
    --temperature 4.0 \
    --alpha 0.5 \
    --save-path models/student_kd.pth
```

**Expected**:
- Training time: 8-12 minutes
- Final accuracy: 98.9-99.1%

### Phase 5: HPM-KD Implementation (Days 9-12)

```python
# HPM-KD with all components
python3 train_hpmkd.py \
    --teacher models/teacher_resnet18.pth \
    --student mobilenet \
    --dataset mnist \
    --use-progressive \
    --use-adaptive-temp \
    --use-multi-teacher \
    --epochs 20 \
    --batch-size 128 \
    --save-path models/student_hpmkd.pth
```

**Expected**:
- Training time: 15-20 minutes
- Final accuracy: 99.0-99.2%
- +0.1-0.3pp improvement over Traditional KD

### Phase 6: Evaluation & Analysis (Days 13-14)

```python
# Comprehensive evaluation
python3 evaluate_all.py \
    --models models/*.pth \
    --dataset mnist \
    --output results/cnn_results.csv
```

**Generate**:
- Accuracy metrics
- Confusion matrices
- Feature visualizations (t-SNE)
- Confidence calibration plots

---

## üìù CODE STRUCTURE

### Proposed File Organization

```
papers/01_HPM-KD_Framework/POR/
‚îú‚îÄ‚îÄ cnn_models.py              ‚úÖ Created
‚îú‚îÄ‚îÄ cnn_training.py            ‚è≥ Next
‚îú‚îÄ‚îÄ cnn_distillation.py        ‚è≥ Next
‚îú‚îÄ‚îÄ train_teacher.py           ‚è≥ Next
‚îú‚îÄ‚îÄ train_student.py           ‚è≥ Next
‚îú‚îÄ‚îÄ train_kd.py                ‚è≥ Next
‚îú‚îÄ‚îÄ train_hpmkd.py             ‚è≥ Next
‚îú‚îÄ‚îÄ evaluate_all.py            ‚è≥ Next
‚îú‚îÄ‚îÄ models/                    (Saved checkpoints)
‚îÇ   ‚îú‚îÄ‚îÄ teacher_resnet18.pth
‚îÇ   ‚îú‚îÄ‚îÄ student_direct.pth
‚îÇ   ‚îú‚îÄ‚îÄ student_kd.pth
‚îÇ   ‚îî‚îÄ‚îÄ student_hpmkd.pth
‚îî‚îÄ‚îÄ results/
    ‚îî‚îÄ‚îÄ cnn_results.csv
```

---

## üéØ INTEGRATION WITH HPM-KD FRAMEWORK

### Connecting to DeepBridge

The DeepBridge HPM-KD framework needs to be extended to support PyTorch models:

**Option 1: Wrapper Approach** (Recommended)
```python
from deepbridge.distillation.techniques.hpm import HPMDistiller

# Wrap PyTorch models for HPM-KD
teacher_wrapped = PyTorchModelWrapper(teacher_model)
student_wrapped = PyTorchModelWrapper(student_model)

# Use existing HPM-KD
distiller = HPMDistiller(
    teacher_model=teacher_wrapped,
    config=hpm_config
)
```

**Option 2: Standalone Implementation**
```python
# Implement HPM-KD components directly in PyTorch
from hpm_pytorch import (
    ProgressiveDistillationChain,
    AdaptiveConfigurationManager,
    MetaTemperatureScheduler
)
```

---

## üìä VALIDATION CRITERIA

### Success Metrics

‚úÖ **Teacher Performance**:
- Accuracy ‚â• 99.3% on MNIST
- Matches paper specification

‚úÖ **Direct Training Performance**:
- Accuracy ‚â• 98.5%
- Establishes strong baseline

‚úÖ **Traditional KD Performance**:
- Accuracy ‚â• 98.9%
- Retention ‚â• 99.5%

‚úÖ **HPM-KD Performance**:
- Accuracy ‚â• 99.0%
- Improvement over Traditional KD ‚â• +0.1pp
- Closes gap to paper results

### Validation Tests

1. **Accuracy Test**: All models achieve target accuracies
2. **Compression Test**: Student has 10√ó fewer parameters
3. **Improvement Test**: HPM-KD > Traditional KD
4. **Retention Test**: HPM-KD retains ‚â•99.5% teacher accuracy
5. **Reproducibility Test**: Results consistent across 5 seeds

---

## üî¨ EXPERIMENTAL PROTOCOL

### Training Configuration

**Teacher Training**:
```yaml
Model: ResNet-18 (adapted)
Optimizer: SGD (momentum=0.9)
Learning Rate: 0.1 ‚Üí 0.001 (cosine annealing)
Batch Size: 128
Epochs: 20
Weight Decay: 5e-4
Data Augmentation: Random crop, horizontal flip (disabled for MNIST)
```

**Student Training (Direct)**:
```yaml
Model: SimpleCNN
Optimizer: SGD (momentum=0.9)
Learning Rate: 0.1 ‚Üí 0.001
Batch Size: 128
Epochs: 20
Weight Decay: 5e-4
```

**Knowledge Distillation (Traditional KD)**:
```yaml
Temperature: 4.0
Alpha (KD weight): 0.5
Optimizer: SGD (momentum=0.9)
Learning Rate: 0.1 ‚Üí 0.001
Batch Size: 128
Epochs: 20
```

**HPM-KD Configuration**:
```yaml
Use Progressive: True
Progressive Stages: [SimpleCNN ‚Üí MobileNet-Small ‚Üí MobileNet]
Adaptive Temperature: True
Initial Temperature: 4.0
Multi-Teacher: False (single teacher)
Parallel: False (sequential)
Cache: True
```

---

## üìà EXPECTED TIMELINE

### Two-Week Plan

**Week 1: Core Implementation**
- Days 1-2: Training infrastructure ‚úÖ
- Days 3-4: Teacher training ‚úÖ
- Days 5-6: Baseline student training ‚úÖ
- Day 7: Traditional KD implementation ‚úÖ

**Week 2: HPM-KD & Validation**
- Days 8-10: HPM-KD implementation ‚úÖ
- Days 11-12: Comprehensive experiments ‚úÖ
- Days 13-14: Analysis & figures ‚úÖ

**Total**: 14 days to complete CNN implementation

---

## üéØ DELIVERABLES

### At Completion

1. ‚úÖ **Trained Models** (4 checkpoints)
   - Teacher ResNet-18
   - Student Direct
   - Student Traditional KD
   - Student HPM-KD

2. ‚úÖ **Experimental Results**
   - Accuracy comparison table
   - Training curves
   - Feature visualizations
   - Statistical significance tests

3. ‚úÖ **Updated Figures**
   - Replace sklearn results with CNN results
   - Add new CNN-specific visualizations
   - Generate paper-ready plots

4. ‚úÖ **Documentation**
   - Training logs
   - Hyperparameter configurations
   - Reproducibility guide

---

## üí° NEXT IMMEDIATE STEPS

### To Start CNN Implementation

1. **Install PyTorch**:
   ```bash
   pip install torch torchvision
   ```

2. **Test CNN Models**:
   ```bash
   python3 cnn_models.py
   ```

3. **Create Training Script**:
   - Implement training loop
   - Add learning rate scheduling
   - Include validation monitoring

4. **Train Teacher**:
   - Run for 20 epochs
   - Save best checkpoint
   - Verify 99.3%+ accuracy

5. **Implement Distillation**:
   - Traditional KD first
   - Then HPM-KD integration
   - Compare results

---

## üìä COMPARISON: sklearn vs CNN

### Why CNN Implementation Matters

| Aspect | sklearn (Current) | CNN (Planned) | Impact |
|--------|------------------|---------------|---------|
| **Teacher Capacity** | 94.10-96.57% | **99.3-99.5%** | +2.7-3.8pp |
| **Absolute Accuracy** | 91.67% | **99.0-99.2%** | +7.3-7.5pp |
| **Gap to Paper** | -7.48% | **~0%** | Close gap! |
| **Publication Value** | Good | **Excellent** | Top-tier |
| **Architecture** | sklearn trees | **Neural nets** | Proper KD |

---

## üèÜ SUCCESS CRITERIA

### Must Achieve

- [x] CNN models defined ‚úÖ
- [ ] PyTorch installed
- [ ] Teacher trained (‚â•99.3%)
- [ ] Baselines trained (‚â•98.5%)
- [ ] Traditional KD (‚â•98.9%)
- [ ] HPM-KD (‚â•99.0%)
- [ ] Improvement demonstrated (+0.1pp)
- [ ] Results reproducible

### Nice to Have

- [ ] Feature visualization (t-SNE)
- [ ] Attention maps
- [ ] Confidence calibration
- [ ] Adversarial robustness tests

---

## üìù NOTES

### Advantages of CNN Implementation

1. **Paper Alignment**: Matches paper specification exactly
2. **Performance**: Expected 99%+ accuracy (vs 91.67% sklearn)
3. **Proper KD**: Neural network distillation (not tree‚Üítree)
4. **Publication**: Top-tier venues expect CNN results
5. **Extensibility**: Can extend to CIFAR, ImageNet

### Challenges

1. **Setup**: Requires PyTorch installation
2. **Compute**: Needs GPU for faster training (optional)
3. **Time**: 2 weeks vs 2 hours for sklearn
4. **Complexity**: More code to maintain

### Mitigation

- Start with CPU version (works, just slower)
- Use Google Colab if no local GPU
- Reuse sklearn experiment infrastructure
- Follow modular design for maintainability

---

**Status**: ‚úÖ **READY TO START**
**Next Action**: Install PyTorch and test cnn_models.py
**Timeline**: 2 weeks to completion
**Expected Result**: 99%+ accuracy, close gap to paper

**Created**: November 5, 2025, 07:20 BRT
**Author**: Claude Code + Gustavo Coelho Haase
