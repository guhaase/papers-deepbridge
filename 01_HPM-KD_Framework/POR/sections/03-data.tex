%% Section 3: Experimental Setup

\section{Experimental Setup}
\label{sec:data}

This section describes the experimental design, datasets, baseline methods, evaluation metrics, and implementation details used to validate the HPM-KD framework. Our experiments are designed to answer four key research questions:

\begin{enumerate}
    \item \textbf{RQ1 (Compression Efficiency)}: Can HPM-KD achieve higher compression ratios while maintaining accuracy compared to state-of-the-art distillation methods?
    \item \textbf{RQ2 (Component Contribution)}: How much does each of the six HPM-KD components contribute to overall performance?
    \item \textbf{RQ3 (Generalization)}: Does HPM-KD generalize across diverse domains (vision, tabular data) and dataset scales?
    \item \textbf{RQ4 (Computational Efficiency)}: What is the computational overhead of HPM-KD compared to traditional single-step distillation?
\end{enumerate}

\subsection{Datasets}

We evaluate HPM-KD on eight benchmark datasets spanning computer vision and tabular data domains, ensuring diversity in task complexity, dataset size, and feature dimensionality.

\subsubsection{Computer Vision Datasets}

\paragraph{MNIST}~\citep{lecun1998gradient}
Handwritten digit recognition dataset with 60,000 training and 10,000 test images ($28 \times 28$ grayscale). Despite its simplicity, MNIST serves as a sanity check and allows rapid prototyping of distillation configurations.

\paragraph{Fashion-MNIST}~\citep{xiao2017fashion}
A more challenging replacement for MNIST, containing 60,000 training and 10,000 test images ($28 \times 28$ grayscale) of 10 fashion product categories. This dataset tests whether distillation benefits hold for more complex visual patterns.

\paragraph{CIFAR-10}~\citep{krizhevsky2009learning}
Natural image classification with 50,000 training and 10,000 test images ($32 \times 32$ RGB) across 10 classes. CIFAR-10 introduces color, texture, and inter-class similarity challenges.

\paragraph{CIFAR-100}~\citep{krizhevsky2009learning}
Extended version of CIFAR-10 with 100 fine-grained classes, making it substantially more difficult. This dataset evaluates how well HPM-KD handles large output spaces where soft targets provide richer information.

\subsubsection{Tabular Datasets}

We include four datasets from the UCI Machine Learning Repository~\citep{dua2017uci} to assess HPM-KD's applicability beyond computer vision:

\paragraph{Adult (Census Income)}
Binary classification task predicting whether income exceeds \$50K/year. Contains 48,842 instances with 14 features (mix of categorical and numerical). Tests distillation on structured socioeconomic data.

\paragraph{Credit (German Credit)}
Binary classification for credit risk assessment with 1,000 instances and 20 features. The small sample size tests HPM-KD's robustness in low-data regimes.

\paragraph{Wine Quality}
Regression task (converted to 6-class classification) predicting wine quality scores. Contains 6,497 instances with 11 physicochemical features. Tests distillation on sensory evaluation data.

\paragraph{OpenML-CC18}
To ensure reproducibility and broader coverage, we also evaluate on a subset of the OpenML Curated Classification benchmark~\citep{vanschoren2014openml}, selecting 10 datasets with diverse characteristics (sample sizes: 150-70,000; features: 4-617; classes: 2-26).

Table~\ref{tab:datasets_summary} summarizes key statistics of all datasets.

\begin{table}[t]
\centering
\caption{Summary statistics of benchmark datasets used in experiments.}
\label{tab:datasets_summary}
\begin{tabular}{@{}lrrrrc@{}}
\toprule
\textbf{Dataset} & \textbf{Train} & \textbf{Test} & \textbf{Features} & \textbf{Classes} & \textbf{Domain} \\
\midrule
MNIST & 60,000 & 10,000 & 784 & 10 & Vision \\
Fashion-MNIST & 60,000 & 10,000 & 784 & 10 & Vision \\
CIFAR-10 & 50,000 & 10,000 & 3,072 & 10 & Vision \\
CIFAR-100 & 50,000 & 10,000 & 3,072 & 100 & Vision \\
\midrule
Adult & 32,561 & 16,281 & 14 & 2 & Tabular \\
Credit & 700 & 300 & 20 & 2 & Tabular \\
Wine Quality & 4,548 & 1,949 & 11 & 6 & Tabular \\
OpenML-CC18 & \multicolumn{2}{c}{Varies} & Varies & Varies & Tabular \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Model Architectures}

For each dataset, we define teacher and student model architectures with substantial capacity gaps to stress-test the distillation process.

\subsubsection{Vision Tasks}

\paragraph{Teachers:}
\begin{itemize}
    \item MNIST/Fashion-MNIST: 3-layer CNN (Conv-128, Conv-256, Conv-512) + 2 FC layers (1024, 512) = \textbf{4.2M parameters}
    \item CIFAR-10/100: ResNet-56~\citep{he2016deep} or WideResNet-28-10~\citep{zagoruyko2016wide} = \textbf{0.85M / 36.5M parameters}
\end{itemize}

\paragraph{Students:}
\begin{itemize}
    \item MNIST/Fashion-MNIST: 2-layer CNN (Conv-32, Conv-64) + 1 FC layer (128) = \textbf{0.4M parameters} (\textbf{10.5$\times$ compression})
    \item CIFAR-10/100: ResNet-20 or MobileNetV2~\citep{sandler2018mobilenetv2} = \textbf{0.27M / 3.5M parameters} (\textbf{3.1-10.4$\times$ compression})
\end{itemize}

\subsubsection{Tabular Tasks}

\paragraph{Teachers:}
\begin{itemize}
    \item Deep Neural Network: 3 hidden layers (256, 512, 256) with dropout (0.3) = \textbf{0.2-0.5M parameters}
    \item Alternatively: XGBoost ensemble with 200 boosting rounds~\citep{chen2016xgboost}
\end{itemize}

\paragraph{Students:}
\begin{itemize}
    \item Shallow Network: 2 hidden layers (64, 32) = \textbf{0.02-0.05M parameters} (\textbf{10-15$\times$ compression})
\end{itemize}

\subsection{Baseline Methods}

We compare HPM-KD against five state-of-the-art knowledge distillation and compression methods:

\paragraph{1. Standard Training (No KD)}
Training the student model directly on hard labels without distillation. This establishes the lower bound performance.

\paragraph{2. Traditional Knowledge Distillation (KD)}~\citep{hinton2015distilling}
Single-step distillation with fixed temperature $T=4$ and loss weight $\alpha=0.7$, as suggested by the original paper. This is the most widely used baseline.

\paragraph{3. FitNets}~\citep{romero2014fitnets}
Progressive distillation using intermediate layer hints. We match the second-to-last hidden layer of the teacher with the student's corresponding layer using L2 loss.

\paragraph{4. Deep Mutual Learning (DML)}~\citep{zhang2018deep}
Collaborative learning where multiple student networks learn simultaneously from each other. We train 3 student networks with mutual distillation losses.

\paragraph{5. Teacher Assistant Knowledge Distillation (TAKD)}~\citep{mirzadeh2020improved}
Two-step progressive distillation with a manually designed intermediate teacher assistant network of size halfway between teacher and student.

All baselines are implemented using identical optimization settings (see Section~\ref{sec:implementation}) to ensure fair comparison.

\subsection{Evaluation Metrics}

We evaluate all methods using five complementary metrics:

\paragraph{1. Compression Ratio}
Defined as the ratio of teacher to student model sizes:
\begin{equation}
\text{Compression Ratio} = \frac{\text{\# Teacher Parameters}}{\text{\# Student Parameters}}
\end{equation}

\paragraph{2. Accuracy Retention}
The percentage of teacher model accuracy preserved by the student:
\begin{equation}
\text{Accuracy Retention} = \frac{\text{Acc}_{\text{student}}}{\text{Acc}_{\text{teacher}}} \times 100\%
\end{equation}

\paragraph{3. Relative Improvement over Direct Training}
How much the distillation method improves over training the student from scratch:
\begin{equation}
\Delta_{\text{improvement}} = \text{Acc}_{\text{distilled}} - \text{Acc}_{\text{direct}}
\end{equation}

\paragraph{4. Training Time}
Total wall-clock time (in hours) for the complete distillation process, including all intermediate steps. Measured on NVIDIA RTX 4090 GPU with consistent experimental conditions.

\paragraph{5. Inference Latency}
Average inference time per sample (in milliseconds) on CPU (Intel i9-12900K) and GPU (RTX 4090), computed over 1,000 test samples with batch size 1 to simulate real-time deployment scenarios.

\paragraph{6. Memory Footprint}
Peak GPU memory usage (in MB) during training and inference. This is critical for deployment in resource-constrained environments.

\subsection{Implementation Details}
\label{sec:implementation}

\subsubsection{Framework and Reproducibility}

All experiments are implemented using the \texttt{DeepBridge} library (v0.8.0) built on PyTorch 2.0.1 and scikit-learn 1.3.0. Code, trained models, and complete experimental logs are publicly available at:
\begin{center}
\url{https://github.com/DeepBridge-Validation/DeepBridge}
\end{center}

We fix random seeds (Python: 42, NumPy: 42, PyTorch: 42) and use deterministic algorithms where possible to ensure reproducibility.

\subsubsection{Training Configuration}

Unless otherwise specified, all models are trained using:
\begin{itemize}
    \item \textbf{Optimizer}: Adam~\citep{kingma2014adam} with $\beta_1=0.9$, $\beta_2=0.999$, $\epsilon=10^{-8}$
    \item \textbf{Learning Rate}: Initial $\eta=10^{-3}$ with cosine annealing schedule~\citep{loshchilov2016sgdr}
    \item \textbf{Weight Decay}: $\lambda=10^{-4}$
    \item \textbf{Batch Size}: 128 (vision), 64 (tabular)
    \item \textbf{Epochs}: 200 for teachers, 150 for students (with early stopping, patience=20)
    \item \textbf{Data Augmentation} (vision only): Random horizontal flips, random crops, normalization
\end{itemize}

\subsubsection{HPM-KD Configuration}

For HPM-KD-specific components:
\begin{itemize}
    \item \textbf{Progressive Chain}: Minimum improvement threshold $\epsilon=0.01$ (1\% relative accuracy improvement)
    \item \textbf{Multi-Teacher Attention}: 2-layer MLP with hidden size 128, dropout 0.2
    \item \textbf{Meta-Temperature Scheduler}: Initial $T=4$, adaptive range $[2, 6]$, adjustment frequency every 10 epochs
    \item \textbf{Parallel Workers}: 4 processes for multi-teacher distillation
    \item \textbf{Shared Memory Cache}: LRU cache with max 100 configurations
\end{itemize}

\subsubsection{Hyperparameter Tuning}

For all baseline methods, we perform grid search over key hyperparameters:
\begin{itemize}
    \item Temperature $T \in \{2, 4, 6, 8\}$
    \item Loss weight $\alpha \in \{0.3, 0.5, 0.7, 0.9\}$
    \item Learning rate $\eta \in \{10^{-4}, 10^{-3}, 10^{-2}\}$
\end{itemize}

We report the best configuration for each baseline on each dataset. In contrast, HPM-KD automatically selects these hyperparameters via the Adaptive Configuration Manager without manual tuning.

\subsection{Experimental Protocol}

\subsubsection{Main Experiments (RQ1, RQ3)}

For each dataset and each method (HPM-KD + 5 baselines):
\begin{enumerate}
    \item Train teacher model to convergence on full training set
    \item Apply distillation method to train student model
    \item Evaluate student on held-out test set
    \item Repeat for 5 independent runs with different random seeds
    \item Report mean and standard deviation across runs
\end{enumerate}

\subsubsection{Ablation Studies (RQ2)}

To isolate the contribution of each HPM-KD component, we evaluate six ablated variants:
\begin{itemize}
    \item \textbf{HPM-KD$_{-\text{AdaptConf}}$}: Without Adaptive Configuration Manager (manual hyperparameters)
    \item \textbf{HPM-KD$_{-\text{ProgChain}}$}: Without Progressive Distillation Chain (single-step)
    \item \textbf{HPM-KD$_{-\text{MultiTeach}}$}: Without Multi-Teacher Attention (single teacher)
    \item \textbf{HPM-KD$_{-\text{MetaTemp}}$}: Without Meta-Temperature Scheduler (fixed $T=4$)
    \item \textbf{HPM-KD$_{-\text{Parallel}}$}: Without Parallel Processing (sequential)
    \item \textbf{HPM-KD$_{-\text{Memory}}$}: Without Shared Optimization Memory (no caching)
\end{itemize}

Each ablation is compared against the full HPM-KD system on all datasets.

\subsubsection{Computational Analysis (RQ4)}

We measure:
\begin{itemize}
    \item Training time breakdown: teacher training, distillation, total
    \item GPU memory usage: peak and average
    \item Inference latency: batch sizes 1, 8, 32, 128
    \item Parallel speedup: 1, 2, 4, 8 workers
\end{itemize}

All timing experiments are conducted on identical hardware (NVIDIA RTX 4090, 24GB VRAM; Intel i9-12900K, 64GB RAM) with exclusive access to avoid interference.

\subsection{Statistical Significance Testing}

We assess statistical significance using paired t-tests (for pairwise comparisons) and repeated measures ANOVA (for multiple method comparisons) at $\alpha=0.05$ significance level. Bonferroni correction is applied for multiple comparisons. Results are marked with asterisks: * ($p < 0.05$), ** ($p < 0.01$), *** ($p < 0.001$).

\subsection{Summary}

This comprehensive experimental design ensures rigorous evaluation of HPM-KD across diverse tasks, fair comparison with strong baselines, thorough ablation analysis, and reproducible results. The combination of vision and tabular datasets, multiple compression ratios, and detailed computational profiling provides a complete picture of HPM-KD's strengths and limitations.
