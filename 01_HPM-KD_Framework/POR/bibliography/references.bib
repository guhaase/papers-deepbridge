%% Bibliography for HPM-KD Paper
%% Generated references for knowledge distillation framework

%% ============================================================================
%% PRIORITY 1: CRITICAL REFERENCES - Knowledge Distillation Foundations
%% ============================================================================

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@inproceedings{bucilua2006model,
  title={Model compression},
  author={Bucilu«é, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru},
  booktitle={Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={535--541},
  year={2006}
}

@inproceedings{ba2014deep,
  title={Do deep nets really need to be deep?},
  author={Ba, Jimmy and Caruana, Rich},
  booktitle={Advances in Neural Information Processing Systems},
  volume={27},
  pages={2654--2662},
  year={2014}
}

@inproceedings{cho2019efficacy,
  title={On the efficacy of knowledge distillation},
  author={Cho, Jang Hyun and Hariharan, Bharath},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4794--4802},
  year={2019}
}

@inproceedings{menon2021statistical,
  title={A statistical perspective on distillation},
  author={Menon, Aditya Krishna and Rawat, Ankit Singh and Reddi, Sashank and Kumar, Sanjiv},
  booktitle={International Conference on Machine Learning},
  pages={7632--7642},
  year={2021},
  organization={PMLR}
}

@inproceedings{phuong2019towards,
  title={Towards understanding knowledge distillation},
  author={Phuong, Mary and Lampert, Christoph},
  booktitle={International Conference on Machine Learning},
  pages={5142--5151},
  year={2019},
  organization={PMLR}
}

%% ============================================================================
%% PRIORITY 1: Multi-Teacher Distillation
%% ============================================================================

@inproceedings{you2017learning,
  title={Learning from multiple teacher networks},
  author={You, Shan and Xu, Chang and Xu, Chao and Tao, Dacheng},
  booktitle={Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={1285--1294},
  year={2017}
}

@inproceedings{zhang2018deep,
  title={Deep mutual learning},
  author={Zhang, Ying and Xiang, Tao and Hospedales, Timothy M and Lu, Huchuan},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4320--4328},
  year={2018}
}

@inproceedings{park2019relational,
  title={Relational knowledge distillation},
  author={Park, Wonpyo and Kim, Dongju and Lu, Yan and Cho, Minsu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3967--3976},
  year={2019}
}

@inproceedings{fukuda2017efficient,
  title={Efficient knowledge distillation from an ensemble of teachers},
  author={Fukuda, Takashi and Suzuki, Masayuki and Kurata, Gakuto and Thomas, Samuel and Cui, Jia and Ramabhadran, Bhuvana},
  booktitle={Interspeech},
  pages={3697--3701},
  year={2017}
}

%% ============================================================================
%% PRIORITY 1: Progressive/Multi-Step Distillation
%% ============================================================================

@inproceedings{mirzadeh2020improved,
  title={Improved knowledge distillation via teacher assistant},
  author={Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Li, Ang and Levine, Nir and Matsukawa, Akihiro and Ghasemzadeh, Hassan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={5191--5198},
  year={2020}
}

@inproceedings{romero2014fitnets,
  title={FitNets: Hints for thin deep nets},
  author={Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  year={2015}
}

@inproceedings{luo2016face,
  title={Face model compression by distilling knowledge from neurons},
  author={Luo, Ping and Zhu, Zhenyao and Liu, Ziwei and Wang, Xiaogang and Tang, Xiaoou},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={30},
  number={1},
  year={2016}
}

%% ============================================================================
%% PRIORITY 1: Deep Learning Architectures
%% ============================================================================

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={770--778},
  year={2016}
}

@article{devlin2018bert,
  title={BERT: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{krizhevsky2012imagenet,
  title={ImageNet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in Neural Information Processing Systems},
  volume={25},
  pages={1097--1105},
  year={2012}
}

%% ============================================================================
%% PRIORITY 1: Model Compression
%% ============================================================================

@inproceedings{han2016deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and Huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  booktitle={International Conference on Learning Representations},
  year={2016}
}

@article{choudhary2020comprehensive,
  title={A comprehensive survey on model compression and acceleration},
  author={Choudhary, Tejalal and Mishra, Vipul and Goswami, Anurag and Sarangapani, Jagannathan},
  journal={Artificial Intelligence Review},
  volume={53},
  number={7},
  pages={5113--5155},
  year={2020},
  publisher={Springer}
}

@article{cheng2017survey,
  title={A survey of model compression and acceleration for deep neural networks},
  author={Cheng, Yu and Wang, Duo and Zhou, Pan and Zhang, Tao},
  journal={arXiv preprint arXiv:1710.09282},
  year={2017}
}

%% ============================================================================
%% PRIORITY 2: Meta-Learning
%% ============================================================================

@article{hospedales2021meta,
  title={Meta-learning in neural networks: A survey},
  author={Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={44},
  number={9},
  pages={5149--5169},
  year={2021},
  publisher={IEEE}
}

@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={1126--1135},
  year={2017},
  organization={PMLR}
}

%% ============================================================================
%% PRIORITY 2: Attention Mechanisms
%% ============================================================================

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  pages={5998--6008},
  year={2017}
}

@inproceedings{zagoruyko2016paying,
  title={Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@inproceedings{woo2018cbam,
  title={CBAM: Convolutional block attention module},
  author={Woo, Sanghyun and Park, Jongchan and Lee, Joon-Young and Kweon, In So},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={3--19},
  year={2018}
}

@inproceedings{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

%% ============================================================================
%% PRIORITY 2: Neural Architecture Search
%% ============================================================================

@inproceedings{zoph2017neural,
  title={Neural architecture search with reinforcement learning},
  author={Zoph, Barret and Le, Quoc V},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@inproceedings{liu2019darts,
  title={DARTS: Differentiable architecture search},
  author={Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{elsken2019neural,
  title={Neural architecture search: A survey},
  author={Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
  journal={Journal of Machine Learning Research},
  volume={20},
  number={55},
  pages={1--21},
  year={2019}
}

%% ============================================================================
%% PRIORITY 2: Optimization
%% ============================================================================

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{loshchilov2016sgdr,
  title={SGDR: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

%% ============================================================================
%% PRIORITY 3: Datasets
%% ============================================================================

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={IEEE}
}

@article{xiao2017fashion,
  title={Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms},
  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  journal={arXiv preprint arXiv:1708.07747},
  year={2017}
}

@techreport{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  institution={University of Toronto}
}

@misc{dua2017uci,
  title={UCI machine learning repository},
  author={Dua, Dheeru and Graff, Casey},
  year={2017},
  publisher={University of California, Irvine, School of Information and Computer Sciences},
  url={http://archive.ics.uci.edu/ml}
}

@article{vanschoren2014openml,
  title={OpenML: networked science in machine learning},
  author={Vanschoren, Joaquin and Van Rijn, Jan N and Bischl, Bernd and Torgo, Luis},
  journal={ACM SIGKDD Explorations Newsletter},
  volume={15},
  number={2},
  pages={49--60},
  year={2014},
  publisher={ACM New York, NY, USA}
}

%% ============================================================================
%% PRIORITY 3: Mobile and Edge AI
%% ============================================================================

@article{howard2017mobilenets,
  title={MobileNets: Efficient convolutional neural networks for mobile vision applications},
  author={Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal={arXiv preprint arXiv:1704.04861},
  year={2017}
}

@inproceedings{sandler2018mobilenetv2,
  title={MobileNetV2: Inverted residuals and linear bottlenecks},
  author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4510--4520},
  year={2018}
}

%% ============================================================================
%% PRIORITY 3: Additional References
%% ============================================================================

@inproceedings{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  booktitle={British Machine Vision Conference},
  year={2016}
}

@inproceedings{chen2016xgboost,
  title={XGBoost: A scalable tree boosting system},
  author={Chen, Tianqi and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={785--794},
  year={2016}
}

@article{strubell2019energy,
  title={Energy and policy considerations for deep learning in NLP},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  journal={arXiv preprint arXiv:1906.02243},
  year={2019}
}

@inproceedings{tang2020understanding,
  title={Understanding and improving fairness-accuracy trade-offs in multi-task learning},
  author={Tang, Raphael and Du, Yao and Li, Xin and Lin, Zixuan and Ororbia, Alexander and Lease, Matthew},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={1628--1637},
  year={2020}
}

%% ============================================================================
%% END OF BIBLIOGRAPHY
%% ============================================================================
