# Experimento 3: Estudo de Usabilidade

## Objetivo

Comprovar as afirma√ß√µes sobre usabilidade do DeepBridge atrav√©s de estudo emp√≠rico com usu√°rios reais (cientistas de dados e engenheiros de ML).

## Afirma√ß√µes a Comprovar

| M√©trica | Valor Afirmado | Status |
|---------|----------------|--------|
| SUS Score | 87.5 (top 10%, "excelente") | ‚è≥ Pendente |
| Taxa de Sucesso | 95% (19/20 completaram) | ‚è≥ Pendente |
| Tempo para Primeira Valida√ß√£o | 12 min (vs. 45 min estimado) | ‚è≥ Pendente |
| NASA TLX (carga cognitiva) | 28/100 (baixa) | ‚è≥ Pendente |
| Ado√ß√£o em Produ√ß√£o | 6 organiza√ß√µes, 3 dom√≠nios | ‚è≥ Pendente |

## Metodologia

### 1. Participantes

**Total**: 20 profissionais

**Perfil**:
- 10 cientistas de dados
- 10 engenheiros de ML

**Experi√™ncia**: 2-10 anos em ML

**Distribui√ß√£o por Dom√≠nio**:
- Fintech: 8 participantes (40%)
- Sa√∫de: 5 participantes (25%)
- Tech: 4 participantes (20%)
- Varejo: 3 participantes (15%)

**Crit√©rios de Inclus√£o**:
- Experi√™ncia m√≠nima de 2 anos com Python
- Familiaridade com scikit-learn
- Experi√™ncia com deployment de modelos ML
- N√£o ter usado DeepBridge anteriormente

**Crit√©rios de Exclus√£o**:
- Desenvolvedores do DeepBridge
- Pessoas com conflito de interesse

### 2. Tarefas

Cada participante deve completar 3 tarefas:

#### Tarefa 1: Validar Fairness de Modelo
**Descri√ß√£o**: Dado um modelo de cr√©dito treinado e um dataset de teste, validar fairness em rela√ß√£o a g√™nero e ra√ßa.

**Entreg√°vel**:
- Executar 15 m√©tricas de fairness
- Identificar viola√ß√µes da regra 80% EEOC
- Interpretar resultados

**Tempo Estimado**: 5-8 minutos

**Crit√©rios de Sucesso**:
- Conseguiu criar DBDataset
- Executou experimento de fairness
- Identificou corretamente as viola√ß√µes

#### Tarefa 2: Gerar Relat√≥rio PDF Audit-Ready
**Descri√ß√£o**: Gerar relat√≥rio PDF completo com todos os resultados de valida√ß√£o.

**Entreg√°vel**:
- PDF profissional com visualiza√ß√µes
- Incluir fairness, robustez, incerteza
- Personalizar com logo da empresa (fornecido)

**Tempo Estimado**: 2-4 minutos

**Crit√©rios de Sucesso**:
- Relat√≥rio gerado com sucesso
- Inclui todas as se√ß√µes esperadas
- Customiza√ß√£o aplicada corretamente

#### Tarefa 3: Integrar Valida√ß√£o em Pipeline CI/CD
**Descri√ß√£o**: Criar script Python que integra valida√ß√£o DeepBridge em pipeline de CI/CD.

**Entreg√°vel**:
- Script que executa valida√ß√£o automaticamente
- Retorna exit code apropriado se viola√ß√µes detectadas
- Salva relat√≥rio em diret√≥rio espec√≠fico

**Tempo Estimado**: 5-8 minutos

**Crit√©rios de Sucesso**:
- Script funciona corretamente
- Detecta viola√ß√µes e retorna exit code != 0
- Relat√≥rio salvo no local correto

### 3. Procedimento

#### Prepara√ß√£o (Antes da Sess√£o)
1. **Enviar Material Antecipadamente**:
   - Instru√ß√µes de instala√ß√£o do DeepBridge
   - Link para documenta√ß√£o
   - Descri√ß√£o do estudo e consentimento informado

2. **Setup do Ambiente**:
   - Ambiente virtual Python com DeepBridge instalado
   - Datasets e modelos pr√©-carregados
   - Jupyter notebook com c√©lulas template

#### Durante a Sess√£o (60 minutos)
1. **Introdu√ß√£o (5 min)**:
   - Explica√ß√£o do estudo
   - Consentimento informado
   - Question√°rio demogr√°fico

2. **Tutorial (10 min)**:
   - Overview r√°pido do DeepBridge (5 min)
   - Demonstra√ß√£o de exemplo simples (5 min)

3. **Execu√ß√£o das Tarefas (30 min)**:
   - Participante trabalha de forma independente
   - Observador toma notas (sem interven√ß√£o)
   - Think-aloud protocol (participante verbaliza pensamento)

4. **Question√°rios (10 min)**:
   - SUS (System Usability Scale)
   - NASA TLX (Task Load Index)
   - Quest√µes abertas sobre experi√™ncia

5. **Entrevista Semi-Estruturada (5 min)**:
   - Pontos positivos
   - Pontos negativos
   - Sugest√µes de melhoria

#### P√≥s-Sess√£o
- An√°lise das grava√ß√µes (se consentido)
- Compila√ß√£o de m√©tricas
- An√°lise qualitativa de feedback

### 4. Instrumentos de Medi√ß√£o

#### System Usability Scale (SUS)

Question√°rio de 10 itens (escala Likert 1-5):

1. Acho que gostaria de usar este sistema frequentemente
2. Achei o sistema desnecessariamente complexo
3. Achei o sistema f√°cil de usar
4. Acho que precisaria de suporte t√©cnico para usar este sistema
5. Achei que as v√°rias fun√ß√µes neste sistema estavam bem integradas
6. Achei que havia muita inconsist√™ncia neste sistema
7. Imagino que a maioria das pessoas aprenderia a usar este sistema rapidamente
8. Achei o sistema muito complicado de usar
9. Senti-me muito confiante usando o sistema
10. Precisei aprender muitas coisas antes de come√ßar a usar este sistema

**C√°lculo**:
```python
def calculate_sus_score(responses):
    # responses: lista de 10 respostas (1-5)
    # Itens √≠mpares (1,3,5,7,9): contribui√ß√£o = resposta - 1
    # Itens pares (2,4,6,8,10): contribui√ß√£o = 5 - resposta

    score = 0
    for i, response in enumerate(responses):
        if i % 2 == 0:  # √≠mpar (0-indexed)
            score += (response - 1)
        else:  # par
            score += (5 - response)

    return score * 2.5  # Escala 0-100
```

**Interpreta√ß√£o**:
- < 50: Abaixo da m√©dia (poor)
- 50-70: M√©dia (ok)
- 70-85: Boa (good)
- 85-90: Excelente (excellent) - **Top 10%**
- > 90: Melhor imagin√°vel (best imaginable)

**Meta**: SUS Score ‚â• 85 (excelente)

#### NASA Task Load Index (TLX)

Avalia carga cognitiva em 6 dimens√µes (escala 0-100):

1. **Mental Demand**: Qu√£o mentalmente exigente foi a tarefa?
2. **Physical Demand**: Qu√£o fisicamente exigente foi a tarefa?
3. **Temporal Demand**: Qu√£o apressado voc√™ se sentiu?
4. **Performance**: Qu√£o bem sucedido voc√™ acha que foi?
5. **Effort**: Quanto esfor√ßo foi necess√°rio?
6. **Frustration**: Qu√£o frustrado voc√™ se sentiu?

**C√°lculo**:
```python
def calculate_nasa_tlx(dimensions):
    # dimensions: dict com 6 valores (0-100)
    return sum(dimensions.values()) / 6
```

**Interpreta√ß√£o**:
- < 20: Carga muito baixa
- 20-40: Carga baixa
- 40-60: Carga moderada
- 60-80: Carga alta
- > 80: Carga muito alta

**Meta**: NASA TLX ‚â§ 30 (carga baixa)

### 5. M√©tricas Objetivas

#### Taxa de Sucesso
```python
success_rate = (participantes_que_completaram_todas_tarefas / total_participantes) * 100
```
**Meta**: ‚â• 90%

#### Tempo para Completar
```python
# Por tarefa
time_task_1 = [tempo_participante_1, ..., tempo_participante_20]
time_task_2 = [...]
time_task_3 = [...]

# Total
time_total = [sum([t1, t2, t3]) for t1, t2, t3 in zip(time_task_1, time_task_2, time_task_3)]
```

**Meta**: Tempo m√©dio ‚â§ 15 minutos (vs. 45 min estimado com ferramentas fragmentadas)

#### Erros Cometidos
Categorias:
- Erro de sintaxe Python
- Erro de API (uso incorreto do DeepBridge)
- Erro conceitual (interpreta√ß√£o incorreta de m√©trica)
- Outro

**Meta**: M√©dia ‚â§ 2 erros por participante

## Resultados Esperados

### Quantitativos

| M√©trica | Meta | Resultado Esperado |
|---------|------|-------------------|
| SUS Score | ‚â• 85 | 87.5 ¬± 3.2 |
| NASA TLX | ‚â§ 30 | 28 ¬± 5.1 |
| Taxa de Sucesso | ‚â• 90% | 95% (19/20) |
| Tempo M√©dio Total | ‚â§ 15 min | 12 ¬± 2.5 min |
| Tempo Tarefa 1 | ‚â§ 8 min | 6.5 ¬± 1.2 min |
| Tempo Tarefa 2 | ‚â§ 4 min | 2.8 ¬± 0.8 min |
| Tempo Tarefa 3 | ‚â§ 8 min | 6.2 ¬± 1.5 min |
| Erros M√©dios | ‚â§ 2 | 1.3 ¬± 0.9 |

### Qualitativos

**Feedback Positivo Esperado** (% de participantes):
- "API intuitiva, similar ao scikit-learn": 75% (15/20)
- "Relat√≥rios profissionais sem esfor√ßo": 90% (18/20)
- "Conformidade autom√°tica √© revolucion√°ria": 60% (12/20)
- "Documenta√ß√£o clara e completa": 70% (14/20)
- "F√°cil de integrar no workflow existente": 65% (13/20)

**Feedback Negativo Esperado** (% de participantes):
- "Instala√ß√£o inicial lenta (muitas depend√™ncias)": 40% (8/20)
- "Desejo mais templates de relat√≥rio": 25% (5/20)
- "Tempo de execu√ß√£o poderia ser mais r√°pido": 15% (3/20)
- "Alguns erros dif√≠ceis de debugar": 10% (2/20)

## An√°lise Estat√≠stica

### SUS Score

**Teste**: One-sample t-test
**H0**: SUS Score = 68 (m√©dia global para ferramentas de software)
**H1**: SUS Score > 68

```python
from scipy import stats

sus_scores = [87, 89, 85, ...]  # 20 scores
t_stat, p_value = stats.ttest_1samp(sus_scores, 68, alternative='greater')

# Esperado: p < 0.001
```

### Compara√ß√£o com Baseline

**Baseline**: Workflow fragmentado (AIF360 + Fairlearn + etc.)
- Tempo estimado: 45 minutos
- SUS Score estimado: 55-65 (baseado em literatura)

**Teste**: Independent t-test
```python
# DeepBridge vs. Baseline (se tivermos grupo controle)
t_stat, p_value = stats.ttest_ind(times_deepbridge, times_baseline)
```

## Recrutamento

### Estrat√©gias
1. **LinkedIn**: Postagem em grupos de Data Science/ML
2. **Meetups**: Apresenta√ß√£o em meetups locais
3. **Empresas Parceiras**: Solicitar indica√ß√µes
4. **Plataformas**: User Testing, UserBrain

### Incentivos
- Compensa√ß√£o: $50-100 por participante (60 min)
- Certificado de participa√ß√£o
- Early access ao DeepBridge
- Relat√≥rio de resultados do estudo

## Considera√ß√µes √âticas

### Consentimento Informado
- Explica√ß√£o clara do prop√≥sito do estudo
- Direito de desistir a qualquer momento
- Anonimiza√ß√£o dos dados
- Uso dos dados apenas para pesquisa

### Privacidade
- Dados pessoais anonimizados
- Grava√ß√µes (se houver) armazenadas de forma segura
- Destrui√ß√£o ap√≥s an√°lise (se consentido)

### Aprova√ß√£o
- Submeter protocolo para comit√™ de √©tica (se institui√ß√£o acad√™mica)
- Obter consentimento por escrito

## Outputs

### Dados Brutos
- `results/03_usability_sus_scores.csv`
- `results/03_usability_nasa_tlx.csv`
- `results/03_usability_task_times.csv`
- `results/03_usability_errors.csv`
- `results/03_usability_feedback.json`

### An√°lise
- `results/03_usability_statistical_analysis.json`
- `notebooks/03_usability_analysis.ipynb`

### Figuras
- `figures/sus_score_distribution.pdf`
- `figures/nasa_tlx_dimensions.pdf`
- `figures/task_completion_times.pdf`
- `figures/success_rate_by_task.pdf`

### Tabelas
- `tables/usability_summary.tex`

## Cronograma

**Total: 3-4 semanas**

### Semana 1: Prepara√ß√£o
- Finalizar protocolo
- Criar materiais (tutorial, tarefas, question√°rios)
- Recrutar participantes
- Obter aprova√ß√£o √©tica (se necess√°rio)

### Semana 2-3: Coleta de Dados
- Executar sess√µes com 20 participantes
- 2-3 sess√µes por dia
- Transcrever notas e feedbacks

### Semana 4: An√°lise
- Calcular m√©tricas quantitativas
- An√°lise qualitativa de feedback
- An√°lise estat√≠stica
- Gerar visualiza√ß√µes e tabelas

## Checklist

- [ ] Finalizar protocolo do estudo
- [ ] Criar tutorial e materiais de treinamento
- [ ] Preparar tarefas e datasets
- [ ] Preparar question√°rios (SUS, NASA TLX)
- [ ] Criar roteiro de entrevista semi-estruturada
- [ ] Recrutar 20 participantes
- [ ] Obter consentimento informado
- [ ] Executar sess√µes piloto (2-3)
- [ ] Ajustar protocolo baseado em piloto
- [ ] Executar 20 sess√µes principais
- [ ] Transcrever notas e feedbacks
- [ ] Calcular SUS scores
- [ ] Calcular NASA TLX scores
- [ ] Analisar tempos de completa√ß√£o
- [ ] An√°lise qualitativa de feedback
- [ ] An√°lise estat√≠stica
- [ ] Gerar visualiza√ß√µes
- [ ] Formatar tabelas LaTeX
- [ ] Documentar metodologia completa

## Prioridade

üü° **M√âDIA** - Importante para demonstrar usabilidade, mas n√£o cr√≠tico para funcionalidade

## Tempo Estimado

**3-4 semanas**

## Refer√™ncias

- Brooke, J. (1996). SUS: A "quick and dirty" usability scale.
- Hart, S. G., & Staveland, L. E. (1988). Development of NASA-TLX.
- Bangor, A., Kortum, P., & Miller, J. (2009). Determining what individual SUS scores mean: Adding an adjective rating scale.
