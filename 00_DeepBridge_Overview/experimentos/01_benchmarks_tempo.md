# Experimento 1: Benchmarks de Tempo

## Objetivo

Comprovar a afirma√ß√£o do paper de que **DeepBridge reduz o tempo de valida√ß√£o em 89%** (17 min vs. 150 min) comparado com workflow manual usando ferramentas fragmentadas.

## Afirma√ß√µes a Comprovar

### Tabela de Benchmarks (Se√ß√£o 6.3)

| Tarefa | DeepBridge | Fragmentado | Afirma√ß√£o |
|--------|------------|-------------|-----------|
| Fairness (15 m√©tricas) | 5 min | 30 min | ‚è≥ A comprovar |
| Robustez | 7 min | 25 min | ‚è≥ A comprovar |
| Incerteza | 3 min | 20 min | ‚è≥ A comprovar |
| Resili√™ncia | 2 min | 15 min | ‚è≥ A comprovar |
| Gera√ß√£o de relat√≥rio | <1 min | 60 min | ‚è≥ A comprovar |
| **Total** | **17 min** | **150 min** | **89% redu√ß√£o** |

### Decomposi√ß√£o dos Ganhos (Se√ß√£o 6.3)

- API unificada: 50% do ganho
- Paraleliza√ß√£o: 30% do ganho
- Caching: 10% do ganho
- Automa√ß√£o de relat√≥rios: 10% do ganho

## Metodologia

### 1. Setup do Experimento

#### Dataset de Teste
- **Nome**: Adult Income Dataset (UCI)
- **Tamanho**: 48,842 amostras
- **Features**: 14 features (6 num√©ricas, 8 categ√≥ricas)
- **Target**: Bin√°rio (income >50K ou ‚â§50K)
- **Atributos Protegidos**: gender, race, age

#### Modelo
- **Algoritmo**: XGBoost
- **Hiperpar√¢metros**:
  - n_estimators: 100
  - max_depth: 6
  - learning_rate: 0.1
- **Split**: 80% treino, 20% teste
- **Seed**: 42 (para reprodutibilidade)

### 2. Workflow DeepBridge

```python
import time
from deepbridge import DBDataset, Experiment

# Timer para cada etapa
times_deepbridge = {}

# 1. Criar dataset (inclu√≠do no tempo)
start = time.time()
dataset = DBDataset(
    data=df_test,
    target_column='income',
    model=xgb_model,
    protected_attributes=['gender', 'race', 'age']
)
times_deepbridge['setup'] = time.time() - start

# 2. Fairness (15 m√©tricas)
start = time.time()
exp_fairness = Experiment(dataset, tests=['fairness'])
results_fairness = exp_fairness.run_tests()
times_deepbridge['fairness'] = time.time() - start

# 3. Robustness
start = time.time()
exp_robustness = Experiment(dataset, tests=['robustness'])
results_robustness = exp_robustness.run_tests()
times_deepbridge['robustness'] = time.time() - start

# 4. Uncertainty
start = time.time()
exp_uncertainty = Experiment(dataset, tests=['uncertainty'])
results_uncertainty = exp_uncertainty.run_tests()
times_deepbridge['uncertainty'] = time.time() - start

# 5. Resilience
start = time.time()
exp_resilience = Experiment(dataset, tests=['resilience'])
results_resilience = exp_resilience.run_tests()
times_deepbridge['resilience'] = time.time() - start

# 6. Report Generation
start = time.time()
exp_all = Experiment(dataset, tests='all')
exp_all.save_pdf('report.pdf')
times_deepbridge['report'] = time.time() - start

# Total time
times_deepbridge['total'] = sum(times_deepbridge.values())
```

### 3. Workflow Fragmentado (Baseline)

```python
import time
from aif360.datasets import BinaryLabelDataset
from aif360.metrics import ClassificationMetric
from fairlearn.metrics import demographic_parity_difference
from alibi_detect.cd import TabularDrift
from uq360.algorithms.posthocuq import PosthocUQ
import matplotlib.pyplot as plt
from fpdf import FPDF

times_fragmented = {}

# 1. Fairness com AIF360 + Fairlearn (15 m√©tricas)
start = time.time()

# Converter para formato AIF360
aif_dataset = BinaryLabelDataset(
    df=df_test,
    label_names=['income'],
    protected_attribute_names=['gender', 'race', 'age']
)

# Calcular 15 m√©tricas manualmente
# - Demographic Parity
# - Equal Opportunity
# - Equalized Odds
# - Disparate Impact
# ... (11 m√©tricas adicionais)

# Para cada m√©trica, precisa:
# 1. Preparar dados no formato correto
# 2. Calcular m√©trica
# 3. Armazenar resultado

times_fragmented['fairness'] = time.time() - start

# 2. Robustness com Alibi Detect
start = time.time()

# Converter para NumPy
X_test_np = df_test.drop('income', axis=1).values

# Testar perturba√ß√µes
# - Noise injection
# - Feature permutation
# - Adversarial examples (se dispon√≠vel)

times_fragmented['robustness'] = time.time() - start

# 3. Uncertainty com UQ360
start = time.time()

# Converter para formato UQ360
# Calcular calibra√ß√£o, intervalos de predi√ß√£o, etc.

times_fragmented['uncertainty'] = time.time() - start

# 4. Resilience com Evidently AI ou custom
start = time.time()

# Calcular drift metrics (PSI, KL, etc.)

times_fragmented['resilience'] = time.time() - start

# 5. Report Generation Manual
start = time.time()

# Criar PDF manualmente
pdf = FPDF()
pdf.add_page()

# Para cada m√©trica:
# 1. Criar visualiza√ß√£o com matplotlib
# 2. Salvar como imagem
# 3. Adicionar ao PDF
# 4. Adicionar texto explicativo

# Isso deve levar ~60 minutos para relat√≥rio completo

times_fragmented['report'] = time.time() - start

# Total time
times_fragmented['total'] = sum(times_fragmented.values())
```

### 4. Medi√ß√µes

**N√∫mero de Runs**: 10 execu√ß√µes independentes
**Seed**: Vari√°vel entre runs (42, 43, 44, ..., 51)
**M√©tricas**:
- Tempo m√©dio (segundos)
- Desvio padr√£o
- Tempo m√≠nimo
- Tempo m√°ximo

## Resultados Esperados

### Tempos DeepBridge (minutos)

| Tarefa | M√©dia | Std | Min | Max |
|--------|-------|-----|-----|-----|
| Fairness | 5.0 | 0.3 | 4.7 | 5.4 |
| Robustez | 7.0 | 0.4 | 6.5 | 7.6 |
| Incerteza | 3.0 | 0.2 | 2.8 | 3.3 |
| Resili√™ncia | 2.0 | 0.1 | 1.9 | 2.2 |
| Relat√≥rio | 0.8 | 0.1 | 0.7 | 1.0 |
| **Total** | **17.8** | **0.8** | **16.6** | **19.5** |

### Tempos Fragmentado (minutos)

| Tarefa | M√©dia | Std | Min | Max |
|--------|-------|-----|-----|-----|
| Fairness | 30.0 | 2.5 | 27.0 | 33.0 |
| Robustez | 25.0 | 2.0 | 22.5 | 27.5 |
| Incerteza | 20.0 | 1.8 | 18.0 | 22.0 |
| Resili√™ncia | 15.0 | 1.5 | 13.0 | 17.0 |
| Relat√≥rio | 60.0 | 5.0 | 55.0 | 65.0 |
| **Total** | **150.0** | **10.0** | **135.5** | **164.5** |

### Speedup

- **Speedup Global**: 150 / 17 = **8.8√ó**
- **Redu√ß√£o Percentual**: (150 - 17) / 150 = **89%**

## An√°lise Estat√≠stica

### Teste de Hip√≥tese

**H0**: N√£o h√° diferen√ßa significativa entre DeepBridge e workflow fragmentado
**H1**: DeepBridge √© significativamente mais r√°pido

**Teste**: Paired t-test (duas caudas)
**N√≠vel de Signific√¢ncia**: Œ± = 0.05

```python
from scipy import stats

# Paired t-test para cada tarefa
for task in ['fairness', 'robustness', 'uncertainty', 'resilience', 'report']:
    t_stat, p_value = stats.ttest_rel(
        times_deepbridge[task],  # 10 runs
        times_fragmented[task]   # 10 runs
    )
    print(f"{task}: t={t_stat:.2f}, p={p_value:.4f}")

# Esperado: p < 0.001 para todas as tarefas
```

## Valida√ß√£o dos Ganhos por Componente

Para comprovar que:
- API unificada contribui 50%
- Paraleliza√ß√£o contribui 30%
- Caching contribui 10%
- Automa√ß√£o relat√≥rios contribui 10%

### Ablation Study

```python
# Baseline: DeepBridge completo
time_full = 17 min

# Ablation 1: Sem API unificada (usar convers√µes manuais)
time_no_api = measure_time_without_unified_api()
gain_api = (time_no_api - time_full) / (150 - 17)  # % do ganho total
# Esperado: ~50%

# Ablation 2: Sem paraleliza√ß√£o
time_no_parallel = measure_time_sequential()
gain_parallel = (time_no_parallel - time_full) / (150 - 17)
# Esperado: ~30%

# Ablation 3: Sem caching
time_no_cache = measure_time_without_cache()
gain_cache = (time_no_cache - time_full) / (150 - 17)
# Esperado: ~10%

# Ablation 4: Relat√≥rios manuais
time_manual_report = measure_time_manual_reporting()
gain_report = (time_manual_report - time_full) / (150 - 17)
# Esperado: ~10%
```

## Ambiente de Execu√ß√£o

### Hardware
- **CPU**: Intel i7-12700K (12 cores, 20 threads) ou similar
- **RAM**: 32GB DDR4
- **Storage**: SSD NVMe
- **GPU**: N√£o necess√°ria para este experimento

### Software
- **OS**: Ubuntu 22.04 LTS
- **Python**: 3.10
- **DeepBridge**: vers√£o atual
- **Bibliotecas de Compara√ß√£o**:
  - aif360==0.5.0
  - fairlearn==0.9.0
  - alibi-detect==0.11.4
  - uq360==0.3.0
  - evidently==0.4.0

## Scripts

### Script Principal
`/experimentos/scripts/01_time_benchmarks.py`

### An√°lise de Resultados
`/experimentos/notebooks/01_time_benchmarks_analysis.ipynb`

### Gera√ß√£o de Figuras
`/experimentos/scripts/01_generate_figures.py`

## Outputs Esperados

1. **CSV com Tempos Brutos**:
   - `results/01_deepbridge_times.csv`
   - `results/01_fragmented_times.csv`

2. **Figuras para Paper**:
   - `figures/time_comparison_barplot.pdf`
   - `figures/speedup_by_task.pdf`
   - `figures/ablation_study.pdf`

3. **Tabela LaTeX**:
   - `tables/time_benchmarks.tex`

4. **An√°lise Estat√≠stica**:
   - `results/statistical_analysis.json`

## Checklist

- [ ] Implementar script de benchmark DeepBridge
- [ ] Implementar script de benchmark fragmentado
- [ ] Executar 10 runs para cada workflow
- [ ] Calcular estat√≠sticas (m√©dia, std, min, max)
- [ ] Realizar teste t pareado
- [ ] Implementar ablation study
- [ ] Gerar visualiza√ß√µes
- [ ] Formatar tabela em LaTeX
- [ ] Documentar ambiente de execu√ß√£o
- [ ] Validar reprodutibilidade

## Prioridade

üî¥ **ALTA** - Este √© um dos resultados centrais do paper

## Tempo Estimado

**2-3 semanas**:
- Semana 1: Implementa√ß√£o dos scripts
- Semana 2: Execu√ß√£o dos experimentos e coleta de dados
- Semana 3: An√°lise estat√≠stica e gera√ß√£o de visualiza√ß√µes
