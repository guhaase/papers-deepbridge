# Roadmap de Corre√ß√µes - DeepBridge Experiments

**Objetivo**: Tornar os experimentos public√°veis em confer√™ncia de qualidade
**Prazo sugerido**: 4-6 semanas
**Prioridade**: Por impacto na validade cient√≠fica

---

## üî¥ CR√çTICO - Semana 1-2

### 1.1 Experimento 5: Implementar Baseline Real

**Problema**: Baseline atual √© simulado, n√£o usa ferramentas reais
**Impacto**: Invalida completamente as conclus√µes
**Tempo**: 4-5 dias

**Tarefas**:

```bash
# 1. Instalar depend√™ncias
pip install aif360 fairlearn

# 2. Criar script validate_baseline_REAL.py
```

**Implementa√ß√£o**:

```python
# experimentos/05_conformidade/scripts/validate_baseline_real.py

from aif360.datasets import BinaryLabelDataset
from aif360.metrics import BinaryLabelDatasetMetric

def validate_with_aif360(case_df):
    """Valida√ß√£o real com AIF360"""
    # Converter para formato AIF360
    dataset = BinaryLabelDataset(
        df=case_df,
        label_names=['approved'],
        protected_attribute_names=['gender', 'race'],
        favorable_label=1,
        unfavorable_label=0
    )

    # Calcular m√©tricas reais
    metric = BinaryLabelDatasetMetric(
        dataset,
        unprivileged_groups=[{'gender': 0}],
        privileged_groups=[{'gender': 1}]
    )

    # Disparate Impact real
    di = metric.disparate_impact()

    return {
        'disparate_impact': di,
        'compliant': di >= 0.80
    }

# Executar para 50 casos e comparar com DeepBridge
```

**Valida√ß√£o**:
- [ ] AIF360 instalado e funcionando
- [ ] Valida√ß√£o executada nos 50 casos
- [ ] Resultados salvos em `baseline_real_results.json`
- [ ] Compara√ß√£o estat√≠stica atualizada
- [ ] Tabelas LaTeX atualizadas

**Resultado esperado**: p-value < 0.01 (mais robusto que 0.0499)

---

### 1.2 Experimento 6: Decis√£o Cr√≠tica

**Problema**: Tempos s√£o completamente simulados
**Impacto**: Experimento inteiro √© inv√°lido
**Tempo**: Decidir em 1 dia, implementar em 1-2 semanas

**Op√ß√£o A: Remover Experimento** ‚è±Ô∏è 1 hora
```bash
# Mais r√°pido e honesto
# Remove Experimento 6 do paper
# Foca nos experimentos v√°lidos
```

**Vantagens**:
- R√°pido
- Honesto
- Evita trabalho massivo

**Desvantagens**:
- Perde an√°lise de abla√ß√£o
- Paper fica com menos experimentos

**Op√ß√£o B: Implementar Abla√ß√£o Real** ‚è±Ô∏è 1-2 semanas
```python
# experimentos/06_ablation_studies/scripts/run_ablation_REAL.py

class DeepBridgeAblation:
    """Vers√µes reais com componentes desabilitados"""

    def run_without_unified_api(self, X, y, model):
        """Simula workflow fragmentado REAL"""
        start = time.time()

        # CONVERS√ÉO 1: Para AIF360
        conversion_start = time.time()
        aif_data = self._convert_to_aif360(X, y)
        conversion_time_1 = time.time() - conversion_start

        # CONVERS√ÉO 2: Para Alibi
        conversion_start = time.time()
        alibi_data = self._convert_to_alibi(X)
        conversion_time_2 = time.time() - conversion_start

        # CONVERS√ÉO 3: Para UQ360
        conversion_start = time.time()
        uq_data = self._convert_to_uq360(X, y)
        conversion_time_3 = time.time() - conversion_start

        # Executar valida√ß√µes
        fairness_results = self._run_fairness_aif360(aif_data)
        robustness_results = self._run_robustness_alibi(alibi_data)
        uncertainty_results = self._run_uncertainty_uq360(uq_data)

        total_time = time.time() - start

        return {
            'total_time': total_time,
            'conversion_overhead': conversion_time_1 + conversion_time_2 + conversion_time_3
        }

    def run_without_parallelization(self, X, y, model):
        """Execu√ß√£o sequencial REAL"""
        start = time.time()

        # For√ßar execu√ß√£o serial (n√£o paralela)
        fairness = self._run_fairness_sequential(X, y, model)
        robustness = self._run_robustness_sequential(X, y, model)
        uncertainty = self._run_uncertainty_sequential(X, y, model)
        resilience = self._run_resilience_sequential(X, y, model)

        total_time = time.time() - start
        return {'total_time': total_time}

    def run_without_caching(self, X, y, model):
        """Sem cache - recomputa predi√ß√µes"""
        start = time.time()

        # Desabilitar cache
        model.predict.cache_clear()  # Se usar functools.lru_cache

        # Cada valida√ß√£o chama predict() novamente
        for _ in range(4):  # 4 valida√ß√µes
            _ = model.predict(X)  # Sem cache!

        total_time = time.time() - start
        return {'total_time': total_time}
```

**Valida√ß√£o**:
- [ ] Implementadas 4 vers√µes ablation
- [ ] Executados 30 runs por vers√£o (6 √ó 30 = 180 runs)
- [ ] Tempos medidos com timer apropriado
- [ ] An√°lise estat√≠stica (ANOVA, Tukey HSD)
- [ ] Intervalos de confian√ßa reportados

**Resultado esperado**:
- Unified API: 40-70 min overhead (n√£o 66 min fixo)
- Parallelization: 20-40 min ganho
- Caching: 5-15 min ganho
- **Total speedup**: 4-7√ó (mais realista que 8.9√ó)

**Recomenda√ß√£o**: ‚úÖ **Op√ß√£o A** (remover) se prazo curto, **Op√ß√£o B** (implementar) se prazo longo

---

### 1.3 Experimento 1: Completar Fairness Benchmark

**Problema**: Fairness test retornou 0.0s (sem dados)
**Impacto**: Benchmark incompleto, speedup n√£o validado
**Tempo**: 2 dias

**Tarefas**:

```python
# experimentos/01_benchmarks_tempo/scripts/benchmark_deepbridge.py

def run_fairness_test_REAL(dataset, model):
    """Executar teste de fairness real"""
    start = time.time()

    # Detectar atributos protegidos
    protected_attrs = dataset.detect_protected_attributes()

    # Calcular m√©tricas de fairness
    fairness_results = {}
    for attr in protected_attrs:
        di = calculate_disparate_impact(dataset, attr)
        fairness_results[attr] = {
            'disparate_impact': di,
            'compliant': di >= 0.80
        }

    elapsed = time.time() - start

    return {
        'execution_time': elapsed,
        'results': fairness_results
    }

# Executar para Adult Income dataset
# Salvar em deepbridge_times_REAL.json
```

**Valida√ß√£o**:
- [ ] Fairness test executado com sucesso
- [ ] Tempo medido > 0 (esperado: 5-15 segundos)
- [ ] Resultados salvos corretamente
- [ ] Total atualizado (esperado: 40-60 segundos total)

---

## üü° IMPORTANTE - Semana 3-4

### 2.1 Adicionar Mais Datasets

**Problema**: Apenas 1 dataset (Adult Income)
**Impacto**: Generaliza√ß√£o question√°vel
**Tempo**: 3-4 dias

**Datasets sugeridos**:

1. **COMPAS** (Criminal recidivism)
   - Protected: race, gender
   - Target: recidivism
   - Fonte: ProPublica

2. **German Credit**
   - Protected: age, gender
   - Target: credit approval
   - Fonte: UCI ML Repository

3. **Law School Admissions**
   - Protected: race, gender
   - Target: admission decision
   - Fonte: Fair ML datasets

**Implementa√ß√£o**:

```bash
# experimentos/01_benchmarks_tempo/data/
‚îú‚îÄ‚îÄ adult_income/      # Existente
‚îú‚îÄ‚îÄ compas/            # Novo
‚îú‚îÄ‚îÄ german_credit/     # Novo
‚îî‚îÄ‚îÄ law_school/        # Novo

# Executar benchmarks em todos
for dataset in adult_income compas german_credit law_school; do
    python scripts/run_experiment.py --dataset $dataset
done

# Agregar resultados
python scripts/aggregate_multidata.py
```

**Valida√ß√£o**:
- [ ] 3 datasets adicionais processados
- [ ] Benchmarks executados em todos
- [ ] Resultados consistentes (speedup 5-9√ó)
- [ ] Tabela comparativa criada

**Resultado esperado**:
```
Dataset         DeepBridge   Fragmented   Speedup
--------------------------------------------------
Adult Income    23.4s        150s         6.4√ó
COMPAS          18.2s        125s         6.9√ó
German Credit   15.1s        95s          6.3√ó
Law School      21.3s        140s         6.6√ó
--------------------------------------------------
M√âDIA           19.5s        127.5s       6.5√ó ¬± 0.3√ó
```

---

### 2.2 Compara√ß√£o com Ferramentas Existentes

**Problema**: N√£o compara com ferramentas al√©m do baseline fragmentado
**Impacto**: Falta contexto, reviewers perguntar√£o
**Tempo**: 2-3 dias

**Ferramentas para comparar**:

1. **Fairlearn** (Microsoft)
2. **AIF360** (IBM)
3. **What-If Tool** (Google)
4. **Alibi** (Seldon)

**Implementa√ß√£o**:

```python
# experimentos/07_comparacao_ferramentas/ (NOVO)

def compare_with_fairlearn(dataset):
    """Comparar com Fairlearn"""
    from fairlearn.metrics import MetricFrame

    start = time.time()

    # Setup Fairlearn
    metric_frame = MetricFrame(
        metrics={'accuracy': accuracy_score, 'selection_rate': selection_rate},
        y_true=dataset.y,
        y_pred=dataset.predictions,
        sensitive_features=dataset.protected_attributes
    )

    # Calcular disparate impact
    results = metric_frame.by_group

    elapsed = time.time() - start

    return {
        'tool': 'Fairlearn',
        'time': elapsed,
        'results': results
    }

# Comparar: DeepBridge vs Fairlearn vs AIF360 vs Alibi
```

**Valida√ß√£o**:
- [ ] 3-4 ferramentas integradas
- [ ] Benchmarks executados
- [ ] Tabela comparativa criada
- [ ] Discuss√£o de tradeoffs

**Resultado esperado**:
```
Ferramenta    Tempo   M√©tricas Cobertas   Facilidade Uso
------------------------------------------------------------
DeepBridge    23s     Fairness, Rob, Unc  ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
Fairlearn     45s     Fairness only       ‚≠ê‚≠ê‚≠ê‚≠ê
AIF360        60s     Fairness only       ‚≠ê‚≠ê‚≠ê
Alibi         40s     Robustness only     ‚≠ê‚≠ê‚≠ê
```

---

## üü¢ OPCIONAL - Semana 5-6

### 3.1 Survey de Usabilidade Real

**Problema**: Exp 3 usa dados mock
**Impacto**: Baixo (usabilidade √© secund√°ria)
**Tempo**: 1 semana

**Implementa√ß√£o**:

1. Recrutar 10-15 participantes (colegas, estudantes)
2. Tarefa: "Validar modelo de cr√©dito para fairness"
3. Grupos:
   - Grupo A: DeepBridge
   - Grupo B: Fairlearn
4. Medir:
   - Tempo para completar
   - Linhas de c√≥digo
   - Satisfa√ß√£o (escala Likert)

**Valida√ß√£o**:
- [ ] ‚â•10 participantes
- [ ] Diferen√ßa significativa (t-test)
- [ ] Question√°rio de satisfa√ß√£o

---

### 3.2 An√°lise de Sensibilidade

**Problema**: Falta robustez das conclus√µes
**Impacto**: M√©dio
**Tempo**: 2-3 dias

**Implementa√ß√£o**:

```python
# Testar diferentes thresholds de DI
for threshold in [0.75, 0.80, 0.85, 0.90]:
    precision, recall = evaluate_compliance(threshold)
    plot(threshold, precision, recall)

# Testar com diferentes tamanhos de amostra
for n_samples in [100, 500, 1000, 5000]:
    speedup = run_benchmark(n_samples)
    plot(n_samples, speedup)
```

**Valida√ß√£o**:
- [ ] Resultados consistentes entre thresholds
- [ ] Speedup escala com tamanho
- [ ] Gr√°ficos de sensibilidade

---

## üìä Checklist de Valida√ß√£o Final

Antes de submeter, garantir:

### Validade Interna
- [ ] Baseline real (n√£o simulado)
- [ ] Tempos medidos (n√£o simulados)
- [ ] M√©tricas corretas
- [ ] An√°lise estat√≠stica apropriada

### Validade Externa
- [ ] ‚â•3 datasets testados
- [ ] Compara√ß√£o com ‚â•2 ferramentas
- [ ] Resultados consistentes

### Reprodutibilidade
- [ ] C√≥digo p√∫blico (GitHub)
- [ ] README com instru√ß√µes claras
- [ ] Dados dispon√≠veis (ou script para gerar)
- [ ] Seeds fixos

### Escrita
- [ ] Se√ß√£o de limita√ß√µes honesta
- [ ] Claims suportados por dados
- [ ] Tabelas e figuras claras
- [ ] Amea√ßas √† validade discutidas

---

## üìÖ Timeline Sugerido

### Semana 1-2 (CR√çTICO)
- Dias 1-5: Exp 5 - Baseline real
- Dias 6-10: Exp 6 - Decis√£o e a√ß√£o

### Semana 3-4 (IMPORTANTE)
- Dias 11-14: Adicionar datasets
- Dias 15-17: Compara√ß√£o com ferramentas
- Dia 18: Completar Exp 1 fairness

### Semana 5 (AN√ÅLISE)
- Dias 19-21: An√°lise estat√≠stica completa
- Dias 22-23: Atualizar visualiza√ß√µes

### Semana 6 (ESCRITA)
- Dias 24-26: Atualizar paper
- Dias 27-28: Revis√£o interna
- Dia 29: Submeter para review interno
- Dia 30: Corre√ß√µes finais

**SUBMISS√ÉO**: Dia 35-40

---

## üí∞ Estimativa de Esfor√ßo

| Tarefa | Prioridade | Tempo | Pessoa-dias |
|--------|-----------|-------|-------------|
| Exp 5 - Baseline real | P0 | 4-5 dias | 5 |
| Exp 6 - Op√ß√£o A (remover) | P0 | 1h | 0.1 |
| Exp 6 - Op√ß√£o B (implementar) | P0 | 1-2 semanas | 10 |
| Exp 1 - Fairness | P0 | 2 dias | 2 |
| Adicionar datasets | P1 | 3-4 dias | 4 |
| Comparar ferramentas | P1 | 2-3 dias | 3 |
| Survey usabilidade | P2 | 1 semana | 5 |
| An√°lise sensibilidade | P2 | 2-3 dias | 3 |
| Escrita/revis√£o | - | 1 semana | 5 |

**TOTAL (Cen√°rio m√≠nimo)**: 10-12 pessoa-dias (2-3 semanas)
**TOTAL (Cen√°rio completo)**: 30-35 pessoa-dias (6-7 semanas)

---

## üéØ Meta Final

**Paper public√°vel em confer√™ncia Tier 2** com potencial para Tier 1 se execu√ß√£o for excelente.

**Investimento**: 4-6 semanas de trabalho focado
**Retorno**: Paper s√≥lido, cientificamente v√°lido, public√°vel

**Alternativa**: N√£o corrigir = rejei√ß√£o certa

**Decis√£o**: Nos pr√≥ximos 2 dias
