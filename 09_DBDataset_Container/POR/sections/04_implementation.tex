\section{Implementacao}

\subsection{Arquitetura de Software}

DBDataset e implementado em Python 3.9+ com arquitetura modular:

\begin{verbatim}
deepbridge/
├── core/
│   └── db_data.py              # Classe DBDataset (383 LOC)
└── utils/
    ├── dataset_factory.py      # Factory pattern
    ├── feature_manager.py      # Inferencia de features
    ├── data_validator.py       # Validacao de inputs
    ├── model_handler.py        # Gestao de modelos
    └── dataset_formatter.py    # String representation
\end{verbatim}

\subsection{Classe Principal: DBDataset}

\subsubsection{Inicializacao}

Constructor com 15+ parametros opcionais para flexibilidade:

\begin{lstlisting}[language=Python, basicstyle=\ttfamily\tiny]
class DBDataset:
    def __init__(
        self,
        data: Optional[Union[pd.DataFrame, np.ndarray, Bunch]] = None,
        train_data: Optional[Union[pd.DataFrame, np.ndarray]] = None,
        test_data: Optional[Union[pd.DataFrame, np.ndarray]] = None,
        target_column: str = None,
        features: Optional[List[str]] = None,
        categorical_features: Optional[List[str]] = None,
        max_categories: Optional[int] = None,
        test_size: float = 0.2,
        random_state: Optional[int] = None,
        stratify: bool = False,
        model: Optional[Union[str, Path, Any]] = None,
        train_predictions: Optional[np.ndarray] = None,
        test_predictions: Optional[np.ndarray] = None,
        prob_cols: Optional[List[str]] = None
    ):
        # Inicializacao de componentes helpers
        self._validator = DataValidator()
        self._feature_manager = FeatureManager()
        self._model_handler = ModelHandler()
        self._formatter = DatasetFormatter()

        # Validacao e processamento
        self._initialize_data(...)
        self._initialize_features(...)
        self._initialize_model(...)
\end{lstlisting}

\subsubsection{Validacao de Inputs}

\texttt{DataValidator} implementa validacoes criticas:

\begin{lstlisting}[language=Python, basicstyle=\ttfamily\tiny]
class DataValidator:
    def validate_data(self, data, train_data, test_data):
        # Valida que pelo menos um modo de dados fornecido
        if data is None and (train_data is None or test_data is None):
            raise ValueError(
                "Must provide either 'data' or both 'train_data' and 'test_data'"
            )

        # Valida tipos
        if data is not None and not isinstance(data, (pd.DataFrame, np.ndarray, Bunch)):
            raise TypeError("Data must be DataFrame, array, or Bunch")

        return True

    def validate_features(self, data, features, target_column, prob_cols):
        # Inferir features se nao fornecidas
        if features is None:
            exclude_cols = [target_column] + (prob_cols or [])
            features = [c for c in data.columns if c not in exclude_cols]

        # Validar existencia
        missing = set(features) - set(data.columns)
        if missing:
            raise ValueError(f"Features not found in data: {missing}")

        return features
\end{lstlisting}

\subsection{Inferencia de Features}

\subsubsection{Algoritmo de Inferencia}

\texttt{FeatureManager.infer\_categorical\_features} implementa inferencia em duas passadas:

\begin{lstlisting}[language=Python, basicstyle=\ttfamily\tiny]
class FeatureManager:
    def infer_categorical_features(
        self,
        data: pd.DataFrame,
        features: List[str],
        max_categories: Optional[int] = None
    ) -> List[str]:
        categorical = []

        for feature in features:
            col = data[feature]

            # Criterio 1: Type-based (dtype object/category)
            is_object_type = col.dtype in ['object', 'category']

            # Criterio 2: Cardinality-based
            n_unique = col.nunique()
            is_low_cardinality = (
                max_categories is not None and n_unique <= max_categories
            )

            if is_object_type or is_low_cardinality:
                categorical.append(feature)

        return categorical

    def get_numerical_features(
        self,
        all_features: List[str],
        categorical_features: List[str]
    ) -> List[str]:
        return [f for f in all_features if f not in categorical_features]
\end{lstlisting}

\subsubsection{Casos Especiais}

\paragraph{Datas} Features com dtype \texttt{datetime64} sao excluidas de inferencia automatica (requerem feature engineering manual).

\paragraph{IDs} Colunas com nomes \texttt{id}, \texttt{index}, \texttt{key} (case-insensitive) sao excluidas automaticamente.

\paragraph{Override Manual} Parametro \texttt{categorical\_features} sobrescreve inferencia:

\begin{lstlisting}[language=Python, basicstyle=\ttfamily\scriptsize]
dataset = DBDataset(
    data=df,
    target_column='y',
    categorical_features=['zip_code', 'state'],  # Override
    max_categories=10  # Aplicado apenas a features nao em override
)
\end{lstlisting}

\subsection{Gestao de Modelos e Predicoes}

\subsubsection{ModelHandler}

Implementa carregamento flexivel e geracao de predicoes:

\begin{lstlisting}[language=Python, basicstyle=\ttfamily\tiny]
class ModelHandler:
    def load_model(self, model_or_path: Union[str, Path, Any]) -> Any:
        if isinstance(model_or_path, (str, Path)):
            # Carrega de arquivo
            path = Path(model_or_path)
            if path.suffix in ['.pkl', '.joblib']:
                return joblib.load(path)
            elif path.suffix == '.h5':
                import tensorflow as tf
                return tf.keras.models.load_model(path)
            else:
                raise ValueError(f"Unsupported model format: {path.suffix}")
        else:
            # Assume objeto modelo ja carregado
            return model_or_path

    def generate_predictions(
        self,
        model: Any,
        X_train: pd.DataFrame,
        X_test: pd.DataFrame
    ) -> dict:
        predictions = {}

        # Predicoes de classe
        predictions['train_pred'] = model.predict(X_train)
        predictions['test_pred'] = model.predict(X_test)

        # Probabilidades (se disponivel)
        if hasattr(model, 'predict_proba'):
            predictions['train_proba'] = model.predict_proba(X_train)
            predictions['test_proba'] = model.predict_proba(X_test)

        return predictions

    def validate_model_compatibility(
        self,
        model: Any,
        n_features: int
    ) -> None:
        # Valida numero de features para scikit-learn models
        if hasattr(model, 'n_features_in_'):
            if model.n_features_in_ != n_features:
                raise ValueError(
                    f"Model expects {model.n_features_in_} features, "
                    f"but data has {n_features}"
                )
\end{lstlisting}

\subsection{Conversao de Formatos}

\subsubsection{Suporte a NumPy Arrays}

DBDataset converte arrays para DataFrames com feature names gerados:

\begin{lstlisting}[language=Python, basicstyle=\ttfamily\scriptsize]
def _convert_to_dataframe(self, data: Union[pd.DataFrame, np.ndarray]) -> pd.DataFrame:
    if isinstance(data, pd.DataFrame):
        return data.copy()
    elif isinstance(data, np.ndarray):
        # Gera nomes de features genericos
        n_features = data.shape[1] - 1  # Assume ultima coluna = target
        feature_names = [f'feature_{i}' for i in range(n_features)]
        return pd.DataFrame(data, columns=feature_names + [self.target_column])
    else:
        raise TypeError("Data must be DataFrame or array")
\end{lstlisting}

\subsubsection{Suporte a scikit-learn Bunch}

Extrai campos estruturados de objetos Bunch:

\begin{lstlisting}[language=Python, basicstyle=\ttfamily\scriptsize]
def _extract_from_bunch(self, bunch: Bunch) -> pd.DataFrame:
    # Extrai dados e feature names
    X = bunch.data
    y = bunch.target
    feature_names = getattr(bunch, 'feature_names', None)

    if feature_names is None:
        feature_names = [f'feature_{i}' for i in range(X.shape[1])]

    # Cria DataFrame
    df = pd.DataFrame(X, columns=feature_names)
    df[self.target_column] = y

    return df
\end{lstlisting}

\subsection{Train/Test Splitting}

\subsubsection{Auto-splitting com Stratificacao}

DBDataset utiliza \texttt{train\_test\_split} do scikit-learn com suporte a stratificacao:

\begin{lstlisting}[language=Python, basicstyle=\ttfamily\scriptsize]
def _split_data(self, data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
    stratify_col = data[self.target_column] if self.stratify else None

    train_data, test_data = train_test_split(
        data,
        test_size=self.test_size,
        random_state=self.random_state,
        stratify=stratify_col
    )

    return train_data.reset_index(drop=True), test_data.reset_index(drop=True)
\end{lstlisting}

\subsubsection{Validacao de Splits Pre-existentes}

Quando train/test fornecidos separadamente, DBDataset valida consistencia:

\begin{lstlisting}[language=Python, basicstyle=\ttfamily\scriptsize]
def _validate_splits(self, train_data, test_data):
    # Valida colunas identicas
    if set(train_data.columns) != set(test_data.columns):
        raise ValueError("Train and test must have same columns")

    # Valida tipos consistentes
    for col in train_data.columns:
        if train_data[col].dtype != test_data[col].dtype:
            warnings.warn(f"Column {col} has different dtypes in train/test")

    # Valida presenca do target
    if self.target_column not in train_data.columns:
        raise ValueError(f"Target column {self.target_column} not found")
\end{lstlisting}

\subsection{Interface de Acesso}

\subsubsection{Propriedades Read-Only}

DBDataset expoe dados via properties para encapsulamento:

\begin{lstlisting}[language=Python, basicstyle=\ttfamily\scriptsize]
@property
def features(self) -> List[str]:
    return self._features.copy()  # Retorna copia para imutabilidade

@property
def categorical_features(self) -> List[str]:
    if self._categorical_features is None:
        # Lazy initialization: infere apenas quando acessado
        self._categorical_features = self._feature_manager.infer_categorical_features(
            self._data, self._features, self.max_categories
        )
    return self._categorical_features.copy()

@property
def numerical_features(self) -> List[str]:
    return [f for f in self.features if f not in self.categorical_features]
\end{lstlisting}

\subsubsection{Metodos de Acesso Flexivel}

\begin{lstlisting}[language=Python, basicstyle=\ttfamily\scriptsize]
def get_feature_data(self, split: str = 'test') -> pd.DataFrame:
    if split == 'train':
        return self.train_data[self.features]
    elif split == 'test':
        return self.test_data[self.features]
    else:
        raise ValueError("split must be 'train' or 'test'")

def get_target_data(self, split: str = 'test') -> pd.Series:
    if split == 'train':
        return self.train_data[self.target_column]
    elif split == 'test':
        return self.test_data[self.target_column]
    else:
        raise ValueError("split must be 'train' or 'test'")

def get_predictions(self, split: str = 'test') -> np.ndarray:
    if split == 'train':
        return self.train_predictions
    elif split == 'test':
        return self.test_predictions
    else:
        raise ValueError("split must be 'train' or 'test'")
\end{lstlisting}

\subsection{Factory Methods}

\texttt{DBDatasetFactory} fornece convenience methods:

\begin{lstlisting}[language=Python, basicstyle=\ttfamily\scriptsize]
class DBDatasetFactory:
    @staticmethod
    def create_from_model(
        train_data: pd.DataFrame,
        test_data: pd.DataFrame,
        target_column: str,
        model: Any,
        **kwargs
    ) -> DBDataset:
        return DBDataset(
            train_data=train_data,
            test_data=test_data,
            target_column=target_column,
            model=model,
            **kwargs
        )

    @staticmethod
    def create_from_probabilities(
        train_data: pd.DataFrame,
        test_data: pd.DataFrame,
        target_column: str,
        train_predictions: np.ndarray,
        test_predictions: np.ndarray,
        prob_cols: List[str],
        **kwargs
    ) -> DBDataset:
        return DBDataset(
            train_data=train_data,
            test_data=test_data,
            target_column=target_column,
            train_predictions=train_predictions,
            test_predictions=test_predictions,
            prob_cols=prob_cols,
            **kwargs
        )

    @staticmethod
    def create_for_alternative_model(
        original_dataset: DBDataset,
        model: Any
    ) -> DBDataset:
        # Cria novo dataset com mesmo dados, modelo diferente
        return DBDataset(
            train_data=original_dataset.train_data,
            test_data=original_dataset.test_data,
            target_column=original_dataset.target_name,
            features=original_dataset.features,
            categorical_features=original_dataset.categorical_features,
            model=model
        )
\end{lstlisting}

\subsection{Otimizacoes de Performance}

\subsubsection{Lazy Initialization}

Features categoricas inferidas apenas quando acessadas:

\begin{lstlisting}[language=Python, basicstyle=\ttfamily\scriptsize]
# Inferencia nao ocorre em __init__ se categorical_features fornecido
dataset = DBDataset(
    data=df,
    target_column='y',
    categorical_features=['gender', 'race']  # Explicitamente fornecido
)
# Inferencia ocorre apenas se .numerical_features acessado
print(dataset.numerical_features)  # Aqui ocorre inferencia de nao-categoricas
\end{lstlisting}

\subsubsection{Caching de Predicoes}

Predicoes armazenadas em cache apos primeira geracao:

\begin{lstlisting}[language=Python, basicstyle=\ttfamily\scriptsize]
# Primeira vez: gera predicoes
preds1 = dataset.test_predictions  # Chama model.predict()

# Proximas vezes: retorna cache
preds2 = dataset.test_predictions  # Retorna array armazenado
\end{lstlisting}

\subsection{Integracao com Validation Suites}

\subsubsection{Exemplo: RobustnessSuite}

\begin{lstlisting}[language=Python, basicstyle=\ttfamily\scriptsize]
class RobustnessSuite:
    def __init__(self, dataset: DBDataset, **kwargs):
        self.dataset = dataset
        self.X_test = dataset.get_feature_data('test')
        self.y_test = dataset.get_target_data('test')
        self.model = dataset.model
        self.categorical_features = dataset.categorical_features

    def run(self):
        # Acessa features categoricas para perturbacao
        for feature in self.categorical_features:
            perturbed_X = self._perturb_categorical(self.X_test, feature)
            robustness_score = self._evaluate_robustness(perturbed_X)
\end{lstlisting}

\subsubsection{Exemplo: UncertaintySuite}

\begin{lstlisting}[language=Python, basicstyle=\ttfamily\scriptsize]
class UncertaintySuite:
    def __init__(self, dataset: DBDataset, **kwargs):
        self.dataset = dataset
        self.X_train = dataset.get_feature_data('train')
        self.X_test = dataset.get_feature_data('test')
        self.y_test = dataset.get_target_data('test')
        self.model = dataset.model

    def run(self):
        # Usa modelo para quantificacao de incerteza
        uncertainties = self._compute_uncertainties(self.X_test)
        coverage = self._compute_coverage(uncertainties, self.y_test)
\end{lstlisting}

\subsection{Dependencias}

DBDataset requer dependencias minimas:

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Biblioteca} & \textbf{Versao Minima} \\
\midrule
Python & 3.9+ \\
pandas & 1.3.0 \\
NumPy & 1.21.0 \\
scikit-learn & 1.0.0 \\
joblib & 1.0.0 \\
\bottomrule
\end{tabular}
\caption{Dependencias do DBDataset}
\label{tab:dependencies}
\end{table}

Dependencias opcionais para integracao completa:
\begin{itemize}
    \item XGBoost >= 1.5.0 (suporte a DMatrix)
    \item LightGBM >= 3.3.0 (suporte a Dataset)
    \item TensorFlow >= 2.8.0 (carregamento de modelos .h5)
\end{itemize}
