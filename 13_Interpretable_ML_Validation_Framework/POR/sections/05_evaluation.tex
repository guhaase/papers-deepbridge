\section{Avaliacao Empirica}

\subsection{Setup Experimental}

\subsubsection{Datasets}

Validamos framework em tres dominios regulados:

\begin{table}[h]
\centering
\caption{Datasets Utilizados}
\begin{tabular}{llll}
\toprule
\textbf{Dataset} & \textbf{Dominio} & \textbf{n} & \textbf{Features} \\
\midrule
HELOC & Lending (credito) & 10,459 & 23 \\
Adult & Hiring (emprego) & 48,842 & 14 \\
COMPAS & Recidivism & 7,214 & 12 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{HELOC (Home Equity Line of Credit)}:
\begin{itemize}
    \item Predicao de default em emprestimos
    \item Protected attributes: Age (ECOA prohibited basis)
    \item Altamente regulado (ECOA, Fair Lending Act)
\end{itemize}

\textbf{Adult (Census Income)}:
\begin{itemize}
    \item Predicao de income $>$ \$50k (proxy para hiring)
    \item Protected attributes: Gender, Race
    \item EEOC Title VII aplicavel
\end{itemize}

\textbf{COMPAS (Correctional Offender Management)}:
\begin{itemize}
    \item Predicao de recidivism
    \item Protected attributes: Race, Age, Gender
    \item High-profile litigation (ProPublica investigation)
\end{itemize}

\subsubsection{Baselines}

Comparamos contra:

\begin{enumerate}
    \item \textbf{Logistic Regression}: Baseline interpretavel
    \item \textbf{Decision Tree (vanilla)}: Treinado diretamente em hard labels
    \item \textbf{Random Forest}: Ensemble nao-interpretavel
    \item \textbf{XGBoost}: State-of-the-art gradient boosting
    \item \textbf{Multi-Teacher Ensemble}: 5 XGBoost models com diferentes seeds
\end{enumerate}

\subsubsection{Configuracoes Testadas}

Framework:
\begin{itemize}
    \item \textbf{KDDT}: $T \in \{2.0, 3.0, 5.0\}$, $\alpha \in \{0.3, 0.5, 0.7\}$, max\_depth $\in \{5, 7, 10\}$
    \item \textbf{GAM Distilled}: n\_splines $\in \{5, 10, 15\}$, lam $\in \{0.1, 0.6, 2.0\}$
    \item Optimization: Optuna 50 trials, CV 5-fold
\end{itemize}

\subsubsection{Metricas}

\textbf{Performance}:
\begin{itemize}
    \item AUC-ROC, AUC-PR, Accuracy, F1-Score
    \item KS Statistic (separacao de distribuicoes)
    \item Fidelity: KL Divergence vs. teacher
\end{itemize}

\textbf{Compliance}:
\begin{itemize}
    \item Fairness: 15 metricas (foco em disparate impact)
    \item Robustness: Performance degradation ($\epsilon=0.1$ a $1.0$)
    \item Uncertainty: Coverage, interval width
\end{itemize}

\textbf{Interpretabilidade}:
\begin{itemize}
    \item Decision Trees: depth, n\_leaves
    \item GAMs: n\_splines, curvature
\end{itemize}

\subsection{Resultados: Performance vs. Interpretabilidade}

\subsubsection{HELOC (Lending)}

\begin{table}[h]
\centering
\caption{Resultados em HELOC Dataset}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{AUC} & \textbf{KS} & \textbf{Depth} & \textbf{Compliance} \\
\midrule
Logistic Reg & 0.721 & 0.38 & -- & 85\% \\
DT (vanilla, d=5) & 0.735 & 0.42 & 5 & 87\% \\
DT (vanilla, d=10) & 0.758 & 0.47 & 10 & 82\% \\
Random Forest & 0.782 & 0.53 & -- & 71\% \\
XGBoost & 0.801 & 0.58 & -- & 68\% \\
Multi-Teacher & 0.809 & 0.60 & -- & 64\% \\
\midrule
\textbf{KDDT (T=3, d=7)} & \textbf{0.784} & \textbf{0.55} & \textbf{7} & \textbf{93\%} \\
\textbf{GAM Distilled} & \textbf{0.772} & \textbf{0.52} & \textbf{--} & \textbf{91\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Observacoes}:
\begin{itemize}
    \item KDDT: 97\% da AUC do Multi-Teacher (0.784 vs. 0.809)
    \item Trade-off: -3.1\% AUC por +29\% compliance score
    \item KDDT passa 100\% de auditorias ECOA (vs. 67\% do XGBoost)
\end{itemize}

\subsubsection{Adult (Hiring)}

\begin{table}[h]
\centering
\caption{Resultados em Adult Dataset}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{AUC} & \textbf{F1} & \textbf{Disparate Impact} & \textbf{Compliance} \\
\midrule
Logistic Reg & 0.743 & 0.64 & 0.86 (\cmark) & 82\% \\
DT (vanilla, d=5) & 0.761 & 0.67 & 0.79 (\xmark) & 78\% \\
Random Forest & 0.802 & 0.73 & 0.72 (\xmark) & 69\% \\
XGBoost & 0.824 & 0.76 & 0.68 (\xmark) & 65\% \\
\midrule
\textbf{KDDT (T=2, d=5)} & \textbf{0.797} & \textbf{0.71} & \textbf{0.82 (\cmark)} & \textbf{89\%} \\
\textbf{GAM Distilled} & \textbf{0.785} & \textbf{0.69} & \textbf{0.84 (\cmark)} & \textbf{91\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Observacoes}:
\begin{itemize}
    \item XGBoost viola EEOC 80\% rule (disparate impact = 0.68)
    \item KDDT mantÃ©m compliance (0.82) com perda de apenas 3.3\% AUC
    \item GAM oferece melhor explicabilidade (efeitos aditivos por feature)
\end{itemize}

\subsubsection{COMPAS (Recidivism)}

\begin{table}[h]
\centering
\caption{Resultados em COMPAS Dataset}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{AUC} & \textbf{Eq. Opportunity} & \textbf{Eq. Odds} & \textbf{Compliance} \\
\midrule
Logistic Reg & 0.688 & 0.12 & 0.18 & 79\% \\
XGBoost & 0.731 & 0.19 & 0.24 & 62\% \\
\midrule
\textbf{KDDT (T=5, d=6)} & \textbf{0.714} & \textbf{0.08} & \textbf{0.11} & \textbf{87\%} \\
\textbf{GAM Distilled} & \textbf{0.702} & \textbf{0.06} & \textbf{0.09} & \textbf{90\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Observacoes}:
\begin{itemize}
    \item GAM atinge melhor equalized opportunity (0.06 vs. 0.19 do XGBoost)
    \item Trade-off: -4.0\% AUC por 68\% reducao em disparidade de oportunidade
\end{itemize}

\subsection{Analise de Trade-offs}

\subsubsection{Pareto Frontier}

Agregando resultados dos 3 datasets:

\begin{figure}[h]
\centering
\begin{verbatim}
    AUC
     |
0.85 |                    * XGBoost
     |                  *   Multi-Teacher
0.80 |              *
     |           * KDDT (d=7)
0.75 |        * KDDT (d=5)
     |      * GAM Distilled
0.70 |   * Logistic Reg
     |
     +-----|-----|-----|-----|-----|-----> Interpretability
           Low         Med        High
\end{verbatim}
\caption{Pareto Frontier Performance vs. Interpretabilidade}
\end{figure}

\textbf{Sweet Spots Identificados}:
\begin{itemize}
    \item \textbf{KDDT (d=5-7)}: 95-97\% da performance, interpretabilidade alta
    \item \textbf{GAM (10-15 splines)}: 93-95\% da performance, interpretabilidade media-alta
    \item Custo medio de compliance: 3-5\% de AUC
\end{itemize}

\subsection{Resultados de Validacao}

\subsubsection{Fairness Audit}

Compliance score medio por categoria de modelo:

\begin{table}[h]
\centering
\caption{Fairness Compliance Scores}
\begin{tabular}{lcccc}
\toprule
\textbf{Model Type} & \textbf{Disp. Impact} & \textbf{Eq. Opp.} & \textbf{Eq. Odds} & \textbf{Overall} \\
\midrule
Logistic Reg & 88\% & 82\% & 79\% & 83\% \\
XGBoost & 65\% & 71\% & 68\% & 68\% \\
Random Forest & 70\% & 74\% & 71\% & 72\% \\
\midrule
\textbf{KDDT} & \textbf{94\%} & \textbf{91\%} & \textbf{89\%} & \textbf{91\%} \\
\textbf{GAM Distilled} & \textbf{96\%} & \textbf{93\%} & \textbf{91\%} & \textbf{93\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Violacoes Detectadas}:
\begin{itemize}
    \item XGBoost: 5/15 metricas violadas (critical risk)
    \item Random Forest: 4/15 metricas violadas
    \item KDDT: 0/15 metricas violadas
    \item GAM: 0/15 metricas violadas
\end{itemize}

\subsubsection{Robustness Analysis}

Performance degradation sob perturbacoes:

\begin{table}[h]
\centering
\caption{Degradacao de AUC ($\epsilon=0.4$)}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Baseline AUC} & \textbf{Perturbed AUC} & \textbf{$\Delta$ AUC} & \textbf{Flip Rate} \\
\midrule
XGBoost & 0.801 & 0.762 & -0.039 & 12.3\% \\
Random Forest & 0.782 & 0.751 & -0.031 & 10.8\% \\
\midrule
\textbf{KDDT} & \textbf{0.784} & \textbf{0.769} & \textbf{-0.015} & \textbf{6.2\%} \\
\textbf{GAM Distilled} & \textbf{0.772} & \textbf{0.761} & \textbf{-0.011} & \textbf{4.9\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Observacoes}:
\begin{itemize}
    \item Modelos interpretaveis sao mais robustos (menor degradacao)
    \item GAM: 72\% menos flip rate que XGBoost
    \item Implicacao: Menor risco de drift em producao
\end{itemize}

\subsubsection{Uncertainty Quantification}

Conformal Prediction results ($\alpha=0.1$ para 90\% coverage):

\begin{table}[h]
\centering
\caption{Uncertainty Quantification Results}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Coverage} & \textbf{Interval Width} & \textbf{Conditional Coverage} \\
\midrule
XGBoost & 89.2\% & 0.34 & 0.12 disparity \\
\midrule
\textbf{KDDT} & \textbf{90.8\%} & \textbf{0.38} & \textbf{0.06 disparity} \\
\textbf{GAM Distilled} & \textbf{91.1\%} & \textbf{0.36} & \textbf{0.04 disparity} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Observacoes}:
\begin{itemize}
    \item GAM: Melhor conditional coverage (menor disparidade entre grupos)
    \item Intervalos ligeiramente maiores mas mais calibrados
    \item Benefit: Decisoes high-uncertainty podem requerer human review
\end{itemize}

\subsection{Case Study: Lending AI Deployment}

\textbf{Cenario}: Banco implementando modelo de aprovacao de credito

\textbf{Requisitos Regulatorios}:
\begin{itemize}
    \item ECOA compliance (reason codes para adverse actions)
    \item Disparate impact $\geq$ 0.80
    \item Auditabilidade para reguladores
\end{itemize}

\textbf{Abordagem Tradicional}:
\begin{itemize}
    \item XGBoost ensemble (AUC=0.809)
    \item SHAP values para explicacoes
    \item Compliance score: 68\%
    \item \textbf{Problema}: Regulador questiona: ``Como sei que SHAP nao muda?''
\end{itemize}

\textbf{Nossa Solucao}:
\begin{itemize}
    \item KDDT (T=3.0, depth=7, AUC=0.784)
    \item Decision path para cada adverse action
    \item Compliance score: 93\%
    \item \textbf{Resultado}: Aprovado em auditoria CFPB
\end{itemize}

\textbf{Trade-off Quantificado}:
\begin{itemize}
    \item Custo: -3.1\% AUC
    \item Beneficio: +25\% compliance score
    \item ROI: Multas evitadas $>>$ perda de receita por rejeicoes adicionais
\end{itemize}

\subsection{Ablation Studies}

\subsubsection{Impacto da Temperatura}

\begin{table}[h]
\centering
\caption{KDDT: Variacao de Temperatura (HELOC)}
\begin{tabular}{lccc}
\toprule
\textbf{Temperature} & \textbf{AUC} & \textbf{KL Divergence} & \textbf{Fidelity} \\
\midrule
1.0 (hard labels) & 0.758 & 0.42 & 0.58 \\
2.0 & 0.771 & 0.31 & 0.69 \\
3.0 & 0.784 & 0.19 & 0.81 \\
5.0 & 0.781 & 0.22 & 0.78 \\
10.0 & 0.768 & 0.29 & 0.71 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Observacao}: Sweet spot em $T=3.0$ (maxima fidelity).

\subsubsection{Impacto de n\_splines (GAM)}

\begin{table}[h]
\centering
\caption{GAM: Variacao de n\_splines (Adult)}
\begin{tabular}{lccc}
\toprule
\textbf{n\_splines} & \textbf{AUC} & \textbf{Interpretability} & \textbf{Compliance} \\
\midrule
5 & 0.762 & Alta & 89\% \\
10 & 0.785 & Media-Alta & 91\% \\
15 & 0.791 & Media & 88\% \\
25 & 0.794 & Baixa & 82\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Observacao}: 10-15 splines = sweet spot (performance vs. interpretabilidade).
