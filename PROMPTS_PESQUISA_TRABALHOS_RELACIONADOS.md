# Prompts para Pesquisa de Trabalhos Relacionados
# HPM-KD e DiXtill Frameworks

**Data de Cria√ß√£o**: 2025-12-07
**Objetivo**: Verificar se j√° existem trabalhos similares publicados antes de submeter os papers
**Frameworks**: HPM-KD (Paper 1) e DiXtill (Paper 16)

---

## üìã Instru√ß√µes de Uso

1. Use cada prompt em ferramentas de IA como ChatGPT, Claude, Perplexity, ou Gemini
2. Tamb√©m pesquise em bases acad√™micas: Google Scholar, arXiv, Semantic Scholar, ACM Digital Library, IEEE Xplore
3. Anote os resultados encontrados em cada se√ß√£o
4. Marque com ‚úÖ ou ‚ùå se encontrou trabalhos muito similares
5. Se encontrar trabalhos similares, anote diferen√ßas e contribui√ß√µes √∫nicas dos nossos frameworks

---

## üîç PARTE 1: Pesquisa sobre HPM-KD Framework

### 1.1 Pesquisa Geral do Framework Completo

```
Existe algum framework de knowledge distillation que combine TODOS os seguintes componentes:
1. Sele√ß√£o autom√°tica de configura√ß√£o via meta-aprendizado (adaptive configuration manager)
2. Destila√ß√£o progressiva hier√°rquica com m√∫ltiplos teachers em cadeia
3. Multi-teacher ensemble com pesos de aten√ß√£o aprendidos (attention-weighted)
4. Agendamento adaptativo de temperatura (meta-temperature scheduler)
5. Pipeline de processamento paralelo com cache inteligente
6. Mem√≥ria compartilhada de otimiza√ß√£o entre experimentos

Procuro especificamente trabalhos que integrem pelo menos 4-5 desses componentes simultaneamente, n√£o apenas um ou dois isoladamente.
```

**Resultados**:
- [ ] Nenhum trabalho encontrado
- [ ] Trabalhos parcialmente similares:
  -
  -
  -

---

### 1.2 Adaptive Configuration Manager

```
Existem trabalhos sobre knowledge distillation que usam meta-aprendizado para selecionar automaticamente:
- Tipo de modelo student
- Temperatura de distillation
- Valores de alpha (peso entre cross-entropy e KD loss)
- Arquitetura do student

Especificamente procuro por "adaptive configuration", "automated hyperparameter selection for distillation", "meta-learning for knowledge distillation configuration", ou "AutoML for knowledge distillation".
```

**Resultados**:
- [ ] Nenhum trabalho encontrado
- [ ] Trabalhos relacionados:
  -
  -

---

### 1.3 Progressive Multi-Teacher Distillation

```
Existem frameworks de knowledge distillation que implementam:
1. Cadeia progressiva de destila√ß√£o (progressive chain) onde:
   - Teacher 1 ‚Üí Student 1 (intermediate)
   - Student 1 ‚Üí Student 2 (smaller)
   - Student 2 ‚Üí Student 3 (final compact model)
2. Com rastreamento de melhoria m√≠nima (minimal improvement tracking)
3. Refinamento incremental hier√°rquico

Termos de busca: "progressive knowledge distillation", "hierarchical multi-teacher distillation", "cascaded distillation", "incremental knowledge transfer", "multi-step distillation chain".
```

**Resultados**:
- [ ] Nenhum trabalho encontrado
- [ ] Trabalhos relacionados:
  -
  -

---

### 1.4 Attention-Weighted Multi-Teacher Ensemble

```
Existem trabalhos sobre multi-teacher knowledge distillation que:
1. Usam m√∫ltiplos teachers simultaneamente (ensemble)
2. Aprendem pesos de aten√ß√£o para cada teacher (n√£o pesos fixos/uniformes)
3. Esses pesos de aten√ß√£o s√£o aprendidos durante o treinamento (learned attention)
4. Combinam soft targets ponderados por import√¢ncia de cada teacher

Procuro por "attention-weighted multi-teacher", "learned teacher weighting", "adaptive teacher ensemble", "dynamic teacher selection in KD".
```

**Resultados**:
- [ ] Nenhum trabalho encontrado
- [ ] Trabalhos relacionados:
  -
  -

---

### 1.5 Meta-Temperature Scheduler

```
Existem trabalhos que prop√µem agendamento adaptativo de temperatura (temperature scheduling) em knowledge distillation onde:
1. A temperatura n√£o √© fixa durante todo o treinamento
2. A temperatura √© ajustada dinamicamente baseada em:
   - Performance no validation set
   - Diverg√™ncia entre teacher e student
   - Fase do treinamento (early/mid/late)
3. Usa meta-aprendizado para determinar o schedule √≥timo

Termos: "adaptive temperature scheduling", "dynamic temperature in knowledge distillation", "meta-learning temperature", "temperature annealing in KD".
```

**Resultados**:
- [ ] Nenhum trabalho encontrado
- [ ] Trabalhos relacionados:
  -
  -

---

### 1.6 Parallel Processing e Shared Optimization Memory

```
Existem frameworks de knowledge distillation que implementam:
1. Processamento paralelo de m√∫ltiplas configura√ß√µes de distillation
2. Cache inteligente de:
   - Soft targets do teacher (para evitar recomputa√ß√£o)
   - Embeddings intermedi√°rios
   - Predictions
3. Mem√≥ria compartilhada de otimiza√ß√£o entre experimentos:
   - Reutiliza aprendizado de experimentos anteriores
   - Warm-start de configura√ß√µes similares

Procuro por "parallel knowledge distillation", "cached distillation", "shared memory optimization", "transfer learning across distillation experiments".
```

**Resultados**:
- [ ] Nenhum trabalho encontrado
- [ ] Trabalhos relacionados:
  -
  -

---

### 1.7 Nome "HPM-KD" ou Similar

```
Existe algum framework chamado "HPM-KD", "Hierarchical Progressive Multi-Teacher", ou siglas muito similares como:
- HPKD (Hierarchical Progressive Knowledge Distillation)
- MPKD (Multi-Progressive Knowledge Distillation)
- HTPD (Hierarchical Teacher-Progressive Distillation)

Ou trabalhos que usem exatamente a combina√ß√£o "Hierarchical + Progressive + Multi-Teacher" no t√≠tulo?
```

**Resultados**:
- [ ] Nenhum trabalho encontrado
- [ ] Trabalhos com nomes similares:
  -
  -

---

## üîç PARTE 2: Pesquisa sobre DiXtill Framework

### 2.1 Pesquisa Geral do Framework DiXtill

```
Existe algum framework de knowledge distillation que adiciona um termo de alinhamento de explicabilidade (explanation alignment) na fun√ß√£o de perda, especificamente:

L = (1-Œ±)L_CE + Œ±(L_KD + Œ≤¬∑L_XAI)

Onde L_XAI alinha explica√ß√µes (SHAP values, attention weights, ou gradientes) entre teacher e student DURANTE o treinamento (n√£o post-hoc)?

Procuro por trabalhos que transfiram n√£o apenas predictions, mas o PROCESSO DE RACIOC√çNIO (reasoning) do teacher para o student.
```

**Resultados**:
- [ ] Nenhum trabalho encontrado
- [ ] Trabalhos relacionados:
  -
  -

---

### 2.2 SHAP Alignment em Knowledge Distillation

```
Existem trabalhos que usam SHAP values (Shapley Additive Explanations) em knowledge distillation para:
1. Calcular SHAP values do teacher e student
2. Minimizar a dist√¢ncia entre esses SHAP values: ||SHAP_teacher - SHAP_student||¬≤
3. Garantir que feature importances sejam preservadas ap√≥s distillation

Termos: "SHAP alignment", "Shapley values in distillation", "feature attribution transfer", "explanation-aware distillation".
```

**Resultados**:
- [ ] Nenhum trabalho encontrado
- [ ] Trabalhos relacionados:
  -
  -

---

### 2.3 Attention Alignment em Transformers

```
Existem trabalhos sobre knowledge distillation de transformers que:
1. Alinham attention weights entre teacher e student: ||A_teacher - A_student||¬≤
2. Fazem esse alinhamento layer-by-layer
3. Tratam casos onde teacher e student t√™m diferentes n√∫meros de layers/heads
4. Usam estrat√©gias de mapeamento (uniform, last-N, skip)

Procuro por "attention transfer", "attention distillation", "attention alignment", especificamente para BERT/transformers.
```

**Resultados**:
- [ ] Nenhum trabalho encontrado
- [ ] Trabalhos relacionados:
  -
  -

---

### 2.4 Gradient Alignment / Saliency Map Alignment

```
Existem frameworks de distillation que alinham gradientes de entrada (input gradients) ou saliency maps entre teacher e student:

L_XAI = ||‚àá_x log p_teacher - ‚àá_x log p_student||¬≤

Onde os gradientes indicam quais pixels/features s√£o importantes para a decis√£o?

Termos: "gradient matching", "saliency alignment", "input gradient distillation", "gradient-based knowledge transfer".
```

**Resultados**:
- [ ] Nenhum trabalho encontrado
- [ ] Trabalhos relacionados:
  -
  -

---

### 2.5 XAI-Driven ou Explainability-Driven Distillation

```
Existem trabalhos sobre "explainability-driven distillation", "XAI-guided knowledge distillation", ou "interpretable knowledge distillation" que:
1. Usam t√©cnicas de XAI (SHAP, LIME, CAM, attention, gradients) DURANTE o treinamento
2. Garantem que student seja interpret√°vel-by-design (n√£o apenas post-hoc)
3. Preservam consist√™ncia de explica√ß√µes entre teacher e student

Procuro por trabalhos que combinem XAI + KD de forma integrada.
```

**Resultados**:
- [ ] Nenhum trabalho encontrado
- [ ] Trabalhos relacionados:
  -
  -

---

### 2.6 Reasoning Transfer (Transfer√™ncia de Racioc√≠nio)

```
Existem trabalhos sobre knowledge distillation que focam em transferir o RACIOC√çNIO (reasoning process) do teacher, n√£o apenas as predictions?

Especificamente trabalhos que argumentam:
- KD tradicional transfere "o que prever"
- Nosso m√©todo transfere "por que prever"
- Student aprende o processo de decis√£o, n√£o s√≥ o resultado final

Termos: "reasoning transfer", "decision process transfer", "rationale distillation", "interpretable student models".
```

**Resultados**:
- [ ] Nenhum trabalho encontrado
- [ ] Trabalhos relacionados:
  -
  -

---

### 2.7 Nome "DiXtill" ou Similar

```
Existe algum framework chamado "DiXtill", "XAI-Driven Distillation", ou nomes muito similares como:
- XAI-KD (XAI Knowledge Distillation)
- Explainable KD
- Interpretable Distillation Framework
- SHAP-Distill

Ou trabalhos que combinem explicitamente "XAI" + "Distillation" no t√≠tulo?
```

**Resultados**:
- [ ] Nenhum trabalho encontrado
- [ ] Trabalhos com nomes similares:
  -
  -

---

### 2.8 DiXtill Espec√≠fico para Ambientes Regulados

```
Existem trabalhos sobre knowledge distillation focados em ambientes regulados (finan√ßas, sa√∫de, contrata√ß√£o) onde:
1. Explicabilidade √© mandat√≥ria por regula√ß√£o (GDPR, ECOA, EEOC)
2. Student precisa ser interpret√°vel para compliance
3. Explica√ß√µes do student s√£o consistentes com o teacher (auditabilidade)

Termos: "regulatory-compliant distillation", "explainable distillation for finance", "interpretable student models for healthcare", "GDPR-compliant model compression".
```

**Resultados**:
- [ ] Nenhum trabalho encontrado
- [ ] Trabalhos relacionados:
  -
  -

---

## üîç PARTE 3: Trabalhos Relacionados Gerais

### 3.1 Survey Papers sobre Knowledge Distillation (√∫ltimos 3 anos)

```
Quais s√£o os survey papers mais recentes (2022-2025) sobre knowledge distillation?
Procuro por:
- "A Survey on Knowledge Distillation"
- "Recent Advances in Knowledge Distillation"
- Reviews abrangentes que cubram multi-teacher, progressive, attention-based methods

Liste os 5-10 surveys mais citados e recentes.
```

**Resultados**:
-
-
-

---

### 3.2 State-of-the-Art em Knowledge Distillation (2024-2025)

```
Quais s√£o os m√©todos estado-da-arte em knowledge distillation publicados em 2024-2025 nas confer√™ncias:
- NeurIPS 2024
- ICML 2024
- ICLR 2024/2025
- AAAI 2024/2025

Liste os papers aceitos nessas confer√™ncias relacionados a KD.
```

**Resultados**:
-
-
-

---

### 3.3 Multi-Teacher Distillation (Estado da Arte)

```
Quais s√£o os trabalhos mais citados sobre multi-teacher knowledge distillation?
Especificamente procuro por:
- Deep Mutual Learning (Zhang et al., 2018)
- TAKD - Teacher Assistant Knowledge Distillation (Mirzadeh et al., 2020)
- Online Knowledge Distillation
- Collaborative Learning

Liste os 5-10 papers fundamentais nessa √°rea.
```

**Resultados**:
-
-
-

---

### 3.4 Attention Transfer e Feature-Based Distillation

```
Quais s√£o os trabalhos cl√°ssicos sobre:
1. Attention Transfer (Zagoruyko & Komodakis, 2017)
2. FitNets (Romero et al., 2015)
3. Feature-based Knowledge Distillation
4. Intermediate layer matching

Liste os papers fundamentais que fazem matching de representa√ß√µes internas (n√£o apenas soft targets).
```

**Resultados**:
-
-
-

---

### 3.5 XAI Methods (SHAP, LIME, Integrated Gradients)

```
Quais s√£o os papers fundamentais sobre t√©cnicas de XAI que usamos:
1. SHAP (Lundberg & Lee, 2017)
2. LIME (Ribeiro et al., 2016)
3. Integrated Gradients (Sundararajan et al., 2017)
4. Attention Mechanisms for Interpretability
5. Saliency Maps / CAM (Class Activation Mapping)

Liste as refer√™ncias principais que devemos citar.
```

**Resultados**:
-
-
-

---

## üîç PARTE 4: An√°lise de Novidade e Diferencia√ß√£o

### 4.1 An√°lise Comparativa HPM-KD

```
Dado que HPM-KD combina:
- Adaptive configuration (meta-learning)
- Progressive multi-teacher chain
- Attention-weighted ensemble
- Meta-temperature scheduling
- Parallel processing + shared memory

Qual trabalho existente √© o MAIS SIMILAR ao HPM-KD?
Liste diferen√ßas espec√≠ficas e contribui√ß√µes √∫nicas do HPM-KD.
```

**An√°lise**:
- Trabalho mais similar:
- Diferen√ßas principais:
  1.
  2.
  3.
- Contribui√ß√µes √∫nicas do HPM-KD:
  1.
  2.
  3.

---

### 4.2 An√°lise Comparativa DiXtill

```
Dado que DiXtill:
- Adiciona L_XAI (explanation alignment) na loss function
- Transfere reasoning, n√£o apenas predictions
- Suporta SHAP, Attention, Gradient alignment
- Foca em interpretabilidade-by-design (n√£o post-hoc)

Qual trabalho existente √© o MAIS SIMILAR ao DiXtill?
Liste diferen√ßas espec√≠ficas e contribui√ß√µes √∫nicas do DiXtill.
```

**An√°lise**:
- Trabalho mais similar:
- Diferen√ßas principais:
  1.
  2.
  3.
- Contribui√ß√µes √∫nicas do DiXtill:
  1.
  2.
  3.

---

## üìö PARTE 5: Bases de Dados para Pesquisa

### Ferramentas de IA Recomendadas

1. **Perplexity AI** (https://www.perplexity.ai/)
   - Melhor para pesquisas acad√™micas com cita√ß√µes
   - Use modo "Academic"

2. **ChatGPT** (https://chat.openai.com/)
   - Use GPT-4 com web browsing ativado
   - Pe√ßa para citar fontes espec√≠ficas

3. **Claude** (https://claude.ai/)
   - Bom para an√°lise comparativa detalhada
   - Pe√ßa para comparar trabalhos

4. **Gemini** (https://gemini.google.com/)
   - Integrado com Google Scholar
   - Bom para encontrar papers recentes

5. **Consensus** (https://consensus.app/)
   - Ferramenta especializada em pesquisa acad√™mica
   - Busca direta em papers cient√≠ficos

---

### Bases Acad√™micas para Verifica√ß√£o Manual

1. **Google Scholar** (https://scholar.google.com/)
   - Pesquisa mais abrangente
   - Mostra cita√ß√µes

2. **arXiv** (https://arxiv.org/)
   - Papers em Machine Learning (cs.LG)
   - Pr√©-prints antes de confer√™ncias

3. **Semantic Scholar** (https://www.semanticscholar.org/)
   - IA para encontrar papers relacionados
   - Gr√°fico de cita√ß√µes

4. **ACM Digital Library** (https://dl.acm.org/)
   - Papers de confer√™ncias ACM (FAccT, KDD)

5. **IEEE Xplore** (https://ieeexplore.ieee.org/)
   - Papers de confer√™ncias IEEE

6. **Papers With Code** (https://paperswithcode.com/)
   - Papers com c√≥digo dispon√≠vel
   - Benchmarks e leaderboards

---

## ‚úÖ Checklist de Valida√ß√£o de Novidade

Antes de submeter os papers, verifique:

### Para HPM-KD:
- [ ] Nenhum trabalho combina TODOS os 6 componentes do HPM-KD
- [ ] Adaptive configuration via meta-learning √© original ou tem diferen√ßas claras
- [ ] Progressive multi-teacher chain tem contribui√ß√£o √∫nica
- [ ] Attention-weighted ensemble √© diferente de m√©todos existentes
- [ ] Shared optimization memory n√£o existe em outros frameworks
- [ ] Nome "HPM-KD" n√£o est√° em uso

### Para DiXtill:
- [ ] L_XAI (explanation alignment loss) √© original
- [ ] SHAP alignment em KD n√£o existe ou √© muito diferente
- [ ] Transfer√™ncia de reasoning (n√£o apenas predictions) √© contribui√ß√£o clara
- [ ] Foco em ambientes regulados √© √∫nico
- [ ] Nome "DiXtill" n√£o est√° em uso
- [ ] Combina√ß√£o XAI + KD durante treinamento (n√£o post-hoc) √© original

---

## üìù Notas Finais

- **Data da √∫ltima pesquisa**: _______
- **Pesquisador respons√°vel**: _______
- **Decis√£o**:
  - [ ] Prosseguir com submiss√£o (novidade confirmada)
  - [ ] Revisar papers para diferencia√ß√£o clara
  - [ ] Adiar submiss√£o (trabalho muito similar encontrado)

**Coment√°rios adicionais**:

---

## üîó Refer√™ncias para Citar (em caso de trabalhos relacionados)

### Knowledge Distillation Cl√°ssico:
1. Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.

### Multi-Teacher:
2. You, S., Xu, C., Xu, C., & Tao, D. (2017). Learning from multiple teacher networks. KDD.
3. Zhang, Y., Xiang, T., Hospedales, T. M., & Lu, H. (2018). Deep mutual learning. CVPR.

### Attention Transfer:
4. Zagoruyko, S., & Komodakis, N. (2017). Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. ICLR.

### Feature-Based:
5. Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta, C., & Bengio, Y. (2015). FitNets: Hints for thin deep nets. ICLR.

### XAI (Explainability):
6. Lundberg, S. M., & Lee, S. I. (2017). A unified approach to interpreting model predictions. NeurIPS.
7. Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). "Why should I trust you?" Explaining the predictions of any classifier. KDD.
8. Sundararajan, M., Taly, A., & Yan, Q. (2017). Axiomatic attribution for deep networks. ICML.

---

**Vers√£o**: 1.0
**√öltima atualiza√ß√£o**: 2025-12-07
