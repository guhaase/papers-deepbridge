# Paper 16: DiXtill - DestilaÃ§Ã£o de Conhecimento Guiada por XAI

## ğŸ“‹ InformaÃ§Ãµes BÃ¡sicas

**TÃ­tulo**: DiXtill: DestilaÃ§Ã£o de Conhecimento Guiada por XAI - Transferindo RaciocÃ­nio, NÃ£o Apenas PrediÃ§Ãµes

**TÃ­tulo em InglÃªs**: DiXtill: XAI-Driven Knowledge Distillation - Transferring Not Just Predictions, But Reasoning

**ConferÃªncias Alvo**:
- AAAI (Association for the Advancement of Artificial Intelligence)
- IJCAI (International Joint Conference on Artificial Intelligence)
- FAccT (ACM Conference on Fairness, Accountability, and Transparency)

**Status**: Draft completo - **ATENÃ‡ÃƒO: 14 pÃ¡ginas (excede limite de 10 pÃ¡ginas)**

---

## ğŸ”¬ ContribuiÃ§Ã£o CientÃ­fica

### Problema
Knowledge distillation tradicional transfere **predictions** (soft targets) de um teacher complexo para um student compacto, mas nÃ£o preserva o **processo de raciocÃ­nio** subjacente. TÃ©cnicas de XAI post-hoc explicam como o student funciona, mas nÃ£o garantem consistÃªncia com o teacher.

### SoluÃ§Ã£o: DiXtill Framework

DiXtill adiciona termo de **alinhamento de explicaÃ§Ãµes** durante treinamento:

```
L = (1-Î±)L_CE + Î±(L_KD + Î²Â·L_XAI)
```

Onde:
- **L_CE**: Cross-entropy (standard supervised learning)
- **L_KD**: Knowledge distillation loss (soft targets)
- **L_XAI**: Explanation alignment loss (NOVO)
- **Î±**: Peso de distillation (tipicamente 0.5)
- **Î²**: Peso de XAI alignment (tipicamente 0.3)

### TrÃªs Mecanismos de Alinhamento

1. **SHAP Alignment**: `||SHAP_teacher - SHAP_student||Â²`
   - Transfere feature attributions
   - Ideal para dados tabulares
   - Teoricamente fundamentado (Shapley values)

2. **Attention Alignment**: `||Attention_teacher - Attention_student||_FÂ²`
   - Para modelos transformers
   - Baixo custo computacional (+40% vs. KD tradicional)
   - Interpretabilidade nativa

3. **Gradient Alignment**: `||âˆ‡_x teacher - âˆ‡_x student||Â²`
   - Saliency maps para visÃ£o computacional
   - Custo moderado (+70%)
   - EscalÃ¡vel para alta dimensionalidade

---

## ğŸ“Š Resultados Principais

### Experimento 1: NLP Financeiro
- **Teacher**: FinBERT (110M parÃ¢metros)
- **Student**: Bi-LSTM (862K parÃ¢metros)
- **CompressÃ£o**: 127Ã—
- **AcurÃ¡cia**: 84.3% (student) vs. 85.5% (teacher) â†’ gap de apenas 1.2%
- **SHAP Correlation**: Ï = 0.92 (vs. 0.58 para KD tradicional)
- **LatÃªncia**: 11.4Ã— menor (3.7ms vs. 42.3ms)

### Experimento 2: VisÃ£o Computacional
- **Teacher**: ResNet-50 (25.6M parÃ¢metros)
- **Student**: MobileNetV2 (3.5M parÃ¢metros)
- **CompressÃ£o**: 7.3Ã—
- **AcurÃ¡cia**: 93.1% vs. 94.2% â†’ gap de 1.1%
- **Spatial Correlation**: 0.81 (saliency maps)
- **IoU (top-20%)**: 0.73

### Experimento 3: Dados Tabulares
- **Teacher**: XGBoost (500 Ã¡rvores)
- **Student**: Logistic Regression
- **CompressÃ£o**: 15,333Ã— (18.4MB â†’ 1.2KB)
- **AcurÃ¡cia**: 86.2% vs. 87.3% â†’ gap de 1.1%
- **SHAP Correlation**: Ï = 0.94 (quase perfeita)
- **Top-3 Feature Overlap**: 93%

---

## ğŸ“ Estrutura do Paper

```
POR/
â”œâ”€â”€ main.tex                      # Arquivo principal LaTeX
â”œâ”€â”€ main.pdf                      # PDF compilado (14 pÃ¡ginas)
â”œâ”€â”€ acmart.cls                    # Template ACM
â”œâ”€â”€ compile.sh                    # Script de compilaÃ§Ã£o
â”œâ”€â”€ sections/
â”‚   â”œâ”€â”€ 01_introduction.tex       # IntroduÃ§Ã£o
â”‚   â”œâ”€â”€ 02_background.tex         # Trabalhos Relacionados
â”‚   â”œâ”€â”€ 03_design.tex             # Design do Framework DiXtill
â”‚   â”œâ”€â”€ 04_implementation.tex     # ImplementaÃ§Ã£o no DeepBridge
â”‚   â”œâ”€â”€ 05_evaluation.tex         # Experimentos e Resultados
â”‚   â”œâ”€â”€ 06_discussion.tex         # DiscussÃ£o e LimitaÃ§Ãµes
â”‚   â””â”€â”€ 07_conclusion.tex         # ConclusÃ£o e Trabalho Futuro
â””â”€â”€ bibliography/
    â””â”€â”€ references.bib            # ReferÃªncias bibliogrÃ¡ficas (30+ refs)
```

---

## ğŸ”§ Como Compilar

### Requisitos
- LaTeX completo (TeX Live 2023 ou superior)
- pdflatex
- bibtex
- Pacotes: babel, inputenc, fontenc, graphicx, booktabs, amsmath, listings, algorithm, algpseudocode

### CompilaÃ§Ã£o

```bash
cd /home/guhaase/projetos/DeepBridge/papers/16_XAI_Driven_Distillation/POR
./compile.sh
```

O script executa:
1. `pdflatex main.tex` (primeira compilaÃ§Ã£o)
2. `bibtex main` (processar referÃªncias)
3. `pdflatex main.tex` (segunda compilaÃ§Ã£o)
4. `pdflatex main.tex` (terceira compilaÃ§Ã£o para cross-references)

### Output
- **PDF gerado**: `main.pdf`
- **NÃºmero de pÃ¡ginas atual**: 14 (excede limite de 10 pÃ¡ginas)

---

## âš ï¸ Status Atual

### âœ… Completo
- [x] Estrutura completa do paper em LaTeX
- [x] Todas as 7 seÃ§Ãµes escritas
- [x] Bibliografia com 30+ referÃªncias
- [x] Tabelas e algoritmos formatados
- [x] Exemplos de cÃ³digo (Python)
- [x] FÃ³rmulas matemÃ¡ticas detalhadas
- [x] PDF compilando sem erros

### âš ï¸ Pendente
- [ ] **REDUZIR de 14 para 10 pÃ¡ginas** (4 pÃ¡ginas a remover)
- [ ] Revisar overfull hbox warnings
- [ ] Adicionar figuras/grÃ¡ficos (atualmente sem figuras)
- [ ] Revisar citaÃ§Ãµes (algumas undefined no primeiro build)

### ğŸ“‰ SugestÃµes para ReduÃ§Ã£o de PÃ¡ginas

Para reduzir de 14 para 10 pÃ¡ginas:

1. **SeÃ§Ã£o 2 (Background)**: Reduzir revisÃ£o de literatura (~1 pÃ¡gina)
   - Remover detalhes de tÃ©cnicas avanÃ§adas de KD (Self-Distillation, Multi-Teacher)
   - Compactar descriÃ§Ãµes de LIME e Integrated Gradients

2. **SeÃ§Ã£o 4 (Implementation)**: Condensar exemplos de cÃ³digo (~1.5 pÃ¡ginas)
   - Manter apenas 1-2 exemplos de cÃ³digo mais importantes
   - Remover detalhes de otimizaÃ§Ã£o (caching, sampling)
   - Compactar tabela de custos computacionais

3. **SeÃ§Ã£o 5 (Evaluation)**: Reduzir detalhamento de experimentos (~1 pÃ¡gina)
   - Mesclar tabelas de resultados
   - Remover exemplos qualitativos detalhados
   - Manter apenas ablation study essencial

4. **SeÃ§Ã£o 6 (Discussion)**: Compactar (~0.5 pÃ¡gina)
   - Reduzir discussÃ£o de trade-offs
   - Condensar consideraÃ§Ãµes Ã©ticas
   - Remover comparaÃ§Ã£o detalhada com pruning

---

## ğŸ¯ Principais ContribuiÃ§Ãµes

1. **Framework DiXtill**: Primeira abordagem integrada de KD com alignment de explicaÃ§Ãµes
2. **TrÃªs mecanismos XAI**: SHAP, Attention, Gradient alignment (modular, plug-and-play)
3. **Metricas de avaliaÃ§Ã£o**: SHAP correlation, FAS, feature overlap, explanation divergence
4. **ValidaÃ§Ã£o empÃ­rica**: 3 domÃ­nios (NLP, visÃ£o, tabular) com 98-99% retenÃ§Ã£o de acurÃ¡cia
5. **ImplementaÃ§Ã£o prÃ¡tica**: Integrado no DeepBridge framework (open-source)

---

## ğŸ“š ReferÃªncias Principais

- Hinton et al. (2015) - Distilling the Knowledge in a Neural Network
- Lundberg & Lee (2017) - A Unified Approach to Interpreting Model Predictions (SHAP)
- Ribeiro et al. (2016) - "Why Should I Trust You?" (LIME)
- Zagoruyko & Komodakis (2017) - Attention Transfer
- Romero et al. (2015) - FitNets (Feature-based KD)

---

## ğŸ”— Links Ãšteis

- **DeepBridge Framework**: `/home/guhaase/projetos/DeepBridge/deepbridge`
- **Paper 08 (Template)**: `/home/guhaase/projetos/DeepBridge/papers/08_Regulatory_Compliance_Automation/POR`
- **DocumentaÃ§Ã£o Original**: `/home/guhaase/projetos/DeepBridge/papers/SUGESTOES_PAPERS.md` (linhas 1369-1459)

---

## âœï¸ Autores

- **Autor 1**: (substituir com nome real)
- **InstituiÃ§Ã£o**: (substituir com instituiÃ§Ã£o real)
- **Email**: autor1@email.com (substituir com email real)

---

## ğŸ“ Notas

- Paper escrito em **portuguÃªs** conforme solicitado
- Seguiu template do Paper 08 (Regulatory Compliance)
- Foco em aplicabilidade prÃ¡tica em ambientes regulados (finanÃ§as, saÃºde, contrataÃ§Ã£o)
- ÃŠnfase em compliance (GDPR, ECOA, EEOC) e explicabilidade mandatÃ³ria
- CÃ³digo de exemplo baseado na infraestrutura existente do DeepBridge (distillation/techniques/)

---

**Data de CriaÃ§Ã£o**: 2025-12-07
**Ãšltima AtualizaÃ§Ã£o**: 2025-12-07
